<?xml version="1.0" encoding="UTF-8"?>
<kboard>
<kboard_board_attached>
	<data>
		<uid><![CDATA[1]]></uid>
		<content_uid><![CDATA[1]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160111163003]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/569359fba30108208404.pdf]]></file_path>
		<file_name><![CDATA[theano-install-in-window.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[2]]></uid>
		<content_uid><![CDATA[10]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160125205338]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a60cc2c1ccb8160888.pdf]]></file_path>
		<file_name><![CDATA[160121연구세미나_Keystroke_문헌조사.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[3]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160126104318]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a6cf36364817914245.pdf]]></file_path>
		<file_name><![CDATA[Evaluating Variable Selection Techniques.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[4]]></uid>
		<content_uid><![CDATA[16]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160126110344]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a6d400a6d756661804.pdf]]></file_path>
		<file_name><![CDATA[Evaluating Variable Selection Techniques 2.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[5]]></uid>
		<content_uid><![CDATA[6]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160127234800]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a8d8a01b17e6389617.pdf]]></file_path>
		<file_name><![CDATA[0114 Research Trend Analysis.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[7]]></uid>
		<content_uid><![CDATA[14]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160127234928]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a8d8f8834006092712.pdf]]></file_path>
		<file_name><![CDATA[0121 Research Trend Analysis.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[10]]></uid>
		<content_uid><![CDATA[18]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160128180625]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a9da111bd249670410.pptx]]></file_path>
		<file_name><![CDATA[01_28.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[9]]></uid>
		<content_uid><![CDATA[11]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160128180548]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a9d9ecc93742924804.pptx]]></file_path>
		<file_name><![CDATA[01_21.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[11]]></uid>
		<content_uid><![CDATA[7]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160128180715]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56a9da43c18844931182.pptx]]></file_path>
		<file_name><![CDATA[01_14.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[12]]></uid>
		<content_uid><![CDATA[23]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160130162139]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201601/56ac648379c611100799.pptx]]></file_path>
		<file_name><![CDATA[0128 Research Trend Analysis.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[13]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160203175847]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56b1c147eb3fb9578399.pdf]]></file_path>
		<file_name><![CDATA[20160114_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[14]]></uid>
		<content_uid><![CDATA[15]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160203180015]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56b1c19f1aa6d3569427.pdf]]></file_path>
		<file_name><![CDATA[20160121_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[15]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160203180234]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56b1c22ae16624173675.pdf]]></file_path>
		<file_name><![CDATA[20160128_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[16]]></uid>
		<content_uid><![CDATA[25]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160204131448]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56b2d038619a68481689.pptx]]></file_path>
		<file_name><![CDATA[Keystroke_Semina_1_20160204_2.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[17]]></uid>
		<content_uid><![CDATA[26]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160204142301]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56b2e035c928d1604248.pdf]]></file_path>
		<file_name><![CDATA[Evaluating Variable Selection Techniques-ElasticNet.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[18]]></uid>
		<content_uid><![CDATA[27]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160205082814]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56b3de8e4ecb51485321.pdf]]></file_path>
		<file_name><![CDATA[20160204_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[19]]></uid>
		<content_uid><![CDATA[28]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160205233131]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56b4b24364d0d7494842.pptx]]></file_path>
		<file_name><![CDATA[02_04.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[20]]></uid>
		<content_uid><![CDATA[32]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160206230058]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201602/56b5fc9abf81a1706970.jpg]]></file_path>
		<file_name><![CDATA[workshop.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[21]]></uid>
		<content_uid><![CDATA[33]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160210221644]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56bb383c153928825561.pptx]]></file_path>
		<file_name><![CDATA[0204 Research Trend Analysis.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[22]]></uid>
		<content_uid><![CDATA[34]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160211134444]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56bc11bc6e1811778930.pdf]]></file_path>
		<file_name><![CDATA[20160211_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[23]]></uid>
		<content_uid><![CDATA[38]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160215203732]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56c1b87c4cde87100708.pptx]]></file_path>
		<file_name><![CDATA[02_11.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[24]]></uid>
		<content_uid><![CDATA[39]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160217125144]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56c3ee5080eda2464202.pptx]]></file_path>
		<file_name><![CDATA[0211 Research Trend Analysis.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[26]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160218003031]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56c4921754b7f8226806.pdf]]></file_path>
		<file_name><![CDATA[Evaluating Variable Selection Techniques-5.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[27]]></uid>
		<content_uid><![CDATA[41]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160218160119]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56c56c3f1534e5766418.pdf]]></file_path>
		<file_name><![CDATA[Keystroke_Semina_1_20160218.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[28]]></uid>
		<content_uid><![CDATA[42]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160219005010]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56c5e832675838717620.pptx]]></file_path>
		<file_name><![CDATA[0218 Research Trend Analysis.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[29]]></uid>
		<content_uid><![CDATA[43]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160219154743]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56c6ba8f01fe29353454.pptx]]></file_path>
		<file_name><![CDATA[02_18.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[30]]></uid>
		<content_uid><![CDATA[44]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160219164423]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56c6c7d7c936c1210113.pdf]]></file_path>
		<file_name><![CDATA[20160218_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[31]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160223174042]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56cc1b0a355c39840698.pdf]]></file_path>
		<file_name><![CDATA[deep-learning 2.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[32]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160227205858]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201602/56d18f82b854e2541107.pdf]]></file_path>
		<file_name><![CDATA[SPARSE CODING-류나현.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[33]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160301152709]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56d5363d564068139465.pdf]]></file_path>
		<file_name><![CDATA[20160229_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[34]]></uid>
		<content_uid><![CDATA[51]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160301221428]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56d595b4162144983093.pptx]]></file_path>
		<file_name><![CDATA[0229 Research Trend Analysis.pptx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[35]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160302163209]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56d696f9a7e545492858.pdf]]></file_path>
		<file_name><![CDATA[Evaluating Variable Selection Techniques-6`.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[36]]></uid>
		<content_uid><![CDATA[48]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160302190535]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56d6baef5d0a81176879.pdf]]></file_path>
		<file_name><![CDATA[02_28.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[37]]></uid>
		<content_uid><![CDATA[53]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160304135847]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56d9160710d1b7052093.pdf]]></file_path>
		<file_name><![CDATA[20160304_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[38]]></uid>
		<content_uid><![CDATA[59]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160316153527]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56e8feafe6ef74215148.pdf]]></file_path>
		<file_name><![CDATA[DEA를 활용한 군집타당성 분석에 관한 연구.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[39]]></uid>
		<content_uid><![CDATA[68]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160316160419]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56e9057396bd78279266.pdf]]></file_path>
		<file_name><![CDATA[2016춘계공동학술대회 발표자료 - 류나현(고려대).pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[40]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160318171226]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56ebb86abb6f17913146.pdf]]></file_path>
		<file_name><![CDATA[2016춘계산업공학회_Keystroke.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[41]]></uid>
		<content_uid><![CDATA[72]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160320230724]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56eeae9c6d2cc8020263.pdf]]></file_path>
		<file_name><![CDATA[Multi-Co-Training을 활용한 문서 분류.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[42]]></uid>
		<content_uid><![CDATA[74]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160321234246]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201603/56f00866527bf9179321.jpg]]></file_path>
		<file_name><![CDATA[IMG_5430-1.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[43]]></uid>
		<content_uid><![CDATA[74]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160321234246]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201603/56f00866529eb4680145.jpg]]></file_path>
		<file_name><![CDATA[Img+5447.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[44]]></uid>
		<content_uid><![CDATA[78]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160324103701]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201603/56f344bdb992e6364349.docx]]></file_path>
		<file_name><![CDATA[회의록_160322.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[45]]></uid>
		<content_uid><![CDATA[79]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160324160604]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201603/56f391dc288c05593383.docx]]></file_path>
		<file_name><![CDATA[회의록_160324.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[46]]></uid>
		<content_uid><![CDATA[81]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160325132627]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56f4bdf30b5a02132690.pdf]]></file_path>
		<file_name><![CDATA[20160325_논문세미나.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[47]]></uid>
		<content_uid><![CDATA[80]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160328221503]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201603/56f92e57142d04423889.pdf]]></file_path>
		<file_name><![CDATA[Backpropagation.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[48]]></uid>
		<content_uid><![CDATA[82]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160329185222]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201603/56fa5056aadb93192321.docx]]></file_path>
		<file_name><![CDATA[회의록_160329.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[49]]></uid>
		<content_uid><![CDATA[83]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160405191400]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201604/57038fe803b861716308.docx]]></file_path>
		<file_name><![CDATA[회의록_160405.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[50]]></uid>
		<content_uid><![CDATA[84]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160407165530]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201604/5706127215d616293487.pdf]]></file_path>
		<file_name><![CDATA[Multi-Co-Training을 활용한 문서 분류_조수현 .pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[51]]></uid>
		<content_uid><![CDATA[85]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160407165811]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201604/57061313aac745526367.pdf]]></file_path>
		<file_name><![CDATA[BI학회_나이브 베이즈 분류기 기반의 협동학습을 활용한 문서 분류_서덕성.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[52]]></uid>
		<content_uid><![CDATA[89]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160411190720]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201604/570b775882e262709472.docx]]></file_path>
		<file_name><![CDATA[회의록(Kick-off)_160408.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[53]]></uid>
		<content_uid><![CDATA[92]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160416181850]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201604/5712037a375956375061.docx]]></file_path>
		<file_name><![CDATA[2016 춘계공동학술대회 에세이_서덕성.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[54]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160418151114]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147a820367b3767181.jpg]]></file_path>
		<file_name><![CDATA[단체사진2.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[55]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160418151114]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147a82038109121093.jpg]]></file_path>
		<file_name><![CDATA[단체사진1.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[56]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file3]]></file_key>
		<date><![CDATA[20160418151646]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147bcee93be5983947.jpg]]></file_path>
		<file_name><![CDATA[크기변환_포맷변환_발표_김보섭.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[57]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file4]]></file_key>
		<date><![CDATA[20160418151646]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147bcee97d36983703.jpg]]></file_path>
		<file_name><![CDATA[크기변환_포맷변환_발표_김준홍.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[58]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file5]]></file_key>
		<date><![CDATA[20160418151646]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147bcee99456308044.jpg]]></file_path>
		<file_name><![CDATA[크기변환_포맷변환_발표_김해동.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[59]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file6]]></file_key>
		<date><![CDATA[20160418151646]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147bcee9aa56831817.jpg]]></file_path>
		<file_name><![CDATA[크기변환_포맷변환_발표_김형석.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[60]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file7]]></file_key>
		<date><![CDATA[20160418151646]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147bcee9bf01583648.jpg]]></file_path>
		<file_name><![CDATA[크기변환_포맷변환_발표_류나현.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[61]]></uid>
		<content_uid><![CDATA[96]]></content_uid>
		<file_key><![CDATA[file8]]></file_key>
		<date><![CDATA[20160418151646]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201604/57147bcee9d346220977.jpg]]></file_path>
		<file_name><![CDATA[크기변환_포맷변환_발표_조수현.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[62]]></uid>
		<content_uid><![CDATA[104]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160420123434]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201604/5716f8cad32057879089.docx]]></file_path>
		<file_name><![CDATA[회의록_160412.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[63]]></uid>
		<content_uid><![CDATA[105]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160420123624]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201604/5716f938c38d57897766.docx]]></file_path>
		<file_name><![CDATA[회의록_160419.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[64]]></uid>
		<content_uid><![CDATA[109]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160421161915]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201604/57187ef3c33071600402.docx]]></file_path>
		<file_name><![CDATA[회의록_160421.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[65]]></uid>
		<content_uid><![CDATA[110]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160425232616]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201604/571e29083b0da4642517.xlsx]]></file_path>
		<file_name><![CDATA[20160425미팅자료.xlsx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[66]]></uid>
		<content_uid><![CDATA[111]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160426180646]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201604/571f2fa67bd664615051.docx]]></file_path>
		<file_name><![CDATA[회의록_160426.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[67]]></uid>
		<content_uid><![CDATA[112]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160428155202]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201604/5721b31219dde7082580.docx]]></file_path>
		<file_name><![CDATA[회의록_160428.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[68]]></uid>
		<content_uid><![CDATA[113]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160428181056]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201604/5721d3a0bc4205261596.docx]]></file_path>
		<file_name><![CDATA[회의록_160428.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[70]]></uid>
		<content_uid><![CDATA[115]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160430163717]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201604/572460ad0ae7f9511932.jpg]]></file_path>
		<file_name><![CDATA[대한산업공학회 춘계학술대회2.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[71]]></uid>
		<content_uid><![CDATA[115]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160430163717]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201604/572460ad2b54a4330230.jpg]]></file_path>
		<file_name><![CDATA[대한산업공학회 춘계학술대회1.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[76]]></uid>
		<content_uid><![CDATA[119]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160506133307]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201605/572c1e831787b7052368.docx]]></file_path>
		<file_name><![CDATA[회의록_160506.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[75]]></uid>
		<content_uid><![CDATA[118]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160504215159]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201605/5729f06fd3ffa8946960.docx]]></file_path>
		<file_name><![CDATA[0504_회의록.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[74]]></uid>
		<content_uid><![CDATA[116]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160503211052]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201605/5728954c644749463867.docx]]></file_path>
		<file_name><![CDATA[회의록_160503.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[77]]></uid>
		<content_uid><![CDATA[121]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160510172919]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201605/57319bdf9f3376301727.docx]]></file_path>
		<file_name><![CDATA[회의록_1605010.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[78]]></uid>
		<content_uid><![CDATA[122]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160510195506]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201605/5731be0ad60e24089080.docx]]></file_path>
		<file_name><![CDATA[0510_회의록.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[79]]></uid>
		<content_uid><![CDATA[124]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160516002636]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201605/5738952cc2e1e2137908.hwp]]></file_path>
		<file_name><![CDATA[데이터보고서20160516_0023_김준홍.hwp]]></file_name>
	</data>
	<data>
		<uid><![CDATA[80]]></uid>
		<content_uid><![CDATA[125]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160517172751]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201605/573ad607271c28434448.docx]]></file_path>
		<file_name><![CDATA[회의록_1605017.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[81]]></uid>
		<content_uid><![CDATA[128]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160520142332]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201605/573e9f5481b3d3950103.docx]]></file_path>
		<file_name><![CDATA[회의록0520.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[82]]></uid>
		<content_uid><![CDATA[129]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160524124110]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201605/5743cd560d87e2773742.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160524_121654067.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[83]]></uid>
		<content_uid><![CDATA[129]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160524124110]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201605/5743cd562afe65092956.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160524_121653618.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[84]]></uid>
		<content_uid><![CDATA[130]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160524223939]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201605/5744599b629706362426.docx]]></file_path>
		<file_name><![CDATA[회의록_160524.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[85]]></uid>
		<content_uid><![CDATA[131]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160526181750]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201605/5746bf3e7d6db4391204.docx]]></file_path>
		<file_name><![CDATA[회의록0526.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[86]]></uid>
		<content_uid><![CDATA[134]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160601182318]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201606/574ea986498267128723.docx]]></file_path>
		<file_name><![CDATA[회의록_160531.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[87]]></uid>
		<content_uid><![CDATA[136]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160603153340]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201606/575124c4651316152038.docx]]></file_path>
		<file_name><![CDATA[회의록_0603.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[88]]></uid>
		<content_uid><![CDATA[137]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160607172328]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201606/5756847ff32768539367.docx]]></file_path>
		<file_name><![CDATA[회의록_160607.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[89]]></uid>
		<content_uid><![CDATA[138]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160610151900]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201606/575a5bd4b7dd91660827.docx]]></file_path>
		<file_name><![CDATA[회의록0610.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[90]]></uid>
		<content_uid><![CDATA[140]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160614192253]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201606/575fdafd740997779937.docx]]></file_path>
		<file_name><![CDATA[회의록_160614.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[91]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/57692828e296e3448303.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230530929.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[92]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/57692829160955972686.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230533270.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[93]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file3]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/57692829245281011535.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230534559.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[94]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file4]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/57692829324af5958129.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230535493.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[95]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file5]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/57692829449974756500.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230537037.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[96]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file6]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/5769282952d2d3159912.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230537991.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[97]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file7]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/576928295fce77311645.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230538410.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[98]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file8]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/576928296d34a7795043.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230539534.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[99]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file9]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/576928297a7163374969.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230540018.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[100]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file10]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/5769282985fa62623504.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230547969.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[101]]></uid>
		<content_uid><![CDATA[141]]></content_uid>
		<file_key><![CDATA[file11]]></file_key>
		<date><![CDATA[20160621204233]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201606/5769282992c749012054.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230548546.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[102]]></uid>
		<content_uid><![CDATA[142]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160621204358]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201606/5769287e7455d4698547.jpg]]></file_path>
		<file_name><![CDATA[KakaoTalk_20160617_230540018.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[103]]></uid>
		<content_uid><![CDATA[143]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160624213457]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201606/576d28f1dd54f9180419.docx]]></file_path>
		<file_name><![CDATA[회의록0624.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[104]]></uid>
		<content_uid><![CDATA[144]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160628144902]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201606/57720fcdf012f6484100.docx]]></file_path>
		<file_name><![CDATA[회의록_160628.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[105]]></uid>
		<content_uid><![CDATA[145]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160629193744]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201606/5773a4f8c59d68811828.docx]]></file_path>
		<file_name><![CDATA[회의록0629.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[106]]></uid>
		<content_uid><![CDATA[148]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160705154724]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201607/577b57fc9580f7313842.docx]]></file_path>
		<file_name><![CDATA[회의록_160705.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[107]]></uid>
		<content_uid><![CDATA[149]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160705154824]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201607/577b5838a1fb14780944.docx]]></file_path>
		<file_name><![CDATA[회의록0705.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[108]]></uid>
		<content_uid><![CDATA[151]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160711225836]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201607/5783a60c607398611602.docx]]></file_path>
		<file_name><![CDATA[회의록0711.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[109]]></uid>
		<content_uid><![CDATA[153]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160712154503]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201607/578491ef785fd8603088.docx]]></file_path>
		<file_name><![CDATA[회의록_160712.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[110]]></uid>
		<content_uid><![CDATA[155]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160719160158]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201607/578dd066e1d041962402.docx]]></file_path>
		<file_name><![CDATA[회의록_160719.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[111]]></uid>
		<content_uid><![CDATA[156]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160719162504]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201607/578dd5d005a6e1872863.docx]]></file_path>
		<file_name><![CDATA[회의록0719.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[112]]></uid>
		<content_uid><![CDATA[157]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160724171634]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201607/57947962555371379852.docx]]></file_path>
		<file_name><![CDATA[회의록0722.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[113]]></uid>
		<content_uid><![CDATA[158]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160726171841]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201607/57971ce1a0c8d4105560.docx]]></file_path>
		<file_name><![CDATA[회의록_160726.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[114]]></uid>
		<content_uid><![CDATA[159]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160727075107]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201607/5797e95ba49f87187774.docx]]></file_path>
		<file_name><![CDATA[회의록 0726.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[118]]></uid>
		<content_uid><![CDATA[163]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160803104157]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201608/57a14be5a3d899501495.docx]]></file_path>
		<file_name><![CDATA[회의록_160803.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[116]]></uid>
		<content_uid><![CDATA[161]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160729134636]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201607/579adfac0a9819220245.jpg]]></file_path>
		<file_name><![CDATA[크기변환_KakaoTalk_20160717_113204727.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[117]]></uid>
		<content_uid><![CDATA[161]]></content_uid>
		<file_key><![CDATA[file3]]></file_key>
		<date><![CDATA[20160729134636]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201607/579adfac347518426483.jpg]]></file_path>
		<file_name><![CDATA[크기변환_KakaoTalk_20160717_113208010.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[119]]></uid>
		<content_uid><![CDATA[164]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160810111314]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201608/57aa8dba5f32c6615661.docx]]></file_path>
		<file_name><![CDATA[회의록_160810.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[120]]></uid>
		<content_uid><![CDATA[167]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160814162847]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201608/57b01dafaca264788909.docx]]></file_path>
		<file_name><![CDATA[회의록 0811.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[121]]></uid>
		<content_uid><![CDATA[168]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160817140838]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201608/57b3f156e12722232116.docx]]></file_path>
		<file_name><![CDATA[회의록 0816.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[122]]></uid>
		<content_uid><![CDATA[169]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160817200406]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201608/57b444a6e21dd1738830.docx]]></file_path>
		<file_name><![CDATA[회의록_160816.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[123]]></uid>
		<content_uid><![CDATA[170]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160818143204]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201608/57b548543f8024363464.docx]]></file_path>
		<file_name><![CDATA[회의록_160818.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[124]]></uid>
		<content_uid><![CDATA[171]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160819133612]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201608/57b68cbc7b5449272979.docx]]></file_path>
		<file_name><![CDATA[0818 회의록.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[125]]></uid>
		<content_uid><![CDATA[172]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160831192518]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201608/57c6b08e9d5a05839752.docx]]></file_path>
		<file_name><![CDATA[회의록0831.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[126]]></uid>
		<content_uid><![CDATA[174]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160901162425]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201609/57c7d7a93f0771196929.docx]]></file_path>
		<file_name><![CDATA[회의록_160831.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[127]]></uid>
		<content_uid><![CDATA[175]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160902133538]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201609/57c9019a457475936981.docx]]></file_path>
		<file_name><![CDATA[회의록-0831.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[128]]></uid>
		<content_uid><![CDATA[176]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160905155459]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201609/57cd16c3934b05183319.docx]]></file_path>
		<file_name><![CDATA[회의록_160905.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[129]]></uid>
		<content_uid><![CDATA[177]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160905160021]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201609/57cd1805cf0907540435.docx]]></file_path>
		<file_name><![CDATA[회의록-0905.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[130]]></uid>
		<content_uid><![CDATA[179]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160905183312]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201609/57cd3bd82713e2573791.docx]]></file_path>
		<file_name><![CDATA[회의록0905.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[131]]></uid>
		<content_uid><![CDATA[183]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160911153319]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201609/57d4faaf859219721221.docx]]></file_path>
		<file_name><![CDATA[회의록_160902.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[132]]></uid>
		<content_uid><![CDATA[184]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160911155939]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201609/57d500dbaa8797955993.docx]]></file_path>
		<file_name><![CDATA[회의록_160906.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[133]]></uid>
		<content_uid><![CDATA[186]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160912175104]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201609/57d66c7836c346081726.docx]]></file_path>
		<file_name><![CDATA[회의록_160912.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[135]]></uid>
		<content_uid><![CDATA[187]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160912180739]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201609/57d6705b86fe62050018.docx]]></file_path>
		<file_name><![CDATA[회의록-0912.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[136]]></uid>
		<content_uid><![CDATA[188]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160913154023]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201609/57d79f574328b3854248.docx]]></file_path>
		<file_name><![CDATA[회의록_160912.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[137]]></uid>
		<content_uid><![CDATA[189]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160913154928]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201609/57d7a1786532a2337036.docx]]></file_path>
		<file_name><![CDATA[회의록0912.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[138]]></uid>
		<content_uid><![CDATA[190]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160926155735]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201609/57e8c6deebe962908874.docx]]></file_path>
		<file_name><![CDATA[회의록_160926.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[139]]></uid>
		<content_uid><![CDATA[191]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160926170836]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201609/57e8d7843b0711639678.docx]]></file_path>
		<file_name><![CDATA[회의록-0926.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[140]]></uid>
		<content_uid><![CDATA[186]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160926230410]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201609/57e92adac9a139141967.pdf]]></file_path>
		<file_name><![CDATA[2016-09-12 미팅자료.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[141]]></uid>
		<content_uid><![CDATA[192]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160926232452]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201609/57e92fb4ca3995579376.docx]]></file_path>
		<file_name><![CDATA[회의록_160926.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[142]]></uid>
		<content_uid><![CDATA[192]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20160926232452]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201609/57e92fb4ca5cf2223327.pdf]]></file_path>
		<file_name><![CDATA[2016-09-26 미팅자료.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[143]]></uid>
		<content_uid><![CDATA[193]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20160927153116]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201609/57ea1234ed6576958984.docx]]></file_path>
		<file_name><![CDATA[회의록0926.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[144]]></uid>
		<content_uid><![CDATA[195]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161005200924]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201610/57f4df6495b4f9279296.docx]]></file_path>
		<file_name><![CDATA[회의록_161004.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[145]]></uid>
		<content_uid><![CDATA[196]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161007134954]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201610/57f72972bedf94094024.docx]]></file_path>
		<file_name><![CDATA[회의록0929.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[146]]></uid>
		<content_uid><![CDATA[198]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161010162825]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201610/57fb43198052d2597412.docx]]></file_path>
		<file_name><![CDATA[회의록-1010.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[147]]></uid>
		<content_uid><![CDATA[199]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161010165035]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/6/201610/57fb484b676161778656.docx]]></file_path>
		<file_name><![CDATA[회의록1010.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[151]]></uid>
		<content_uid><![CDATA[200]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161014133304]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201610/580060005f5c38033447.docx]]></file_path>
		<file_name><![CDATA[회의록_161010.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[149]]></uid>
		<content_uid><![CDATA[201]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161011005058]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201610/57fbb8e20bd4d7271545.docx]]></file_path>
		<file_name><![CDATA[회의록_161010.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[150]]></uid>
		<content_uid><![CDATA[202]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161014133228]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201610/58005fdcf19948795623.docx]]></file_path>
		<file_name><![CDATA[회의록_161014.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[153]]></uid>
		<content_uid><![CDATA[203]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161014152308]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201610/580079cc61a5a4658172.docx]]></file_path>
		<file_name><![CDATA[회의록-1014.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[154]]></uid>
		<content_uid><![CDATA[205]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161021132054]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201610/580997a649dce3339263.docx]]></file_path>
		<file_name><![CDATA[회의록_161021.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[155]]></uid>
		<content_uid><![CDATA[206]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161021133658]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201610/58099b6a02ace1943450.jpg]]></file_path>
		<file_name><![CDATA[빅스타 수상.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[156]]></uid>
		<content_uid><![CDATA[207]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161021141047]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/11/201610/5809a356ea68f1885498.jpg]]></file_path>
		<file_name><![CDATA[seolhyeon_test.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[157]]></uid>
		<content_uid><![CDATA[208]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161021164126]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201610/5809c6a68b2b84941070.docx]]></file_path>
		<file_name><![CDATA[회의록-1021.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[158]]></uid>
		<content_uid><![CDATA[209]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161101145302]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201611/58182dbe7e0a36230316.docx]]></file_path>
		<file_name><![CDATA[회의록_161031.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[159]]></uid>
		<content_uid><![CDATA[210]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161101210716]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201611/58188574a84134561218.docx]]></file_path>
		<file_name><![CDATA[회의록_161101.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[160]]></uid>
		<content_uid><![CDATA[211]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161102124457]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201611/58196139857e45655456.docx]]></file_path>
		<file_name><![CDATA[회의록-1101.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[161]]></uid>
		<content_uid><![CDATA[212]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161104151803]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201611/581c281b63b7d2030242.docx]]></file_path>
		<file_name><![CDATA[회의록-1104.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[162]]></uid>
		<content_uid><![CDATA[213]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161105191651]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201611/581db19366bb05735382.docx]]></file_path>
		<file_name><![CDATA[회의록_161104.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[163]]></uid>
		<content_uid><![CDATA[215]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161109173401]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201611/5822df79dabd96693389.docx]]></file_path>
		<file_name><![CDATA[회의록_161109.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[164]]></uid>
		<content_uid><![CDATA[216]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161110174331]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201611/582433334cdb74870758.docx]]></file_path>
		<file_name><![CDATA[회의록_161107.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[165]]></uid>
		<content_uid><![CDATA[217]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161111142914]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201611/5825572ab8d435224792.docx]]></file_path>
		<file_name><![CDATA[회의록_161111.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[166]]></uid>
		<content_uid><![CDATA[218]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161111152309]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201611/582563cd291f12771270.docx]]></file_path>
		<file_name><![CDATA[회의록-1111.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[167]]></uid>
		<content_uid><![CDATA[219]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161116171451]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201611/582c157bd327c3596618.docx]]></file_path>
		<file_name><![CDATA[회의록_161115.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[168]]></uid>
		<content_uid><![CDATA[220]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161121153243]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201611/5832950ac92672546051.jpg]]></file_path>
		<file_name><![CDATA[김준홍 발표.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[169]]></uid>
		<content_uid><![CDATA[220]]></content_uid>
		<file_key><![CDATA[file2]]></file_key>
		<date><![CDATA[20161121153243]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201611/5832950aeb3909489685.jpg]]></file_path>
		<file_name><![CDATA[김형석 발표.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[170]]></uid>
		<content_uid><![CDATA[220]]></content_uid>
		<file_key><![CDATA[file3]]></file_key>
		<date><![CDATA[20161121153243]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201611/5832950b076c97840637.jpg]]></file_path>
		<file_name><![CDATA[박민식 발표.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[173]]></uid>
		<content_uid><![CDATA[220]]></content_uid>
		<file_key><![CDATA[file6]]></file_key>
		<date><![CDATA[20161121153441]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201611/58329581af6508637420.jpg]]></file_path>
		<file_name><![CDATA[서덕성 발표.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[172]]></uid>
		<content_uid><![CDATA[220]]></content_uid>
		<file_key><![CDATA[file5]]></file_key>
		<date><![CDATA[20161121153243]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201611/5832950b235b02390319.jpg]]></file_path>
		<file_name><![CDATA[조수현 발표.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[174]]></uid>
		<content_uid><![CDATA[220]]></content_uid>
		<file_key><![CDATA[file7]]></file_key>
		<date><![CDATA[20161121153441]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201611/58329581c01b73916320.jpg]]></file_path>
		<file_name><![CDATA[조수현 수상.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[175]]></uid>
		<content_uid><![CDATA[222]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161121174624]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201611/5832b460b87432211791.docx]]></file_path>
		<file_name><![CDATA[회의록_161121.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[176]]></uid>
		<content_uid><![CDATA[223]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161121174742]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201611/5832b4ae0d3db8399017.docx]]></file_path>
		<file_name><![CDATA[회의록-1121.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[177]]></uid>
		<content_uid><![CDATA[227]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161122183903]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201611/583412373060d8289154.docx]]></file_path>
		<file_name><![CDATA[회의록_161121.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[178]]></uid>
		<content_uid><![CDATA[233]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161128124930]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201611/583ba94a8dbbf4167633.docx]]></file_path>
		<file_name><![CDATA[회의록-1128.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[179]]></uid>
		<content_uid><![CDATA[235]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161202174157]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201612/584133d50ff041983276.docx]]></file_path>
		<file_name><![CDATA[회의록_161202.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[180]]></uid>
		<content_uid><![CDATA[236]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161202180712]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201612/584139c0a5e9d4273376.docx]]></file_path>
		<file_name><![CDATA[회의록-1202.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[182]]></uid>
		<content_uid><![CDATA[237]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161202183814]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201612/5841410643b185684844.docx]]></file_path>
		<file_name><![CDATA[회의록_161202.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[183]]></uid>
		<content_uid><![CDATA[241]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161209173505]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201612/584a6cb97ba989709686.docx]]></file_path>
		<file_name><![CDATA[회의록_161209.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[184]]></uid>
		<content_uid><![CDATA[242]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161209174235]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201612/584a6e7aede017469024.docx]]></file_path>
		<file_name><![CDATA[회의록-1209.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[185]]></uid>
		<content_uid><![CDATA[244]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161219155538]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201612/5857846a730612267547.docx]]></file_path>
		<file_name><![CDATA[회의록_161219.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[186]]></uid>
		<content_uid><![CDATA[245]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161219175939]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201612/5857a17b4a03b1641601.docx]]></file_path>
		<file_name><![CDATA[회의록-1219.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[187]]></uid>
		<content_uid><![CDATA[247]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161222214717]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201612/585bcb555ee1d4037719.docx]]></file_path>
		<file_name><![CDATA[회의록_161219.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[188]]></uid>
		<content_uid><![CDATA[248]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161223170000]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201612/585cd9806d9b13667755.docx]]></file_path>
		<file_name><![CDATA[회의록_161223.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[189]]></uid>
		<content_uid><![CDATA[249]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161223173502]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201612/585ce1b6e2da55848815.docx]]></file_path>
		<file_name><![CDATA[회의록-1223.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[190]]></uid>
		<content_uid><![CDATA[250]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161223175816]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201612/585ce72847f708471801.docx]]></file_path>
		<file_name><![CDATA[회의록_161223.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[191]]></uid>
		<content_uid><![CDATA[253]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161230164742]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201612/5866111e5df171447967.docx]]></file_path>
		<file_name><![CDATA[회의록_161230.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[192]]></uid>
		<content_uid><![CDATA[254]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20161230193021]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201612/5866373dbdf7b3879241.docx]]></file_path>
		<file_name><![CDATA[회의록-1230.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[193]]></uid>
		<content_uid><![CDATA[256]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170104170631]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201701/586cad075b4ec1013458.docx]]></file_path>
		<file_name><![CDATA[회의록_161230.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[194]]></uid>
		<content_uid><![CDATA[257]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170105162050]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201701/586df3d2ba9c93593322.docx]]></file_path>
		<file_name><![CDATA[회의록-0105.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[195]]></uid>
		<content_uid><![CDATA[258]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170105171648]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201701/586e00f0a2f311945098.docx]]></file_path>
		<file_name><![CDATA[회의록_170105.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[198]]></uid>
		<content_uid><![CDATA[259]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170112162627]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201701/58772fa33a1758621490.docx]]></file_path>
		<file_name><![CDATA[회의록_170105.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[200]]></uid>
		<content_uid><![CDATA[265]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170112163922]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201701/587732aae58899431182.docx]]></file_path>
		<file_name><![CDATA[회의록_170110.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[201]]></uid>
		<content_uid><![CDATA[266]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170112164141]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201701/587733353463d5447540.docx]]></file_path>
		<file_name><![CDATA[회의록_170112.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[202]]></uid>
		<content_uid><![CDATA[267]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170115111556]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201701/587adb5c914166128967.docx]]></file_path>
		<file_name><![CDATA[회의록_170112.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[203]]></uid>
		<content_uid><![CDATA[268]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170117124315]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201701/587d92d39c5131067840.docx]]></file_path>
		<file_name><![CDATA[회의록-0112.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[204]]></uid>
		<content_uid><![CDATA[269]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170117124618]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201701/587d938a4ec3d9567413.docx]]></file_path>
		<file_name><![CDATA[회의록_170116.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[205]]></uid>
		<content_uid><![CDATA[272]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170120104227]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201701/58816b036bb565886169.docx]]></file_path>
		<file_name><![CDATA[회의록_170120.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[206]]></uid>
		<content_uid><![CDATA[276]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170124193609]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201701/58872e19759e81526519.png]]></file_path>
		<file_name><![CDATA[logo.png]]></file_name>
	</data>
	<data>
		<uid><![CDATA[207]]></uid>
		<content_uid><![CDATA[287]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170126163903]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201701/5889a797956412072540.docx]]></file_path>
		<file_name><![CDATA[회의록_170126.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[208]]></uid>
		<content_uid><![CDATA[290]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170127024803]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/9/201701/588a365391f9c2808624.docx]]></file_path>
		<file_name><![CDATA[회의록_170126.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[209]]></uid>
		<content_uid><![CDATA[291]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170130190804]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201701/588f10846a81a4953155.docx]]></file_path>
		<file_name><![CDATA[회의록-0126.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[210]]></uid>
		<content_uid><![CDATA[295]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170208225421]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/4/201702/589b230c71efe3090148.jpg]]></file_path>
		<file_name><![CDATA[연구실 워크샵.jpg]]></file_name>
	</data>
	<data>
		<uid><![CDATA[211]]></uid>
		<content_uid><![CDATA[297]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170212000422]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201702/589f27f6d12209943969.docx]]></file_path>
		<file_name><![CDATA[회의록_170203.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[212]]></uid>
		<content_uid><![CDATA[298]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170212000449]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201702/589f28116791f5885345.docx]]></file_path>
		<file_name><![CDATA[회의록_170209.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[213]]></uid>
		<content_uid><![CDATA[302]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170303172245]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201703/58b927d537f867272369.docx]]></file_path>
		<file_name><![CDATA[회의록_170303.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[214]]></uid>
		<content_uid><![CDATA[303]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170303172808]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201703/58b9291849d128048278.docx]]></file_path>
		<file_name><![CDATA[회의록-0303.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[215]]></uid>
		<content_uid><![CDATA[305]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170310165409]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201703/58c25ba1a649a9628936.docx]]></file_path>
		<file_name><![CDATA[회의록_170310.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[216]]></uid>
		<content_uid><![CDATA[307]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170317164512]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201703/58cb940845d884208557.docx]]></file_path>
		<file_name><![CDATA[회의록_170317.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[217]]></uid>
		<content_uid><![CDATA[309]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170324163417]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201703/58d4cbf977d252570220.docx]]></file_path>
		<file_name><![CDATA[회의록_170324.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[218]]></uid>
		<content_uid><![CDATA[311]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170329184323]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201703/58db81bb84d104894653.docx]]></file_path>
		<file_name><![CDATA[회의록_170329.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[219]]></uid>
		<content_uid><![CDATA[312]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170331170756]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201703/58de0e5c9a6d21772888.docx]]></file_path>
		<file_name><![CDATA[회의록_170331.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[220]]></uid>
		<content_uid><![CDATA[314]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170406175706]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201704/58e602e24caa01963500.docx]]></file_path>
		<file_name><![CDATA[회의록-0406.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[221]]></uid>
		<content_uid><![CDATA[315]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170410154216]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201704/58eb2948e4e3e5217102.docx]]></file_path>
		<file_name><![CDATA[회의록_170410(최종보고).docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[222]]></uid>
		<content_uid><![CDATA[316]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170412182607]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201704/58edf2af2e4b77544006.docx]]></file_path>
		<file_name><![CDATA[회의록_170412.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[223]]></uid>
		<content_uid><![CDATA[317]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170413002341]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201704/58ee467d5b5cb1190887.docx]]></file_path>
		<file_name><![CDATA[회의록-0412.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[224]]></uid>
		<content_uid><![CDATA[318]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170415143129]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/1/201704/58f1b0314c5fe8474273.pdf]]></file_path>
		<file_name><![CDATA[20170414+개인연구+발표+-+민식.pdf]]></file_name>
	</data>
	<data>
		<uid><![CDATA[225]]></uid>
		<content_uid><![CDATA[320]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170512175747]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201705/5915790bd9ff76125396.docx]]></file_path>
		<file_name><![CDATA[회의록_170512.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[226]]></uid>
		<content_uid><![CDATA[323]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170516131810]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201705/591a7d8265b3c7306976.docx]]></file_path>
		<file_name><![CDATA[회의록-0512.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[227]]></uid>
		<content_uid><![CDATA[325]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170519175151]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201705/591eb227b9a528457794.docx]]></file_path>
		<file_name><![CDATA[회의록_170519.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[228]]></uid>
		<content_uid><![CDATA[326]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170526170355]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201705/5927e16bdf77c5788116.docx]]></file_path>
		<file_name><![CDATA[회의록_170526.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[229]]></uid>
		<content_uid><![CDATA[329]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170602183002]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201706/5931301a818a22274688.docx]]></file_path>
		<file_name><![CDATA[회의록_170601.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[230]]></uid>
		<content_uid><![CDATA[330]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170602183114]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201706/593130628555e1323547.docx]]></file_path>
		<file_name><![CDATA[회의록_170602.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[231]]></uid>
		<content_uid><![CDATA[331]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170629124616]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201706/5954780881dcf9739074.docx]]></file_path>
		<file_name><![CDATA[회의록_170623.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[232]]></uid>
		<content_uid><![CDATA[332]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170703091653]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201707/59598cf52a9a55106689.docx]]></file_path>
		<file_name><![CDATA[회의록_170630.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[233]]></uid>
		<content_uid><![CDATA[337]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170707164107]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201707/595f3b13d87037267150.docx]]></file_path>
		<file_name><![CDATA[회의록_170707.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[234]]></uid>
		<content_uid><![CDATA[340]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170721135114]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201707/59718842671364344787.docx]]></file_path>
		<file_name><![CDATA[회의록-0718.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[235]]></uid>
		<content_uid><![CDATA[341]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170725165144]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/8/201707/5976f89086d768118865.docx]]></file_path>
		<file_name><![CDATA[회의록-0725.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[236]]></uid>
		<content_uid><![CDATA[349]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170818162752]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201708/599696f8bc6229561096.docx]]></file_path>
		<file_name><![CDATA[회의록_170818.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[237]]></uid>
		<content_uid><![CDATA[355]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170901164053]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201709/59a90f05078111449340.docx]]></file_path>
		<file_name><![CDATA[회의록_170901.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[238]]></uid>
		<content_uid><![CDATA[357]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20170907145802]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201709/59b0dfeae2bd86693389.docx]]></file_path>
		<file_name><![CDATA[회의록_170907.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[241]]></uid>
		<content_uid><![CDATA[365]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20171110160139]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/7/201711/5a054ed35ccca9378448.png]]></file_path>
		<file_name><![CDATA[캡처.PNG]]></file_name>
	</data>
	<data>
		<uid><![CDATA[242]]></uid>
		<content_uid><![CDATA[382]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20171116181129]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201711/5a0d5641c901d5072906.docx]]></file_path>
		<file_name><![CDATA[회의록1116.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[243]]></uid>
		<content_uid><![CDATA[383]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20171201161346]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201712/5a21012a32bb84721069.docx]]></file_path>
		<file_name><![CDATA[회의록1129.docx]]></file_name>
	</data>
	<data>
		<uid><![CDATA[244]]></uid>
		<content_uid><![CDATA[384]]></content_uid>
		<file_key><![CDATA[file1]]></file_key>
		<date><![CDATA[20171207145428]]></date>
		<file_path><![CDATA[/wp-content/uploads/kboard_attached/5/201712/5a28d7947d2899018371.docx]]></file_path>
		<file_name><![CDATA[회의록1207.docx]]></file_name>
	</data>
</kboard_board_attached>
<kboard_board_content>
	<data>
		<uid><![CDATA[1]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[윈도우에서의 딥러닝 환경 구축]]></title>
		<content><![CDATA[윈도우에서 딥러닝 환경을 구축하는 방법에 관련한 pdf입니다.
dsba 연구실의 석사과정 김동화 학생이 작성하였습니다.]]></content>
		<date><![CDATA[20160111163003]]></date>
		<update><![CDATA[20160111163003]]></update>
		<view><![CDATA[33]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[true]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[2]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[Restricted Boltzmann Machine I (발표자 : 김준홍)]]></title>
		<content><![CDATA[오늘의 세미나에서는 
[1] 간략한 Feed Forward Neural Network의 Review
[2] RBM의 개념, Visible Layer와 Hidden Layer의 업데이트 식과 그 증명
을 행하였다. 개인적으로 Energy based model에 대한 개념을 처음 접하였는데, Energy based function을 이용한 Visible Layer와 Hidden Layer의 파라미터 최적화에 대한 직관적인 이해가 잘 되지 않았다. 이를 좀 더 보완하여 다음주 이어질 세미나의 이해와 학습을 하여야 할 것으로 생각된다.]]></content>
		<date><![CDATA[20160111172738]]></date>
		<update><![CDATA[20160111172738]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[3]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[Restricted Boltzmann Machine II (발표자 : 김형석)]]></title>
		<content><![CDATA[금주의 딥러닝 세미나시간에는 지난 시간에 이어서 RBM에 대하여 다루도록 하였다. 지난 시간발표에서는 홉필드-신경망에서 착안한 Energy Based Model의 정의와 그 특징을 알아보았다. 또한, Boltzmann machine(BM)과 달리 Restricted Boltzmann Machine의 RBM의 구조적 특징으로 인해 생성되는 Energy function의 정의와 그 확률분포의 성질 등을 알아보았다. 금주에는 실제로 RBM을 Training하는 과정을 다루었다. RBM은 Visible layer와 Hidden layer는서로 internal edge가 존재하지 않고, Layer간에 undirected fully connected된 형태로 이러한 성질로 인해 각 층내의 노드값들은 주어진 서로 다른층의 값에 대해서 서로 독립이기에 이를 이용하여 각각의 조건부 확률은 서로 conditionally independent하다는 속성을 가진다. 이를 통해서 BM에서의 compute complexity문제를 해결하여 training 할 수 있게 되었다. Training의 목적은 Negative Log-likelihood(NLL)를 최소화 하는 것으로 이를 Stochastic Gradient Descent를 진행함으로써 트레이닝 하고자 한다. 하지만 실제로 Negative Log-likelihood을 편미분 진행하게 되면, training data에 기반한 Positive phase와 Negative phase로 나누어 지게 되는데, 이때 Negative phase의 경우는 모델에서 가능한 모든 배열(v,h)과 p(v,h)확률과 E(v,h)를 계산해야 하는 문제가 발생하게 된다. 하지만 이를 극복하기 위해서 RBM의 학습과정에서 MCMC방법론중 하나인 Gibbs sampling을 통한 approximation을 통해서 추정하게 된다. Gibbs sampling이란 MCMC(Markov Chain Monte Carlo)의 방법론 중 하나로 각 차원에 대한 조건부 확률을 알고있을 때, 처음 하나의 초기 샘플을 랜덤으로 정한 후, 그 샘플에서 차원을 1개씩 순차적으로 정하는 방법이다. 일반적으로 MCMC는 Converge할 때 까지 걸리는 시간이 상당하고, 이를 매번 Gradient를 업데이트 할 때마다 사용한다는 것은 비효율적이므로 CD-k(Contrastive Divergence – k)를 사용하여 RBM update 과정 중에서 이 Gibbs sampling을 일정 횟수만을 수행 후 update하게 된다. 이 같은 아이디어는 어차피 정확하게 converge한 distribution이나, 중간에 멈춘 distribution이나 대략의 방향성은 공유할 것. 따라서 완벽한 gradient 대신 Gibbs sampling을 중간에 멈추고 그 approximation 값을 update에 사용하자는 것이다. 실제로 경험적으로 k=1일 경우에도 상당히 그 성능이 확보된다는데 서 매우 흥미로운 부분이다. 이러한 과정을 통해서 우리는 NLL(negative log-likelihood)의 gradient를 효율적으로 approximation하여 연산이 가능해지고 이를 기존의 Neural Network에서의 stochastic gradient descent방법론과 마찬가지로 parameter를 학습할 수 있게 되는 것이다. 추후에는 실제로 coding 구현을 통해서 알고리즘을 확인해보도록 하겠다.]]></content>
		<date><![CDATA[20160111173420]]></date>
		<update><![CDATA[20160111173420]]></update>
		<view><![CDATA[15]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[4]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[Enhanced keystroke dynamics user authentication (2016/01/14 김준홍, 김해동)]]></title>
		<content><![CDATA[오늘은 키스트로크 연구 관련 첫번째 발표를 행하였다.

  12월부터 이루어진  Data collection에 대한 설명과 앞으로 행하게 될 키스트로크 Android App Version의 개발 상황과 문제점, 앞으로의 방향에 대하여 서술하였다. 그리고 4가지 기본 변수,  Key mapping table 에 대하여 서술하였고 기본 데이터 탐색으로 인해, 나타난 문제점을 서술하였다. 그리고 토의해야할 부분에서, 사용자별 Outlier 처리와 Android App의 경우 한글을 제외한 영어, 10000 Keystroke를 100명에게 실험하는것으로 결정지었다.
  다음주까지, 앞으로에 대한 Research Framework와 Digraph 통계치를 통하여 새로운 변수를 통해 모델링을 할 계획이며, 다음주 월요일부터 Android App을 통해 데이터를 수집할 계획이다.]]></content>
		<date><![CDATA[20160114142457]]></date>
		<update><![CDATA[20160114142457]]></update>
		<view><![CDATA[26]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[5]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[Integrating Cluster Validity Indices based on DEA (2016/01/14, 김보섭)]]></title>
		<content><![CDATA[논문연구세미나에서 제가 맡은 연구인 Integrating Cluster Validity Indices based on DEA에 관해서 가장 기본이 될 수 있는 이론인 DEA(Data Envelopment Analysis)에 관해서 발표를 하게되었습니다. 
 DEA에서 기본이 되는 컨셉인 Production possibility set(생산가능집합), Free disposability(자유가처분성), Convexity (볼록성), CRS (Constant Returns to Scale, 불변규모수익), VRS(Variable Returns to Scale, 가변규모수익), Production frontier (생산 변경), Strongly efficient (강효율성), Weakly efficient (약효율성) 등을 설명하고 이를 토대로 CCR 모형과 BCC 모형에 대해서 발표를 진행하였습니다. CCR 모형과 BCC 모형은 단순히 CRS를 가정하느냐 VRS를 가정하느냐에 차이로 해석할 수 있고 본 연구인 Integrating Cluster Validity Indices based on DEA에서는 실제로 Input과  Output이Clustering Algorithm을 여러 지표로 산출한 Performance로써 임의로 조절할 수가 없기 때문에 실제로 DEA 연구의 주 목적인 Decision Making Unit (본 연구에서는 Clustering Algorithm)의 효율성을 개선하는 문제는 다룰 수가 없습니다. 하지만 본 연구에서는 DEA를 통해 Performance measure를 종합하여 global measure로서의 가능성을 살펴보는 연구이기때문에 위의 사항은 중요한 문제는 아니라고 보여집니다.]]></content>
		<date><![CDATA[20160114142921]]></date>
		<update><![CDATA[20160114142921]]></update>
		<view><![CDATA[28]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[6]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[Research Trend Analysis : Deep learning 2016-01-14]]></title>
		<content><![CDATA[금일 발표에는 Deep learning 관련 논문에 대한 키워드 정의를 위한 paper로써 Nature에 게재된  Deep Learning paper 를 간단하게 리뷰하였으며, 이를 통해 정의된 키워드는 다음과 같습니다.
-------------------------------------------------------------------------------------------
Deep learning, Deep neural network, Deep belief network, Convolutional neural network, Convolutional deep belief network, Deep restiricted boltzmann machine, Recurrent neural network, LSTM, RBM, Deep reinforcement learning, Deep Autoencoder, Deep Q-network 가 되겠습니다.
-------------------------------------------------------------------------------------------
 검색 기준은 위의 키워드를 기준으로 Title, Abstract, Author 기준으로 검색하여 사용하도록 정의 하였습니다.
 차주까지 모든데이터 수집을 완료하여 전처리까지 완료하는 것을 목표로 준비를 하도록 할 것 입니다.

감사합니다.]]></content>
		<date><![CDATA[20160114174157]]></date>
		<update><![CDATA[20160114174157]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[7]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[Document Classification based on Co-Training of LDA and Doc2Vec]]></title>
		<content><![CDATA[Document Classification based on Co-Training of LDA and Doc2Vec  주제의 목적은 multi target document classification 입니다. 하지만  label이 주어져 있는 데이터를 얻기 힘들 뿐더라 그 label을 얻기 위한 노력이 일반적으로 큽니다.  그래서 label 데이터가 10% ~ 90% 의 비율로 주어졌을 때 얼마나 성능의 차이를 보이는 지 실험계획을 가지고 있습니다. semi-supervised의 co-training의 특성을 적용하여 LDA example 과 doc2vec example로 K-NN, NB의 두개의 leaner을 만들어 document classification 을 진행하고자 합니다.]]></content>
		<date><![CDATA[20160114174358]]></date>
		<update><![CDATA[20160114174358]]></update>
		<view><![CDATA[27]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[8]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/14] Evaluating Variable Selection Techniques -1]]></title>
		<content><![CDATA[1주차에서는 먼저 사전 연구에 관한 literature review를 통하여 유사 논문이 있는지를 확인하였다. 그리고 supervised feature selection과 multiple linear regression에 알맞는 데이터를 수집하였으며 전반적인 실험 과정을 계획 및 정리해 보았다. 차후에는 수집한 데이터 셋을 random sampling하여 subset selection methods의 실험을 진행하고 ridge regression에 관한 이론을 공부하고자 한다.]]></content>
		<date><![CDATA[20160114225021]]></date>
		<update><![CDATA[20160114225021]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[9]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/18] 딥러닝 세미나]]></title>
		<content><![CDATA[오늘 세미나에서는 크게 MCMC 기법과 persistent CD에 대해서 살펴 보았습니다. MCMC는 마코프 체인의 stationary distribution을 형태로 바꾼 후 Monte Carlo의 시뮬레이션을 통해서 고차원의 데이터의를 샘플을 할때 사용됩니다. 이러한 방식에 사용되는 기법이 metropolis 와  gibbs 샘플링 방식이 있습니다. metropolis는 데이터가 주어졌을 때 모든 변수의 모수들의 확률 값을 고려하지만 gibbs 샘플링은 데이터와 다른  변수들의 모수가 주여 졌을 때 특정 변수의 모수가 나올 조건부 확률의 형태로 샘플링을 하게 되며 RBM 과 topic modeling 에서 이러한 방식이 사용되고 있습니다. 기존의 CD-K 는 training example들을 사용하여 gibbs sampling을 한후 restarting 반면에  persistent CD는 gibbs sampling이 된  negative example을 지속적으로  gibbs sampling을 하는 방식이며 일반적으로 좀더 좋은 추정치를 찾을 수 있습니다. visible layer 의 변수 타입이 real value 일 때 가우시안분포 의 형태를 가정하면 unbound된 값을  가질수 있습니다. BM은 weight를 업데이트를 하는것이  RBM보단 상대적으로 복잡하고 오래 걸리기 때문에  많이 사용되고 있지 않지만 이미지 분야에서는 visible layer는 BM의 형태로 connect 되어 있고 hidden layer는 RBM의 형태로 서로 연결되지 않은 semi-RBM이 사용되고 있습니다.]]></content>
		<date><![CDATA[20160118192413]]></date>
		<update><![CDATA[20160118192413]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[10]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/21][연구세미나]Enhanced Keystroke Dynamics Authentication]]></title>
		<content><![CDATA[발표자: 김준홍, 김해동

이번 발표는 김해동이 먼저 최근 두 달 이내에 발표된 Keystroke Dynamics Authentication (KDA)관련 논문을 찾아보고 흥미로운 것을 골라 발표하였습니다. 두 편의 논문을 소개하고 근래에 소개된 새로운 도메인을 사용한 KDA기법을 소개 하였습니다. 첫 번째 논문은 KDA에 Linguistic Context를 측정하는 변수를 추가하고 효과를 측정하였습니다. Linguistic Information을 포함함으로 ERR (Equal Error Rate)를 7.6% 포인트 정도 낮출 수 있었지만 극적인 효과는 없었습니다. 두 번째 논문은 데이터 pre-processing에서 자료변환을 통해 성능을 높일 수 있음을 보였습니다. 마지막으로 압력변수를 추가한 터치 스크린, 패턴잠금해제를 KDA로 분석하는 방법을 소개 하였습니다.
두 번째 발표에서 김준홍은 개발과 디버그가 완료된 Keystroke 수집 안드로이드 앱의 사용법을 소개하였습니다. 안드로이드폰의 특성상 제조사, 그리고 같은 제조사라도 기종에 따라 발생하는 버그가 조금씩 다른 어려움이 있었습니다. 하지만 가장 많이 사용되는 갤럭시와 베가 휴대폰에 대해서 디버그를 완료 하였기 때문에 실험에 큰 어려움을 없을거라 예상합니다. 성공적인 연구를 위해서 아웃라이어 처리에 집중하였습니다. 평균값을 사용하는 경우 아웃라이어에 영향을 많이 받을 수 있기 때문에 이에 대해 많이 고민 하였습니다.]]></content>
		<date><![CDATA[20160121221421]]></date>
		<update><![CDATA[20160121221421]]></update>
		<view><![CDATA[22]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[11]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/21] 논문 세미나 co-training lda+doc2vec]]></title>
		<content><![CDATA[오늘 세미나에서는 수집한 데이터를 어떤 목적으로 class를 선정하지와 실험계획을 구체적으로 논의를 해보았습니다. 반복실헙이 많은 만큼  작업을 하고 분석이 된 자료를 3명의 연구원이 혼동 되지 않게 ouput들을 DB로 공유할 계획이고 다음주까지는 LDA, Doc2vec의 input 형태인 corpus와 document의 형태로 preprocessing 할 예정입니다.]]></content>
		<date><![CDATA[20160122174040]]></date>
		<update><![CDATA[20160122174040]]></update>
		<view><![CDATA[12]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[16]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/21]연구세미나 Evaluating Variable Selection Techniques-2]]></title>
		<content><![CDATA[Ridge regression은 다중공선성있는 선형회귀 즉 예측값이 unbiased인 경우에 bias를 증가시켜 error를 줄이는 방법입니다. 수리적으는 least square estimates에 제약 조건식을 넣어 bias를 증가시킵니다. 조건식의 라그랑지안 승수 항 때문에 선형회귀 계수의 예측값을 구할 수 없는 경우 근사값을 구할 수 있게 되며 이때 선형회귀 계수의 예측값이 shrinking 하게 됩니다. Ridge regression은 설명력이 적은 독립변수의 선형회귀 계수 예측값을 더 감소시켜 변수 선택의 효과가 있으나 완전히 변수를 소거하진 않습니다.]]></content>
		<date><![CDATA[20160125093739]]></date>
		<update><![CDATA[20160125093739]]></update>
		<view><![CDATA[13]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[15]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[Integrating Cluster Validity Indices based on DEA (2016/01/21, 김보섭)]]></title>
		<content><![CDATA[금일 세미나에서는 Integrating Cluster Validity Indices based on DEA 연구에서 Hard clustering의 performance를 측정하는 measure중 Internal한 measure의 개념에 대해서 간략하게 언급하고 본 연구에서 활용할 measure를 선택하고 그 이유를 설명하였습니다. 또한 기존의 Clustering measure에 관한 연구에서 쓰이는 Gaussian 분포에 기반하여 생성된  Cluster에 대해서 global한 measure 없게되는 일이 발생하는 지에 대한 다섯가지 이유에 대해서 설명하였습니다. 이와 더불어 Gaussian 분포 뿐만아니라 기타 다른 분포에서 생성된 cluster에 대한 여러가지 Artificial Data를 수집한 과정을 보이고 DEA 연구상에 쓰일 Benchmarking package를 간략하게 발표하였습니다.]]></content>
		<date><![CDATA[20160124224939]]></date>
		<update><![CDATA[20160124224939]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[14]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/21][연구세미나] Reasearch Trend Analysis]]></title>
		<content><![CDATA[금일 연구세미나에서는 지난 주에 선정한 Deep Learning 키워드 관련 논문들을 scopus 와 arxiv.org 를 통해서 데이터를 수집하였으며, 그 전처리 과정에 대해서 발표를 진행하였습니다. 그 결과 journal data 와 conference papers 의 경우는 3,338개의 논문을 수집하였으며, unpublished papers 의 경우에는 1,682개의 논문을 수집하였습니다. 차주에는 향후 수행할 분석들에 대한 계획 수립 및 그 분석 기법들에 대한 이해를 바탕으로 실험에 임하고자 합니다.
이상입니다.
감사합니다.]]></content>
		<date><![CDATA[20160122180729]]></date>
		<update><![CDATA[20160122180729]]></update>
		<view><![CDATA[20]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[17]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/26][딥러닝 세미나]Autoencoder (발표자 : 김보섭)]]></title>
		<content><![CDATA[금일 딥러닝세미나에서는 Autoencoder에 대한 발표를 진행하였습니다. 기본적으로 Unsupervised learning으로서 Autoencoder는 지난 시간에 다른 restrict boltzman machine과는 달리 어떤 생성적 모델 생각하는 것이아닌 일반적인 feed-forward neural network의 학습방식과 같다는 점이 차이점입니다. 이를 통해 Autoencoder에서는 간단히 gradient descent의 식이 어떤식으로 전개되는지 증명하였고 Input layer에서 hidden layer로 갈 때의 활성함수, hidden layer에서 output layer로 갈 때 활성함수에 따라서 해당 Autoencoder가 binary input을 reconstruction하는 문제인지 real-valued input을 reconstruction하는 문제인지 설명하고 간단하게 svd를 이용해서 real-valued input에 대해서 autoencoder를 풀 때 두 활성함수를 모두 linear activation(Identity function)을 활용하는 것이 최적일 수 있음을 증명하였습니다. 또한 앞에서 다루는 autoencoder는 undercomplete한 경우에는 input의 정보를 compress한다는 개념을 설명하였고 만약 overcomplete일 경우, input의 정보를 compress ( feature extraction)을 하기위해서 행해지고 있는 방법인 denoising autoencoder와 contractive autoencoder에 대해서 둘의 개념와 차이점을 간략히 설명하였습니다.]]></content>
		<date><![CDATA[20160126001950]]></date>
		<update><![CDATA[20160126001950]]></update>
		<view><![CDATA[21]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[18]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2015.01.28] [연구세미나] co-training LDA &amp; Doc2vec (김동화)]]></title>
		<content><![CDATA[오늘 연구에서는 실험을 위해 아래와 같이 데이터들 정의 하였습니다
20newsgroup: 18788 document (20 class)
Economic: 7864 document (4 class)
Ohsumed: 23166 document (23 class)
Reuters: 8653 document (9 class)
stemming을 하게 될 경우 stemming 후 원래 단어의 형태로 돌아가지 못하는 경우가 대부분이었지만
Lemmatization으로 충분히 원래 단어를 표현하는 것이 가능하였습니다.
그리고 전처리과정중에 패키지를 쓴다고 무조건 좋은 것이 아니라는 사실을 알게 되었습니다.
텍스트데이터가 있을때 그 데이터의 특성(html, e-mail)을 이해하고 해당되는 문제를 해결하는 방식을 항상 강구해야 될 것 같습니다
앞으로 이 데이터를 TF-IDF에 가중치에 대한 기준으로 변수(=column, term,차원)을 줄여 하나의 example을 생성을 할것입니다.
다음주까지는 LDA, Doc2vec의 example들을 만들 예정입니다.]]></content>
		<date><![CDATA[20160128180417]]></date>
		<update><![CDATA[20160128180417]]></update>
		<view><![CDATA[12]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[19]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/28][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 Integrating Cluster Validity Indices based on DEA 연구에 활용되는 인공 데이터를 보강한 점과 연구에 사용될 Clustering Algorithm을 어떠한 Algorithm을 선정하였는지 발표하였습니다. 결과적으로 선택한 Algorithm 중에서 Graph 방법에 기반한 Algorithm을 보강하여야한다는 조언을 교수님께 받았고 인공데이터셋을 몇가지 수정할 것을 지시받았습니다.]]></content>
		<date><![CDATA[20160128200430]]></date>
		<update><![CDATA[20160128200430]]></update>
		<view><![CDATA[25]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[20]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/28][연구세미나]Keystroke DA]]></title>
		<content><![CDATA[1. 연구 설계와 계획
* 이번 연구를 Performance를 향상 시킬 수 있는 Input Variable 개발에 촛점을 둔다.
KDA에서 새로운 알고리즘을 개발하는건 매우 어려운 일이다. 따라서 본 연구에서는 우선적으로 Input Variable을 개발하여 성능을 향상시키는데 목적이 있다. 그러기 위해 먼저 이전에 키보드로 입력하고 웹을 거쳐 수집한 데이터에 EDA를 해보았다. Outliers를 제거하여 성능을 올릴 수 있을거라 가정하고 수행 한 EDA 였지만 색다른 결과가 나왔다. 일반적으로 사용하는 사분위수의 1.5 표준편차배로 Outlier를 제거하면 20% 가량의 데이터 손실이 있다. 너무 많은 데이터가 이상치로 분류된다. 따라서 Outlier가 쓰레기 정보가 아니라 의미있는 정보고이고 KDA에서 사용자의 입력시간 간 편차가 매우 큼을 알 수 있다. 따라서 이 방법은 차치하고  클러스터링 기법을 활용하여 Input Variable을 개발 할 예정이다.

2. 실험 설계와 진행상황
실험은 안드로이드 폰으로 진행된다. 현재 DSBA연구원들을 대상으로 1차 베타 테스트가 끝났다. 일반 시민을 대상으로 실험 참여 공모에 들어갔으며 현재까지 40명 가량이 실험에 참여 할 의사를 보였다. 실험은 2월 1일 월요일 부터 시작될 예정이다. 주말 기간 동안 홍보에 전력을 다해 70명 이상을 채우려 한다.]]></content>
		<date><![CDATA[20160128235115]]></date>
		<update><![CDATA[20160128235115]]></update>
		<view><![CDATA[21]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[23]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/28]연구세미나  Research Trend - Deep Learning]]></title>
		<content><![CDATA[이번주  Research Trend - Deep Learning 에서는 수집된 논문 문헌으로부터 저자의 소속기관을 통해 소속기관과 저자의 수를 EDA를 통해 파악해보았으며, 이를 통해 각 국가간의 Cowork 현황을 파악해 볼 수 있었습니다. 그 결과, 중국과 미국이 독보적으로 Deep learning 분야에서 많은 양의 저자활동을 진행해 오고 있음을 알 수 있었습니다. 또한 2013년 이후로 기하급수적으로 논문이 많이 쓰여지고 있는것을 파악가능 하였습니다. 한국은 Top10는 들지 못하는 13위권에 위치하여 아직은 다른 선진국에 비해 뒤쳐져 있음을 확인할수 있었습니다. 차주에는 이러한 Co-work 데이터를 통해 실제로 Network를 구현해보며, Title과 Abstract를 전처리 과정을 거쳐 벡터화 시키는 작업을 진행하고자 합니다. 앞으로 많은 관심과 조언 부탁드립니다. 감사합니다.]]></content>
		<date><![CDATA[20160130162139]]></date>
		<update><![CDATA[20160130162139]]></update>
		<view><![CDATA[14]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[22]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/28]연구세미나 Evaluating Variable Selection Techniques-3]]></title>
		<content><![CDATA[지난 세미나에서 ridge regression은 다중공선성 때문에 생기는 prediction error를 줄이기 위해 bias를 증가시키는 방법이었는데 Lasso는 ridge regression과 변수선택법의 효과가 합쳐진 것으로 ridge regression의 L2-norm 대신에 L1-norm을 써서 회귀계수 예측값이 0이 될 수 있게 합니다. 따라서 다중공선성과 많은 변수 때문에 생기는 predicrion error를 줄일 수 있습니다. 그리고 적접 coding을 하였더니 실험시간이 너무 길어져 data description과 실험결과를 미처 정리하지 못했는데 마저 채우지 못한 부분까지 다음 세미나에 발표하도록 하겠습니다.]]></content>
		<date><![CDATA[20160130060023]]></date>
		<update><![CDATA[20160130060023]]></update>
		<view><![CDATA[12]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[24]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/02/01][딥러닝 세미나] DeepLearning1 (발표자 : 김준홍)]]></title>
		<content><![CDATA[금일은 HL 7.1~7.5 부분인의 Deep learning 1 부분을 발표 하였습니다.

[1] Deep learning의 학술적 정의

[2] 사람의 구조와 Neuralnet의 구조의 비교

[3] Application Area

[4] Training의 한계점

[5] Pre-training

[6] pre-training의 성능에 대한 연구 논문 요약

[7] Dropout과 간단한 예시를 발표 하였습니다.]]></content>
		<date><![CDATA[20160201130805]]></date>
		<update><![CDATA[20160201130805]]></update>
		<view><![CDATA[23]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[25]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/02/04][연구세미나]Keystroke DA]]></title>
		<content><![CDATA[오늘은 Keystroke 연구 결과에 대한 보고를 하였습니다.

[1] Key Digraph의 통계량에 따른 150명이 전부 있는 경우만 이용하였으며, 
     최종 Preprocessing에 대하여 보고 하였습니다.

[2] R A K-S C-M Model에 대한 5가지 Input 변수 Unigraph 1가지 Digraph 4가지에 대한 결과를 보였습니다.

[3] Train과 Validation의 기존 방식에서 공정하지 않은 부분에 대한 것을 토의하였고 새로운 방향을 잡았습니다.

[4] EER산출법과 A Measure의 Cut off parameter Search에 대하여 발표 하였습니다.

[5] 자료가 많이 모이면 Classification 방법으로 모델링을 하는 방향에 대하여 토론하였습니다.]]></content>
		<date><![CDATA[20160204131448]]></date>
		<update><![CDATA[20160204131448]]></update>
		<view><![CDATA[28]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/1/201602/56b2d038687f49159271.pptx]]></thumbnail_file>
		<thumbnail_name><![CDATA[Keystroke_Semina_1_20160204_2.pptx]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[26]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/01/28][연구세미나]Evaluating Variable Selection Techniques-4]]></title>
		<content><![CDATA[- 데이터가 클 경우 다중공선성이 생기게 되며 이로 인해 변수 간 correlation이 발생한다. lasso는 변수들의 correlation과 무관하게 변수를 선택하지만 elastic net은 correlated variable groups도 선택할 수 있습니다(grouping effect).
또한 독립변수가 데이터 샘플 수보다 많을 경우 lasso는 최대 샘플 수만큼만 변수 선택을 할 수 있는 반면 elastic net는 최대 독립변수 개수만큼 변수를 선택할 수 있습니다.
이러한 elastic net의 특징은 ridge와 lasso의 제약식을 합친 것으로 볼 수 있고 lasso 제약식처럼 좌표축 위에서 미분할 수 없고 ridge처럼 축 위가 아닌 점들에서 strictly convex하기 때문에 나타납니다.
 - 지금 까지는 subset selections로 변수 선택까지 실험을 하였고 앞으로는 변수선택율이나 에측 오차 등을 이용하여 변수 선택법들을 어떻게 비교할 수 있을지 고민하고자 합니다.]]></content>
		<date><![CDATA[20160204142301]]></date>
		<update><![CDATA[20160204142301]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[27]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[2016/02/04][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 Integrating Cluster Validity Indices based on DEA 연구에 활용되는 인공 데이터를 몇가지 수정 점과 Well-separated case 경우의 Data로 초기실험한 결과를 선보인 자리였습니다. 초기 실험은 계획에서 선정한 clustering performance index 들 중 두 가지를 빼고 진행을 하였습니다. input에 들어가는 index 중 하나인 SD Validity index는 연구자가 정해야할 parameter가 존재하고 조사결과 여러가지 다른 방식으로 parameter를 결정하고 있었기에 다른 index로 대체할 지에 관하여 토의하였습니다. 
또한 Output에 들어가는 index 중 하나인 I index의 경우는 다른 논문에서 performance index로써 많이 활용하지않는 듯하여 해당 index에 대해서도 토의를 한 자리였습니다. 연구자로서 이 연구의 의의가 무엇이냐 헷갈렸던 점이 있는데 교수님에게 이 연구가 가질 수 있는 의의에 대해서 말씀해주어 앞으로의 실험설계를 좀 더 명확히 설계하여 진행할 수 있을 것 같습니다.]]></content>
		<date><![CDATA[20160205082814]]></date>
		<update><![CDATA[20160205082814]]></update>
		<view><![CDATA[17]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[28]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[dsba_admin]]></member_display>
		<title><![CDATA[[02/04] Co-training [김동화]]]></title>
		<content><![CDATA[오늘 세미나에 대해서는 실험에 여러가지 문제점에 대해서 논의 해 보았습니다.

Document-Term  Matrix의 고차원 문제로 각 class 들의  누적 TF-IDF 상위 랭크 Term들을 사용할 예정입니다.
(20,50,70 차원)

LDA의 학습속도도 수렴하기 위해 굉장히 오래걸리기 떄문에 이부분에 대해서 모든 데이터에 대래서 확률분포를 계산한후 partition을  할 예정입니다.
(20,50,70 차원)

Doc2vec의 경우도 LDA보다 더 오랜 학습시간이 거리며 위와 같은 파티션으로 진행할 예정입니다.
 (20,50,70 차원)

 20,50,70 차원 별로 같은 차원에서 어떠한 방식이 더 효울적인지 다음주 까지 비교해 보고 차 후 semi-supervised를 진행할 것입니다.]]></content>
		<date><![CDATA[20160205233131]]></date>
		<update><![CDATA[20160205233131]]></update>
		<view><![CDATA[14]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[33]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[[2016/02/04][연구세미나] Reasearch Trend Analysis]]></title>
		<content><![CDATA[금일 세미나에서는 지난주 발표에서의 변경사항과 추가할 내용에 대한 발표를 하였습니다.
년도별 논문수 그래프에서 1994년부터 2008년까지를 한 범주로 묶었고, Review Document-&gt;Article Document 하는 방향으로 빈도수가 적은 Review문서를 Article에 포함시켰습니다. 
Co-work bubble chart에 대한 그림을 추가하였고, 다음주 까지 y축과 bubble의 크기를 변경하는 작업을 진행하도록 하겠습니다.
또한 국가간의 Co-work를 시각화하여 볼 수 있는 network 그림을 그려보았습니다.
차주에는 Title과 Abstract data를 이용하여 Topic modeling를 진행할 예정입니다.
앞으로 많은 관심과 조언 부탁드립니다. 감사합니다.]]></content>
		<date><![CDATA[20160210221644]]></date>
		<update><![CDATA[20160210221644]]></update>
		<view><![CDATA[12]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[32]]></uid>
		<board_id><![CDATA[4]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016.1.22~ 2016.1.23 동계워크샵[하이원리조트]]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160206230058]]></date>
		<update><![CDATA[20160430164108]]></update>
		<view><![CDATA[11961]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/4/201602/56b5fcb4e1be08297393.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[workshop.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[34]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/02/11][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 Integrating Cluster Validity Indices based on DEA 연구의 초기 test의 결과를 선보이는 자리였습니다. 먼저 "well-separated" data의 경우는 군집이 가우시안분포에서 나온 모양의 형태를 띄고 있기 때문에 해당 dea연구가 의도했던 바가 명확히 보여 의미있는 실혐결과 였습니다. 다만 "Jain" case의 경우 가우시안분포에서 나온 형태가 아니었는데 이 경우 dea가 의도된 바와 같이 작동하진않았습니다. 따라서 추가적으로 paper를 통해서 dea 연구에 쓰이는 cluster indice에 대해서 더 조사하고 인공데이터를 좀 더 수정하여 "Jain" case와 같은 경우의 데이터에 대해서도 dea의 효율성값에서 어떠한 의미를 도출해낼 수 있는 지 탐색할 계획입니다.]]></content>
		<date><![CDATA[20160211134444]]></date>
		<update><![CDATA[20160211134444]]></update>
		<view><![CDATA[17]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[39]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2016/02/11][연구세미나] Reasearch Trend Analysis]]></title>
		<content><![CDATA[이번 세미나에서는 network visualization 결과, 국가별 cowork 그림 수정 사항, 토픽 모델링 prototype 결과에 대해 발표하였습니다. 다음 세미나에서는 토픽 모델링 수정 결과에 대해서 발표할 계획입니다.]]></content>
		<date><![CDATA[20160217125144]]></date>
		<update><![CDATA[20160217125144]]></update>
		<view><![CDATA[13]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[40]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/02/11][연구세미나]Evaluating Variable Selection Techniques-5]]></title>
		<content><![CDATA[이번 세미나에서는 지금까지 실험한 forward, backward, stepwise subset selection의 결과를 정리해 보았습니다. 변수 선택 전후 예측 오차의 차이가 크지 않았지만 선택된 설명변수 수가 적을 경우 성능이 예측 성능이 좋아졌다고 할 수 있습니다.]]></content>
		<date><![CDATA[20160218002923]]></date>
		<update><![CDATA[20160218002923]]></update>
		<view><![CDATA[12]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[41]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2016/02/17][연구세미나]Keystroke DA]]></title>
		<content><![CDATA[본 팀이 당면한 가장 중요한 문제점은 연구의 구체적인 Contribution을 정하는 일 이었다. 연구 Flow를 구체화 하기 위해 팀 내에서 고민해 본 결과 아래와 같은 세가지 연구 아이디어를 모았다.
[1] Training으로 사용하는 5000 Keystroke의 Time Latency를 내림차순으로 정렬한 후 8개의 구간으로 쪼개서 각각 변수로 사용
[2] 한국어와, 영어와 같은 각 언어별 전체 Keystroke를 Time Latency의 내림차순으로 정리한 후 8가지 변수를 생성
[3] 기존의 KS, CM, R, 그리고 A Measure를 Input변수롤 사용하여 One class classification에 투입함
이중에서도 [1]과 특히 [3]을 집중적으로 탐구해볼 예정이다.

P.S. 기존에 연구하고 있던 Clustering 을 활용하는 방법은 Sample이  하나도 되지 않아 Modeling 자체가 불가능한 경우가 생겨서 현재는 잠정 보류 상태이다.]]></content>
		<date><![CDATA[20160218160119]]></date>
		<update><![CDATA[20160218160119]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[37]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2016/02/11][연구세미나] Keystroke]]></title>
		<content><![CDATA[[1] Data를 생성하는 방법을 교수님의 도움을 받아 좀 더 명확히 하였다.  

[2] 본 연구의 contribution을 좀 더 명확히 해야 한다.
    2.1 현재 생각해 놓은 후보군으로는 Machine Learning기법을 적용할 때 Outlier를 제거하고 Average대신 Median을 
          사용해볼 예정이다.
    2.2 Input Structure의 Layout을 변형시킨 후 실험해 볼 예정이다.]]></content>
		<date><![CDATA[20160211225108]]></date>
		<update><![CDATA[20160211225108]]></update>
		<view><![CDATA[21]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[38]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[[2016/02/11] [연구세미나] co-training 발표자 : 김동화]]></title>
		<content><![CDATA[이번 세미나에서는  라벨링 비율를 다르게 한 supervised learning에 대해서 결과를 살펴보았습니다.
클래스  사전확률이 다를 경우 KNN의 컷오프를 바꿔줘야 하는 내용을 반영하여 다음 세미나 시간까지 정리할 예정입니다. 시각화 관련해서도 좀 더 고민해 볼것이고, self-training 에 관한 결과를 다음주까지 보여드겠습니다.]]></content>
		<date><![CDATA[20160215203732]]></date>
		<update><![CDATA[20160215203732]]></update>
		<view><![CDATA[9]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[42]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[형석 김]]></member_display>
		<title><![CDATA[[2016/02/17][연구세미나]Research Trend Analysis]]></title>
		<content><![CDATA[금일 세미나에는 한주간 여럿 반복실험을 통해 stop-words를 제거하여 총 20, 30개의 토픽을 통해 토픽모델링을 진행 하였으며, 추출된 키워드를 통해서 네이밍 작업을 진행하였습니다. 또한 이러한 연도별 분포를 통해서 토픽들의 시간에 따른 변화량을 알아보았습니다. 이 과정에서 딥러닝 분야의 발생하는 논문의 양이 최근들어 급속히 많이 발생함에 따라, 그 절대적인 수치를 통해 비교하기 보다는 연도별 발생량을 고려한 토픽들의 비율을 통해 알아보면서, 주목받는 토픽과 그렇지 않은 토픽 문서들을 비교할수 있었습니다. 다음주에는 이러한 토픽간의 네트워크를 구성해보고, 시간에 따른 변화량을 통계적으로 검증하여 Hot topic과 Cold topic을 통해서 확인해보고자 합니다.
이상입니다. 감사합니다.]]></content>
		<date><![CDATA[20160219005010]]></date>
		<update><![CDATA[20160219005010]]></update>
		<view><![CDATA[17]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[43]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[김 동화]]></member_display>
		<title><![CDATA[[2016/02/18] 논문 세미나 Co-training]]></title>
		<content><![CDATA[오늘 세미나에서는 지도학습, 자가학습, 멀티 공동 학습에 대해 비교 해 보았다.
이 논문주제 결과에 대한 요약을 하자면 
적은 라벨 비율을 가지면 자가학습이 제대로 학습이 되지 않았는데
멀티 공동 학습의 경우에는  적은 라벨을 가지더라도 라벨이 많은 때 와 유사한 결과를 보였다.]]></content>
		<date><![CDATA[20160219154743]]></date>
		<update><![CDATA[20160219154743]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[44]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/02/18][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 Integrating Cluster Validity Indices based on DEA 연구의 third test의 결과를 선보이는 자리였습니다. 지난 test의 "Jain"을 "Two moon" dataset으로 교체하여 진행하였고 어느 정도 예상했던 결과가 나왔습니다. "Flame" dataset의 결과도 예상했던 결과를 얻었습니다. 현재까지 실험을 진행하면서 DEA로 만든 효율성 지표를 해석하자면 해당 효율성 지표는 결국 Input, Output에 들어가는 cluster indices가 구형의 cluster에서 잘 작동하기 때문에 실제로 구형의 군집이 아닌 2개인 군집을 가진 데이터에 대해서도 해당 군집을 구형(or 타원)의 형태로 잘쪼개어간다면 효율성이 1이 나올 수 있음을 확인하였습니다. 따라서 앞으로는 cluster indices에 관한 paper들을 읽고 Input, Output에 들어가는 cluster indices들의 조합과 새로운 cluster indice를 조사할 예정입니다.]]></content>
		<date><![CDATA[20160219164423]]></date>
		<update><![CDATA[20160219164423]]></update>
		<view><![CDATA[10]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[45]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[형석 김]]></member_display>
		<title><![CDATA[[2016/02/17][딥러닝 세미나] Deep Learning 2]]></title>
		<content><![CDATA[금일 발표에는 Deep Auto-encoder의 효과에 대한 간략한 소개와 Deep Belief Network에 대해서 다루도록 하였습니다. DBN은 기존의 DNN(Deep Neural Network)과는 다른 확률모델이다. DBN의 경우 상단의 은닉층으로부터 데이터까지 순차적으로 연산되는 top-down 그래프인 반면, FNN은 데이터로 부터 상단의 출력층까지 순차적으로 연산되는 bottom-up 그래프로 그 구조 자체가 다르다. 은닉 노드들은 통해서 층간의 인과관계를 설명하는 인과 그래프로 구성된다. DBN 추론방식은 가시층의 log likelihood를 직접 구할수 없으니, 이를 최대한 근사한 q(h|v)를 정의하고 그 로그우도의 lower bound를 구하고,  그 lower bound를 최대화 하는 parmeter를 찾아가는 작업을 통해서 은닉노드간의 w를 찾을수 있다. 하지만 DBN 자체가 학습에 있어서 한층 한층 사이에서의 단계에서는 RBM의 학습과 동일한 방식으로 W를 학습하며 이러한 과정을 반복적으로 수행함으로 학습이 진행되므로 학습속도가 매우 더딘 단점을 가지고 있다고 생각된다.]]></content>
		<date><![CDATA[20160219170302]]></date>
		<update><![CDATA[20160219170302]]></update>
		<view><![CDATA[26]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[48]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[김 동화]]></member_display>
		<title><![CDATA[[2015.02.29] [연구세미나] co-training LDA &amp; Doc2vec (김동화)]]></title>
		<content><![CDATA[오늘 세미나에서는 전반적인 결과에 대해서 보여드렸습니다. NB의 learner 의 경우가 라벨 비율에 대한 BCR이 덜 민감하였고 self-training 의 단일 반 지도학습에서는 LDA 가 높은 성능을 보였습니다. multi-co-training 의 장점을 상대적으로 덜 민감하고 괜찮은 정확도를 가지고 있습니다.]]></content>
		<date><![CDATA[20160229222425]]></date>
		<update><![CDATA[20160229222425]]></update>
		<view><![CDATA[16]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[49]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2016/02/29][연구세미나]Keystroke DA]]></title>
		<content><![CDATA[이번주차 수행한 실험 내용은 저번주 발표에서 계획했던 실험 중 두 가지를 실행 하였고 새로 Idea를 내서 한 가지 실험을 추가로 더 수행 하였다. 내용을 정리하면 아래와 같다.

[1] KS와 MS Measures를 더하기, 그리고 곱하기로 혼합한 지표로 실험읗 해 본 결과 낮은 Keystroke 수에서 항상 다른 지표들 보다 좋은 성능을 발휘했다.
[2] 특이한 범은 영어에서는 주로 KS*MS가 가장 우수한 성능을 보였지만 한글에서는 KS+MS가 대체로 더 좋은 결과를 주었다,
[3] Gaussian과 Parzen 방법으로 실험하였다.
[4] Training data set의 Size가 커질수록 되려 성능이 비슷하거나 안좋아지는 경향이 드러났다. 특정한 고정된 Data set에서 Bootstrap을 너무 여러번 하였기 때문이 아닌가 추측한다.
[5] Reference Size에 따라 가장 적합한 방법론을 분류하고 정리하면 상용화에 도움이 될 것이다.

다음 주차에는 기계학습 방법들을 적용해볼 예정이다.]]></content>
		<date><![CDATA[20160229235515]]></date>
		<update><![CDATA[20160229235515]]></update>
		<view><![CDATA[23]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[47]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/02/26][딥러닝 세미나] Sparse Coding]]></title>
		<content><![CDATA[Sparse coding은 unsupervised 학습을 하는 neural networks 중 하나로 over-complete bases를 이용하여 데이터를 효과적으로 표현하기 위해 사용되며 변수 추출에 주로 사용됩니다. 즉 output x ̂은 x의 features의 linear combination으로 표현되며 input과 output의 오차를 최소화하도록 하는 것을 목적으로 합니다. 또한 features는 over-complete이기 때문에 L1 norm term을 더하여 representation을 최대한 ‘sparse’하도록 합니다. Sparse coding을 preprocessing 할 때에는 decorrelation을 하지 않기 위해 ZCA whitening을 사용합니다.
 학습을 시작할 때에는 basis vectors를 임의로 설정하여 선형 계수를 학습하고 다시 basis vectors를 학습합니다. 이를 반복하여 local optima를 구할 수 있으며 coordinate descent를 사용하면 학습률을 조절할 수 있습니다.]]></content>
		<date><![CDATA[20160227205858]]></date>
		<update><![CDATA[20160227205858]]></update>
		<view><![CDATA[26]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[50]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/02/29][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 Integrating Cluster Validity Indices based on DEA 연구의  fourth test의 결과를 선보이는 자리였습니다. 먼저 이전의 test와 다른 점은 DEA의 Input, Output indices을 대폭 줄여 Input에는 Xie_beni, Output에는 Dunn index만 사용한 점입니다. 이를 토대로 네 가지의 Data "Well-separated", "two moon", "tri moon", "Flame" dataset에 적용시켜본 결과 DEA의 eff가 원하던 대로 최적의 구조를 찾아냈습니다. 근데 heuristic하게 찾아낸 Input, Output이므로 좀 더 연구를 논리적으로 진행시킬 필요성이 있습니다. 다만 Xie-beni, Dunn이 eff measure와 같은 성향 (방향성이 정확히 일치)을 보여 좀 더 고찰이 필요해보입니다.]]></content>
		<date><![CDATA[20160301152044]]></date>
		<update><![CDATA[20160301152044]]></update>
		<view><![CDATA[22]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[51]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2016/02/29][연구세미나]Deep Learning Research Trend Analysis]]></title>
		<content><![CDATA[금일 세미나에는 문서 타입(conference, journal, arxiv)별로 Hot cold topic 그래프 표현 방법에 대한 발표와, 20개와 30개의 토픽에서 토픽 간의 네트워크가 어떻게 구성되어 있는지 시각화 한 자료를 보여드렸습니다. 한 plot에 여러 종류의 문서 타입을 보여주는 것 보다 문서 개별적으로 보여주는 것이 더 좋다고 생각하여 수정하기로 하였습니다. 토픽 네트워크에 대해서는 토픽을 군집화 하여서 보여드릴 계획입니다.

이상입니다, 감사합니다.]]></content>
		<date><![CDATA[20160301221428]]></date>
		<update><![CDATA[20160301221428]]></update>
		<view><![CDATA[13]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[52]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/02/29/연구세미나]Evaluating Variable Selection Techniques-6]]></title>
		<content><![CDATA[지금까지 총 7가지 변수 선택법 중 6가지 실험을 마쳤습니다. 진행된 실험 결과, 사용된 데이터에 대해서 차원 축소 후에도 예측 오차의 변화가 거의 없음을 알 수 있었습니다. Shrinkage methods(lasso, ridge regression, elastic net)를 비교하였더니 ridge는 선형계수들이 shrink 되었고, lasso의 경우 변수 선택과 shrinkage를 보였으며, elastic net은 ridge와 lasso의 두 특징과 grouping effect를 보였습니다.]]></content>
		<date><![CDATA[20160302163209]]></date>
		<update><![CDATA[20160302163209]]></update>
		<view><![CDATA[13]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[53]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/03/04][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 Integrating Cluster Validity Indices based on DEA 연구의 fifth test의 결과를 선보이는 자리였습니다. 이전의 test와 다른 점은 Input, Output에 들어가는 index들을 convex, non-convex로 나누어 집어넣고 실험한 것입니다. Input에는 Xie-beni, Comp_Sepa를 사용하였고, Output에는 Dunn, Dunn_mst를 사용하였는데 각각의 index 모두 subcluster구조에는 취약한 단점이 있습니다. 이는 Aggregation Data(subcluster 구조 존재)에서 확인할 수 있었습니다.
 새로운 실험계획으로 지표전체를 사용하는 것이 아닌 각각 지표를 구성하는 compactness 부분을 Input에 넣고  Separation 부분을 Output에 넣어 DEA 연구를 계속 진행해보려고 합니다.]]></content>
		<date><![CDATA[20160304135847]]></date>
		<update><![CDATA[20160304135847]]></update>
		<view><![CDATA[16]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[54]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/03/04][연구세미나]Evaluating Variable Selection Techniques-7]]></title>
		<content><![CDATA[금일 세미나에서는 error rate improvement와 variable reduction rate로 변수선택법의 performance이 어떻게 나타나는지에 대한 실험 결과를 발표하였습니다. 실험 전 예상과 달리 elastic net과 lasso는 비교적 낮은 성능을 보였으며 stepwise selection이 가장 좋은 성능을 보임을 확인하였습니다. 실험 결과를 분석하고 그것을 시각화하는 데에 어려움을 느꼈으며 논문이나 다른 자료를 더 공부해야 겠다고 생각했습니다.]]></content>
		<date><![CDATA[20160304145224]]></date>
		<update><![CDATA[20160304145224]]></update>
		<view><![CDATA[15]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[55]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2016/03/04][딥러닝 세미나] Introduction to deep learning]]></title>
		<content><![CDATA[오늘 딥러닝 세미나에서는 Nando de Freitas 교수님의 강의내용을 바탕으로 딥러닝에 대한 소개 내용을 발표하였습니다. 인공신경망의 역사, deep neural network를 훈련시키기 위한 방법, Deep Belief Network, Autoencoder, Convolutioinal Neural Network, Recurrent neural networks, Deep reinforcement Learning 등에 대한 내용을 발표하였습니다. 또 Tensorflow에 대한 간략한 소개를 하였습니다. Tensorflow를 직접 설치하고 구동하는 경험을 통해서 쉽지 않다는 것을 느꼈는데 앞으로 Tensorflow가 중요해질것이라 생각하기 때문에 연구실원들도 함께 Tensorflow 공부를 시작했으면 하는 바램입니다.]]></content>
		<date><![CDATA[20160304153123]]></date>
		<update><![CDATA[20160304153123]]></update>
		<view><![CDATA[16]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[56]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[2016/03/04][연구세미나]Keystroke DA]]></title>
		<content><![CDATA[오늘은 Imposter Training Set이 필요한 경우의 One classification Algorithm인

1-SVM, k-NN, K-means를 이용한 방법론을 행하기 위해 Data Partition과 Modeling을 실행 하였다는것을 

보고 하였습니다. 해당 방법론을 하기 위해서는 상당한 시간이 소요되었습니다.

k-NN의 경우 Fast Algorithm으로 KD-Tree를 이용하여 시간을 단축하였습니다.

다음시간까지, Meta Learning을 활용한 방법까지 정리하여

발표 초안을 완성 시킬 생각입니다.]]></content>
		<date><![CDATA[20160304173555]]></date>
		<update><![CDATA[20160304173555]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[57]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[형석 김]]></member_display>
		<title><![CDATA[[2016/03/04][연구세미나] Research Trend]]></title>
		<content><![CDATA[금일 발표에는 기존 Topic들의 발행 출처별 Hot &amp; Cold를 선형성검정을 통해 알아보았으며, Topic간의 유사도를 통해서 Hierarchical clustering을 통해 최적의 군집을 파악해보았습니다. 하지만 직관적으로 이해가 가지 않는 부분이 발생하여 이점을 추후에 다른 클러스터링 기법을 통해서, 납득이 갈만한 군집을 찾아보도록 하겠습니다.]]></content>
		<date><![CDATA[20160304205525]]></date>
		<update><![CDATA[20160304205525]]></update>
		<view><![CDATA[20]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[58]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[김 동화]]></member_display>
		<title><![CDATA[[2015.03.04] [연구세미나] co-training LDA &amp; Doc2vec (김동화)]]></title>
		<content><![CDATA[이번세미나에서는 차원, 라벨비율,  학습알고리즘,  실험방법등의 결과를 어떻게 보여줄지 시각화하는 방법들을 보여드렸습니다.]]></content>
		<date><![CDATA[20160310230049]]></date>
		<update><![CDATA[20160310230049]]></update>
		<view><![CDATA[15]]></view>
		<comment><![CDATA[2]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[59]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/03/11][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 Integrating Cluster Validity Indices based on DEA 연구의 산업공학회 발표자료 초안의 결과를 선보이는 자리였습니다. 발표자료 초안의 완성도가 부족하여 여러가지 수정사항을 전달 받았습니다. 또한 향후 UCI dataset에 대해서 실험설계에 관한 방향성을 들었습니다. 초안을 보강하여 제출할 수 있도록 하겠습니다.
(해당 파일은 제출한 파일입니다. 추후 발표에 활용할 수 있도록 코멘트 부탁드립니다.)]]></content>
		<date><![CDATA[20160316144308]]></date>
		<update><![CDATA[20160316144308]]></update>
		<view><![CDATA[13]]></view>
		<comment><![CDATA[2]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[68]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/03/11][연구세미나]Evaluating Variable Selection Techniques for Linear Regression]]></title>
		<content><![CDATA[이번 세미나는 춘계산공학회에 제출할 발표자료 초안에 대해 발표를 하는 시간이었습니다. 발표 자료를 준비하면서 기존에 연구를 진행하면서 깨닫지 못하고 있었던 부분도 정리할 수  있었고 부족한 부분에 대한 피드백을 받을 수 있어서 남은 연구를 하는데에 많은 도움이 될 것 같습니다. 첨부된 발표자료는 세미나 이후 피드백을 반영하여 수정한 자료입니다. 작은 의견이라도 남겨주시면 다음 학회 발표에 도움이 될  것 같습니다.]]></content>
		<date><![CDATA[20160316160419]]></date>
		<update><![CDATA[20160316160419]]></update>
		<view><![CDATA[11]]></view>
		<comment><![CDATA[2]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[69]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[형석 김]]></member_display>
		<title><![CDATA[[2016/03/18][연구세미나]Research Trend - Deep Learning]]></title>
		<content><![CDATA[금일 세미나에서는 산공학회의 발표 피티에 대한 점검을 받았습니다.
 수정할 사항으로는 다음과 같습니다.

1. 각 토픽에 대한 모두 보여주기보다는 HOT 과 Cold를 구분지어 기울기를 기준으로 정리 및 해석
2. 가독성을 위한 글자 크기 변경
3. 의미없는 TF-IDF 대신 Stopwords 추가
4. 그림은 되도록 하단부 위치
5. IOT와 연관지어 서술부분 추가
6. 캐나다에 대한 강조~

 위와같은 사항을 보완하여 다음시간에는 좀더 완성도 높은 발표자료를 제시하도록 하겠습니다.
이상입니다.]]></content>
		<date><![CDATA[20160318164359]]></date>
		<update><![CDATA[20160318164359]]></update>
		<view><![CDATA[14]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[70]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2016/03/18][연구세미나] Keystroke]]></title>
		<content><![CDATA[발표자료에 대해 오늘 교수님으로 부터 받은 피드백은 아래와 같습니다.

[1] 연구동기(3쪽)와 데이터 수집 및 방법론(4쪽) 사이에 과거의 연구결과를 한 장에서 두 장의 분량으로 추가
 -&gt; 우리 연구가 처음이 아니라 다른 연구가 이미 있었다는 점을 알려주기 위한 형식의 문제

[2] 5쪽 - Mapping table을 삭제하고 두 개의 Keystroke 일원분석 그래프를 더 크게 만들어라

[3] 8쪽 - Gaussian density estimation 항목에 Gaussian 분포 그래프 삽입

[4]  9쪽 - FAR, 그리고 FRR이 뭔지 한글로 간략히 설명해 주어라

[5]  실험1 -&gt; Distribution or heuristic based model 
      실험2 -&gt; Machine learning based model 또는 One-classification based model로 이름 변경

[6] 12~13쪽 - 2015년 Information Science 지에 게재된 교수님의 논문 (Kang, Cho, Keystroke dynamics-based user~)의 결과를 인용해서 얼마만큼 더 좋아졌는지 상단의 박스에 한 꼭지 더 추가하라

[7] 24쪽~25쪽[6]과 마찬가지로 이전 교수님이 논문에 비해 좋아진 비율을 명시

이외에도 추가로 해주실 말이 있으시면 어떤 코멘트라도 감사하겠습니다.
주말 잘 보내세요.]]></content>
		<date><![CDATA[20160318171226]]></date>
		<update><![CDATA[20160318171226]]></update>
		<view><![CDATA[17]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[71]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/03/18][연구세미나]Evaluating Variable Selection Techniques for Linear Regressio]]></title>
		<content><![CDATA[저번 세미나에 이어산업공학회 발표자료 관련하여 받은 피드백과 수정사항은 다음과 같으며 첨부된 자료는 다음을 반영하였습니다.

1) 사전 연구 다음 연구 목적을 발표
2) 실험설계의 데이터 설명 보충
       - 데이터 소스, 전체 데이터에서 변수와 관측치의 최대최소 개수
3) 실험 결과 평가에서 시간효율성과 관련한 질문 대비
4) 실험 결과를 자세히 설명할 수 있도록 발표 준비

예상질문이나 코멘트가 있으면 남겨주시기 바랍니다.]]></content>
		<date><![CDATA[20160319211412]]></date>
		<update><![CDATA[20160319211412]]></update>
		<view><![CDATA[15]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/1/201603/56ed42947e79f7335815.pdf]]></thumbnail_file>
		<thumbnail_name><![CDATA[2016춘계공동학술대회 - 류나현(고려대).pdf]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[72]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[김 동화]]></member_display>
		<title><![CDATA[[2016/03/18] 논문 세미나 multi-co-training]]></title>
		<content><![CDATA[오늘 세미나에서는 연구결과에 대해서 어떤 결과가 무엇인지 명시적으로 작성하는 부분이 미흡한점을 알았고, 발표에 있어서 표현하는 방식들을 알수 있게 되는 시간이었습니다.]]></content>
		<date><![CDATA[20160320230706]]></date>
		<update><![CDATA[20160320230706]]></update>
		<view><![CDATA[12]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[73]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[김 동화]]></member_display>
		<title><![CDATA[2016.03.18 딥러닝 세미나 [Maximum Likelihood]]]></title>
		<content><![CDATA[이번 세미나에서는 Maximum Likelihood 관해서 세미나를 진행 하였습니다.
Maximum Likelihood 와 Expectation Likelihood의 비교,
베이지안의 사전확률, 우도의 관계를 살펴보았고,
Variational inference의 Kull-leiber divergence를  접근해보는 시간이 되었습니다.]]></content>
		<date><![CDATA[20160320235806]]></date>
		<update><![CDATA[20160320235806]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[74]]></uid>
		<board_id><![CDATA[4]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016.3.18 단체사진 촬영]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160321234246]]></date>
		<update><![CDATA[20160430163854]]></update>
		<view><![CDATA[10191]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/4/201604/571472b48ab7a9462493.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[56f00866527bf9179321.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[75]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[우분투 14.04에서 한/영키 사용방법]]></title>
		<content><![CDATA[우분투 14.04에서는 한/영키 전환 버그가 있어서 우분투를 처음 설치하면 한글 타이핑을 할 수 없습니다.

우분투 터미널 창을 열고(Ctrl+Alt+T) 다음 명령어를 입력해 주시면 한영 전환키를 사용할 수 있습니다.

gsettings set org.gnome.desktop.wm.keybindings switch-input-source "['Hangul']"]]></content>
		<date><![CDATA[20160323155424]]></date>
		<update><![CDATA[20160323155424]]></update>
		<view><![CDATA[33]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[true]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[76]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/03/18][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 인공데이터가아닌 실제데이터에 적용하는 계획에 대한 조언을 들었습니다.
앞으로 세미나에서 실제데이터셋의 결과를 반영하여 장표를 다시 작성할 계획입니다.
기존 장표의 수정사항으로는 지표마다 최적의 클러스터의 개수와 그것이 아닌경우에 score에 tie가 있을 경우
해당 지표는 파악하지 못한 걸로 정의했으나 앞으로의 장표에서는 해당 경우에도 최적의 개수는 파악한 것으로 정의할 계획입니다.]]></content>
		<date><![CDATA[20160323230504]]></date>
		<update><![CDATA[20160323230504]]></update>
		<view><![CDATA[11]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[78]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 03. 22 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160324103701]]></date>
		<update><![CDATA[20160324103701]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[79]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 03. 24 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160324160604]]></date>
		<update><![CDATA[20160324160604]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[80]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/03/25][딥러닝 세미나]Backpropagation]]></title>
		<content><![CDATA[금일 연구세미나에서는 Neural Net, Deep Neural Net을 학습하는 알고리즘 중 가장 기본이 되는 Backpropagation과 optimization의 방법 중 gradient descent와 newton's method를 설명하였습니다. 먼저 gradient descent와 newton's method의 차이를 설명하면서 Backpropagation에서 왜 gradient descent를 쓰는 지 설명하였고 이를 통해 간단한 regression과 multiclass classification을 하는 Neural net에 대해서 Backpropagation을 해보았습니다.]]></content>
		<date><![CDATA[20160325131911]]></date>
		<update><![CDATA[20160325131911]]></update>
		<view><![CDATA[14]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[81]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/03/25][연구세미나]Integrating Cluster Validity Indices based on DEA(발표자:김보섭)]]></title>
		<content><![CDATA[금일 본 연구세미나에서는 실제데이터에 DEA를 적용하고 DEA의 결과물인 eff와 여러 다른 지표를 각각의 지표의 관점에서 데이터마다 최적의 알고리즘을 선택하고 해당 알고리즘의 결과물에 대해서 BCR을 계산한 결과물을 선보였습니다. 예상했던 것과는 달리 결과물은 좋지는 않았고 이는 DEA에 활용한 알고리즘이 인공데이터셋에세 과적합되어 벌어진 문제라고 보여집니다. 실제 여러가지 DEA 변수의 조합을 탐색해본뒤 다시 더 많은 실제데이터에 적용해 볼 생각입니다.]]></content>
		<date><![CDATA[20160325132627]]></date>
		<update><![CDATA[20160325132627]]></update>
		<view><![CDATA[16]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[82]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 03. 29 회의록]]></title>
		<content><![CDATA[논문 수집 자료 압축 링크입니다.
https://www.dropbox.com/s/06kgyxzrfk9q766/%EB%85%BC%EB%AC%B8.zip?dl=0]]></content>
		<date><![CDATA[20160329185222]]></date>
		<update><![CDATA[20160329185222]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[83]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 04. 05 회의록]]></title>
		<content><![CDATA[금일 회의 관련 논문 및 요약본, 그리고 각종 포털 사이트 카테고리 정리한 내용이 담긴 압축파일 링크입니다.

https://www.dropbox.com/s/5rjrwqcgai7ufa3/160405_NC%20%ED%9A%8C%EC%9D%98%20%EC%9E%90%EB%A3%8C.zip?dl=0]]></content>
		<date><![CDATA[20160405191400]]></date>
		<update><![CDATA[20160405191400]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[84]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[2016/04/01][연구세미나]Multi-Co-Training]]></title>
		<content><![CDATA[4월 14일 학회 발표에 대비하여 발표 리허설을 했습니다. 

외운 대본에 의존하지 않고 멘트가 자연스럽게 나오도록 다음 리허설과 발표 당일까지 

더 연습하겠습니다.]]></content>
		<date><![CDATA[20160407165530]]></date>
		<update><![CDATA[20160407165530]]></update>
		<view><![CDATA[7]]></view>
		<comment><![CDATA[2]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[85]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[2016/04/01][연구세미나]Multi-Co-Training_NB]]></title>
		<content><![CDATA[BI데이터마이닝 학회에서의 발표 리허설을 진행했습니다.

기존 연구에서  NB부분만 다루었으며, 다소 짧은 시간을 사용한 느낌을 받았습니다.
조금더 말을 천천히 하도록 노력하고, 버벅대는 부분이 없도록 해당 장표의 keyword를 정확히 알고있도록 노력하겠습니다.]]></content>
		<date><![CDATA[20160407165811]]></date>
		<update><![CDATA[20160407165811]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[2]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[88]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[형석 김]]></member_display>
		<title><![CDATA[[2016/03/25][딥러닝 세미나]Neural Networks and Deep Learning]]></title>
		<content><![CDATA[금일 세미나시간에는 Neural network에서 multi-layer를 사용하게 되면서 가지는 강점과 이와 더불어 hidden-layer가 증가 함에 따른 이점에 대해서 소개하였습니다. 또한 backpropagation이 가지는 의미와 그 해석에 대해서도 알아보았습니다. 또한 batch와 mini-batch, stochastic batch의 차이점을 알아보았으며, 다양한  activation function과 stochastic gradient descent방법론의 다양한 알고리즘과 이를 통해서 무엇이 개선되었는지에 대해서 다루었습니다. 세미나시간에 다룬 내용이 많은 만큼 다들 한번씩 다시 스스로 복습하면서 자신의 것으로 만드셨으면 하는 바램입니다.
이상입니다.]]></content>
		<date><![CDATA[20160410035316]]></date>
		<update><![CDATA[20160410035316]]></update>
		<view><![CDATA[16]]></view>
		<comment><![CDATA[2]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[87]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[형석 김]]></member_display>
		<title><![CDATA[[2016/04/01][연구세미나]research trend - Deep learning]]></title>
		<content><![CDATA[4월 1일 연구세미나 시간에는 다가올 산업공학회발표에 앞서 리허설 발표를 진행하였습니다.
발표준비에 미흡한 부분은 반성하고 차주에는 완벽한 모습으로 리허설을 준비할 수 있도록 하겠습니다.

 - 수정부분 수정하여 업로드합니다. - 
1. Hot &amp; Cold plot의 폰트 사이즈 수정 및 배치 재확인
2. 관련연구 부분의 인용문구 추가
3. 그외 시각화 자료의 폰트 사이즈 수정

발표자료 URL
https://www.dropbox.com/s/m5itd3oawqbx8xi/version_8%202016-04-07.pdf?dl=0]]></content>
		<date><![CDATA[20160407170917]]></date>
		<update><![CDATA[20160407170917]]></update>
		<view><![CDATA[7]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[89]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 04. 08 회의록 (kick off)]]></title>
		<content><![CDATA[녹취한 음성파일 링크입니다.

https://www.dropbox.com/s/p3wa79diptds5xc/NC%ED%82%A5%EC%98%A4%ED%94%84%EB%AF%B8%ED%8C%85_20160408-145313.mp3?dl=0]]></content>
		<date><![CDATA[20160411190720]]></date>
		<update><![CDATA[20160411190720]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[90]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[2016/04/13][BI 학회 발표] Keystroke Dynamics Analysis]]></title>
		<content><![CDATA[오늘은 BI 학회에서 Keystroke관련으로 발표를 진행 하였습니다.

이상없이 발표를 진행하였지만 이번에 연구를 진행 하면서 느낀점이 많습니다.

   1. 사전 연구 조사를 확실히 아주 확실히 하여야 한다는것

 사전 연구 조사를 바탕으로 새로운 것을 개선하지 않으면 전체 퍼즐을 맞추는것이 과제라면 같은 퍼즐 조각을

  또 찾아서 끼울려고 하는 꼴이 된다고 생각합니다. 이번 연구를 통해서 사전연구 조사가 Idea와 전체적 연구의
  
  질을 산출하는데에 얼마나 중요한 역할을 하는지 알게 된점이라 생각합니다.


     2. 이번 연구를 전체적으로 진행한 결과 행해야 할 부분을 확실하게 알게 되었습니다. 다음 연구에서는 이를 반영하

          주도적으로 KDA의 결과물을 산출할 생각입니다.



      3. 학회에서 받은 질문들은 아래와 같고 답변은 아래와 같이 실시 하였습니다.


[Q1]  Valid User의 사용자가 자신의 패턴을 이상하게 칠경우 사용자 인증을 할수가 없는것 아닙니까?
[A1] 질문 감사드립니다. 말씀하신대로 특정 사용자(Valid User)가 일반적인 경우가 아닌 적은 확률로 
         일부러 자신의 패턴을 다르게 행동한다면 사용자 인증에 대한 성능이 말씀하신 것처럼 떨어질 확률이 있습니다.
         왜냐하면 본 연구에서 가정하는것은 각 사용자별로 타이핑하는 패턴이 다르며, 이것이 일정하게 진행된다는
         가정을 하고 있기 때문입니다.

[Q2] KS-CM의 경우 분포 가정없이 저런식으로 곱하여도 이상이 없는것인가?
[A2] Two Pair KS Test나 CM Test의 경우, 2개의 샘플 기준으로 분포 가정없이 행하는 방법론이므로 1 Pair KS, CM
        Test처럼 분포 가정을 테스트 하는것이 아닌 2개의 샘플의 차이를 계산을 하는것이기 때문에 큰 상관이 없다고
        생각합니다.
     

 이상입니다.
오늘부터 시행되는 산공 학회에서도 다양한 학회내용을 볼 수 있었으면 합니다.]]></content>
		<date><![CDATA[20160413021456]]></date>
		<update><![CDATA[20160413021456]]></update>
		<view><![CDATA[8]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[91]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[2016/04/13][BI 학회 발표] 나이브베이즈 분류기 기반의 협동학습을 활용한 문서분류]]></title>
		<content><![CDATA[학회를 통해 느낀점과 질문 / 답변은 다음과 같습니다.

1. 실험의 구석구석을 살펴야 한다.
 실험 설계는 왜 그렇게 했으며, 그 결과에 대한 해석이 왜 그런식으로 도출되는지에 대한 고민을 많이 해봐야겠다는 생각을 했습니다. 설계에 있어서는 사전연구에 대한 관찰을 열심히 하여 보완하고, 해당 실험에 많은 고민을 함으로써 질의에 대해 답변할 수 있는 실험자가 되야겠다는 생각을 했습니다.

2. 여러 분야의 연구가 진행된다.
 의학, 스마트폰 기반, 기상데이터등을 활용한 분석이 이루어지는 것을 확인하고 왔습니다. 앞으로 하고싶은 연구 idea를 고민할 때 더 다양한 접근을 생각해볼 필요가 있겠다고 생각했습니다.

3. 질문 / 답변
 1) NB의 히트맵에서 차원이 증가함에 따라 점차 성능이 좋아지는 경향을 보이는데, 이것의 이유가 어떻게 됩니까?
  - 문서를 벡터의 형태로 표현할 때, 작은 차원보다는 큰 차원일 때 더 많은 정보를 가지고 있으므로, 문서를 잘 표현하게 되고 이를 기반으로 분류할 때 더 좋은 성능을 보이게 된다.(제주 산공학회에서 더 진전된 질문이 있었는데, 차원이 높아질수록 분류기가 성능이 안좋아진다 하셨습니다. 이에 대한 trade off가 있다고 하셨는데, 이부분에 대한 내용은 잘 모르겠습니다. 댓글로 관련한 내용을 알려주시는 분이 계시면 감사하겠습니다.)

 2) TF-IDF와 LDA를 같은 선상의 view로 사용하는 것이 이해가 안가는데 비교가 가능한겁니까?
  - 이는 질문자께서 문서분류에 대한 연구임을 정확히 파악하시지 못한 채 질문하신 내용이라, 문서를 벡터로 표현하는 방법의 차원에서 view로 사용 가능함을 설명드렸습니다.]]></content>
		<date><![CDATA[20160414131639]]></date>
		<update><![CDATA[20160414131639]]></update>
		<view><![CDATA[9]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[92]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[2016/04/15][춘계공동학술대회] 에세이]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160416181850]]></date>
		<update><![CDATA[20160416181850]]></update>
		<view><![CDATA[10]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[93]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[2016/04/15][춘계공동학술대회] 후기]]></title>
		<content><![CDATA[2박 3일동안 제주도에서 개최된 산업공학과 학회에 참석하였으며,

이 안에서 많은 하나의 세미나 발표와 여러가지의 세션안에서의 발표를 들었습니다.

먼저 내용 외적으로는 발표할때의 PPT의 구성과 발표의 자신감, 요약정리가 중요한 점인지 다시 한번 알게 되었습니다.

PPT의 장수가 많더라도 거기에 맞추어서 제한된 시간안에 이해를 시키는 발표도 있었고, 내용이 짧아도 장황하게

설명하여 발표의 중심을 전달받지 못한 경우도 있었기 때문입니다.

기억에 남는 발표들중에서는

 먼저, 개인적으로 주상병코드를 한번 Network로 구성해보고 싶다는 생각은 이전부터 생각해 왔었고, 그 기저가 되는 Edge weight를 여러 계층별로 실 처방 데이터를 기반으로 해보면 참 재미있겠다는 생각을 하였는데, 아주대학교에서 비슷한 주제로 발표로 하여서 재미있게 청강하였고, 발표뒤에도 여러가지를 질문하여서 재미있었습니다.

 다음으로는 연구의 필요성이 명확해야 한다는것입니다. 연구를위한 연구가 아닌, 정말 이것을 어떻게 써먹을것이며,
무엇을 위해서 하는가? 에 대한 질문이 명확해야지 연구후의 발표와 질문에 대해서도 정확하게 대답할 수 있다고 생각합니다. 따라서 앞으로 연구를 할 시에 평소에 필요한것, 해보고 싶은것중 적용이 가능한것에 대해서 항상 염두해 두고 생활하여야 한다고 생각이 되었습니다.

  Deep Neural Network의 대한 내용이 많은 부분을 차지한다는 것을 느꼈습니다. 몇몇의 발표에서는 단순히 기존의 방법론에서 DNN을 적용하여 성능이 향상되었다는 내용이 있었는데, 각각에서는 한가지 방식의 뉴럴 네트워크를 기반으로 실험하였는데 전체적인 여러가지 방향의 결과를 나타낸 발표자료가 있으면 더 좋겠다는 생각을 하였습니다.
또한, Q&amp;A중에는 현업자들의 대한 생각도 옅볼수가 있었는데, 앞으로는 어떤 Framework을 생각할때에 실제로 데이터가 확장이 되면 어떻게 최적을 할것인가? 라는 생각도 염두해 두어야 되겠다고 생각 했습니다.

 그리고 Cluster와 Density 기반의 Outlier detection 같은 향상된 알고리즘을 발표한 사례도 있었는데, 실제로 알고리즘을 사용하다가 가끔식 이렇게 저렇게 하면 더 좋을것 같은데... 라는 생각을 종종하게 되는데, 이런 아이디어가 나오면 정리해 놓고 한번씩 실험해 보고, 가설 검증을 하는식을 생활화 하다보면 재미있는 결과가 나올수 있겠다고 생각이 들었습니다.

 결론적으로, 연구에 대해서 명확한 정의와 평소에 항상 고민하는 습관을 좀더 길러야 되겠다고 결론이 들었습니다.]]></content>
		<date><![CDATA[20160416185111]]></date>
		<update><![CDATA[20160416185111]]></update>
		<view><![CDATA[10]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[95]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[[2016/04/15][춘계공동학술대회] 발표동영상]]></title>
		<content><![CDATA[조수현
https://drive.google.com/open?id=0Bx5wSprkmz0HNWlGZUtqckMtT00

김보섭
https://drive.google.com/open?id=0Bx5wSprkmz0HOGJaajB5bDdBZUk

류나현
https://drive.google.com/open?id=0Bx5wSprkmz0HTGlMdWxfeFdpMkE

김해동
https://drive.google.com/open?id=0Bx5wSprkmz0Hc0NhU3hfTzVrTVk

김형석
https://drive.google.com/open?id=0Bx5wSprkmz0HZHA5eVZ2WmZ5RUU

김준홍
https://drive.google.com/open?id=0Bx5wSprkmz0HLWlHWV9iVUN5MnM]]></content>
		<date><![CDATA[20160418144830]]></date>
		<update><![CDATA[20160430205949]]></update>
		<view><![CDATA[14]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[96]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016 대한산업공학회 춘계학술대회 참석]]></title>
		<content><![CDATA[2016 대한산업공학회 춘계학술대회

1.장소 및 기간
   2016/04/13~2016/04/15, 제주국제컨벤션센터

2. 참가인원
     박사과정 : 김준홍
     통합과정 : 김형석
     석사과정 : 류나현,김보섭,박민식,김해동,조수현,서덕성

3. 발표자
    김준홍, 김형석, 류나현, 김보섭, 김해동, 조수현 연구원

4. 사진첨부
     연구원 단체사진 및 개별 연구원의 발표사진 첨부합니다.]]></content>
		<date><![CDATA[20160418151026]]></date>
		<update><![CDATA[20160418151026]]></update>
		<view><![CDATA[2330]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/57147a8208a583090423.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[단체사진1.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[97]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 춘계학술대회 - 서덕성]]></title>
		<content><![CDATA[2016 춘계공동학술대회 에세이
서덕성

 제주라는 좋은 장소에 와서 관심 있는 분야에 대해 여러 연구가 진행되고 있으며, 그 안에서 어떻게 나의 연구를 진행해야 할지 조금이나마 알아가는 자리가 되었습니다.

 학회에서 주로 비지니스 애널리틱스 세션에 들어갔으며, 우리 연구실과 아주대학교 신현정교수님 연구실, 그리고 여러 다른 연구실의 연구 성과에 대해 들었습니다. 어려운 내용들이 참 많았는데, 연구실에서 진행하는 딥러닝 세미나를 통해 RBM, Auto encoder 등을 배워둔 상황이라 다른 연구들의 세부 방법론을 들을 때 도움이 많이 되었습니다.

 특히 관심 있게 들었던 연구가 몇 개 있는데, 그 중 하나는 "stratfied netwrok"입니다. 수식이 많아서 그 자리에서 모든 것을 이해하는 것은 힘들었지만, 층이 있는 구조의 network가 있다는 것을 알게 된 것만으로도 충분히 의미 있는 시간이지 않았나 생각합니다. 해당 연구 설명 중간까지는 이 구조가 왜 필요하지? 굳이 층을 만들지 않아도 하나의 평면으로 network를 그려서 시각적으로 볼 수 있으며, 이들의 관계를 층이 없는 상태나 있는 상태나 같은 차원의 matrix로 표현되는데 왜 필요한지에 대해 고민하며 들었습니다. 결국 데이터 자체가 층 구조가 있으면 더 표현이 정당하게 되는 경우에 대한 연구라는 것이라는 내용으로 이해하며 이 층 구조의 network가 유용하게 사용될 수 있을 것이라는 생각이 들었습니다.
 또 다른 흥미로웠던 주제로는 김성범 교수님 연구실 소속의 유재홍 박사과정이 발표한 " Randomized ensemble-based unsupervised feature selection for clustering"가 기억에 남습니다. 변수를 선택하는 것이 최종 목적이며, 그것을 clustering을 이용하고, 사용하는 지표는 모든 변수를 사용했을 때의 실루엣과 변수 선택 후 군집의 실루엣을 통해 얼마나 잘 데이터를 표현하는지에 대해 적용했다고 이해했습니다. 너무 어려워서 제대로 이해하지 못하긴 했지만, 클러스터링을 통해 변수를 선택한다는 것 자체가 참신한 아이디어라고 생각했으며, 앞으로 어떤 아이디어를 낼 때 열린 생각을 해야겠다는 생각을 했습니다.

 제가 팀원으로 진행했던 연구인 Multi-co-training에 대해 수현이가 발표하며 질문사항을 통해 우리 연구가 어떤 부분의 약점이 있는지, 그리고 앞으로 연구를 진행할 때 어떤 부분을 경험적 조언으로 사용할 수 있을 지 배울 수 있었습니다. 마지막 질문이었던 "차원이 증가할수록 문서를 잘 표현할 수 있는 장점이 있는데 반해 분류기의 성능이 좋지 않을 텐데, 그 문제에 대해 생각해 봤습니까?"에 대한 것은 차원의 저주와 관련된 내용이지 않을까 생각이 들기는 하지만 정확히 알지는 못했습니다. 이뿐 아니라 여러 질문들을 통해 우리 연구를 좀 더 깊게 생각해 볼 수 있는 기회를 얻은 것도 좋은 영향이라고 생각합니다.

 이번 학회를 통해 많은 것을 배웠으며, 다음에 진행될 학회에서도 많은 것을 보고 배우는 자리가 되길 기대합니다.]]></content>
		<date><![CDATA[20160418152345]]></date>
		<update><![CDATA[20160418152345]]></update>
		<view><![CDATA[2259]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/57147d7136d083658966.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo_03.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[112]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 04. 28 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160428155202]]></date>
		<update><![CDATA[20160428155202]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[113]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 04. 24 회의록]]></title>
		<content><![CDATA[금일 NC 관계자분들과 진행한 회의 자료입니다.
https://www.dropbox.com/s/rxnybnjur2q101h/2016-04-28_NC%EC%A0%95%EA%B8%B0.zip?dl=0]]></content>
		<date><![CDATA[20160428181056]]></date>
		<update><![CDATA[20160428181056]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[111]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 04. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160426180646]]></date>
		<update><![CDATA[20160426180646]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[98]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 춘계학술대회 - 김준홍]]></title>
		<content><![CDATA[2박 3일동안 제주도에서 개최된 산업공학과 학회와 당일로 진행된 BI 데이터마이닝 학회에 참석하여, 2번의 발표와 여러가지의 세션에서 다양한 발표를 들었습니다.

    개인적으로 기억에 남는 부분은 의료 명세서 기반으로 주상병코드기준으로 Network로 구성해보고 싶다는 생각을 이전에 해보았고 그 기저가 되는 Edge weight를 여러 계층별로 실 처방 데이터를 기반으로 해보면 참 재미있겠다는 생각을 하였었습니다. 이번 학회에서 아주대학교에서 비슷한 주제로 발표로 하여서 재미있게 청강하였고, 발표뒤에도 발표자 분께 여러가지를 질문 후 답변을 말씀해주셔서 재미있었습니다.

    Deep Neural Network의 대한 내용이 많은 부분을 차지한다는 것을 느꼈습니다. 몇몇의 발표에서는 기존의 방법론에서 Deep learning을 적용하여 성능이 향상되었다는 내용이 있었는데, 각각에서는 한가지 방식의 뉴럴 네트워크를 기반으로 실험하였는데 전체적인 여러가지 방향의 결과를 나타낸 발표자료가 있으면 더욱 재미있을것 같다는 생각이 들었습니다.
또한, Q&amp;A중에는 현업자분들의 Training 시간에 따른 부담감의 대한 생각도 옅볼수가 있었는데, 앞으로는 어떤 분석과정을 생각할때에 실제로 데이터가 확장이 되면 시간적으로, 특정 제약조건안에서 어떻게 최적을 할 것인가? 라는 생각도 염두해 두어야 되겠다고 생각 했습니다.

    그리고 Cluster와 Density 기반의 Outlier detection분야에서 향상된 알고리즘을 발표한 사례도 있었습니다.
실제로 알고리즘을 사용하다가 가끔식 '이렇게 저렇게 하면 더 좋을것 같은데...' 라는 생각을 가끔씩 하는 경우가 있는데, 이런 아이디어가 나오면 정리해 놓고 한번씩 실험해 보고, 가설 검증을 하는것을 생활화 하다보면 재미있는 결과가 나올수 있겠다고 생각이 들었으며 이를 위해서는 좀 더  부지런히 생활해야 함을 인식 하였습니다.

    내용 외적으로는 발표할때의 PPT의 구성과 발표의 자신감, 요약정리가 중요한 점이라는것을 다시 한번 알게 되었습니다.  PPT의 장수가 많더라도 거기에 맞추어서 제한된 시간안에 이해를 시키는 경우도 있었고, 내용이 짧아도 발표의 중심을 전달받지 못한 경우도 있었기 때문입니다.
다음으로는 연구의 필요성이 명확해야 한다는것입니다.  이것을 어떻게 적용할 것이며, 무엇을 위해서 하는가? 에 대한 답변이 명확해야지 연구후의 발표와 질문에 대해서도 정확하게 대답할 수 있다고 생각합니다.  따라서 앞으로 연구를 할 시에 평소에 필요한것, 해보고 싶은것중 적용이 가능한것에 대해서 항상 염두해 두고 생활하여야 한다고 생각이 되었습니다.

    결론적으로, 연구에 대해서 명확한 정의와 평소에 항상 고민하는 습관을 좀 더 길러야 되겠다고 결론이 들었습니다.

읽어주셔서 감사합니다.

- 김준홍 올림 -]]></content>
		<date><![CDATA[20160418152621]]></date>
		<update><![CDATA[20160418152621]]></update>
		<view><![CDATA[2349]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/57147e0dedf5a6535736.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo_03.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[99]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 춘계학술대회 - 김보섭]]></title>
		<content><![CDATA[- 발표 후기
발표제목 : 자료포락분석법(DEA)를 활용한 군집타당성에 관한 연구
 작년 9월에 입학하여 석사, 연구실 생활을 하면서 처음으로 학회에서 발표를 하게 되었다. 처음에 생각했던 방향대로 연구결과가 원활하게 나오지 않아 어려움이 많았지만 개인적으로 이 발표를 준비하면서 나름대로 얻은 경험들을 기술해보고자 한다. 
 첫째로 연구에 대한 프레임워크를 대략적으로 파악할 수 있었다. 학부에서는 연구를 경험하기힘들고 또한 학부생활이 아닌 다른 동아리 또는 모임에서는 넓게 보았을 때 연구라는 것을 경험할 수는 있지만 체계적인 프레임워크가 아닌 주먹구구식이라고 볼 수 있다. 하지만 이 연구를 진행하면서 체계적으로 진행되는 연구라는 것에 대해 맛을 보았다고 생각한다.
 둘째로 해당 연구를 통해서 논문을 읽는 것에 대한 부담감이 덜어졌다. 사실 대부분이 영어 또는 수식으로 이루어져있는 논문은 읽기 부담스러운 것이 사실이었다. 하지만 연구과정에서 문헌탐구 또는 내 연구분야에 활용되는 알고리즘을 깊이이해하기위해서는 해당 과정이 필수적인 것이었으므로 다수의 논문을 읽었고 이를 통해서 논문을 읽는 경험을 쌓았다고 생각한다.
 셋째로 공개적인 자리에서 연구에 대하여 발표하고 내 연구에 활용한 논리를 디펜스하는 경험을 얻었다. 사실 위와 같은 경험은 생각보다 경험하기 힘든 일이라고 생각되는데 위와 같은 경험이 비록 연구뿐만아니라 앞으로 사회생활을 함에 있어서 올바른 토론에 대한 경험으로서 내 인생에 도움이 될 것이라고 생각한다.

- 청취 후기
이론적으로만 알고 있는 기계학습 알고리즘 혹은 통계방법론들이 실제로 어떤 분야 활용되고 있는 지, 어떤 방식으로 다른 연구자들이 활용하여 실제 문제를 풀어나가고 있는 지 볼 수 있어서 특히 좋았던 것 같다. 앞으로 나의 연구분야를 정하는 데에 있어서 여러가지 깊이 고찰해볼 수 있는 시발점이 될만한 아이디어를 생각하는 데에 도움이 될 것이다. 
개인적으로 이번 학회에서는 군집화에 관련된 알고리즘을 토대로 발표를 했기 때문에 이와 관련하여 인상 깊었던 연구가 하나 있었는데 이는 고려대학교 DMQM 연구실의 유재홍 박사과정의 “Randomized Ensemble-based Unsupervised Feature Selection for Clustering”이라는 연구이다. 해당 연구에서는 짧은 지식으로 평가하자면 큰 줄기의 아이디어는 어느 정도 각각의 다른 알고리즘에서 활용되고 있는 아이디어였는데, 이를 토대로 자신만의 작은 개념을 정립시켜 연구를 진행한 것이 굉장히 인상 깊었다. 또한 슬라이드도 개인적을 생각하기에는 학회발표에 가장 적합한 슬라이드 구성이었다고 생각하였다. 위와 같은 연구를 할 수 있도록 평소에 알고리즘에 관한 나의 여러가지 생각들을 정리해야겠다는 필요성을 느꼈다.]]></content>
		<date><![CDATA[20160418152735]]></date>
		<update><![CDATA[20160418152735]]></update>
		<view><![CDATA[2259]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/57147e572782e6021850.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo_03.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[101]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 춘계학술대회 - 박민식]]></title>
		<content><![CDATA[이번 학술대회는 연구실에 들어와서 두번째로 참여하게 된 학회 입니다. 제주도라는 환경이 만족스러웠고, 혼자 공부하거나 연구실 내에서의 세미나 범위를 넘어서 같은 분야를 연구하는 다른 학교, 연구원, 회사 사람들이 모이고 각자의 연구동향을 파악할 수 있는 기회여서 뜻 깊은 시간이었습니다. 이번 학회에서는 주도적으로 연구를 해보고 발표를 하지 못한 점이 아쉽지만, 다음 학회부터는 제 연구내용을 청중들 앞에서 발표하고 이야기를 들어보는 시간을 가지고 싶습니다.
 
 ● 발표 청취 후기

  이번 학회에서는 주로 데이터 분석과 관련된 '비즈니스 애널리틱스' 프로그램의 발표를 들었습니다. 다른 학교 사람들의 발표를 들으면서 놀랐던 점은, 한정된 분야의 연구에 얽매이지 않고 다른 학과 사람들과 공동 연구를 하며 의학, 역사 분야 등의 다양한 데이터마이닝 연구를 수행하고 있다는 점이었습니다. 공부를 하면서도 제가 배우는 것들이 이론적인 내용 이외에 실제 상황에서 어떻게 활용할 수 있을까에 대한 고민이 항상 있었는데, 저런 방법으로도 해볼 수 있겠구나 하고 배울 수 있어서 좋았습니다. 인상 깊었던 발표들은 다음과 같습니다.

1.	불균형 데이터에서의 확률 기반 언더샘플링 기법
 산업 현장에서는 신용카드 사기 검출, 이탈 고객 예측 등과 같이 소수 클래스에 대한 예측이 필요한 문제가 발생하는데, 불균형 데이터는 소수 클래스가 다수 클래스로 오 분류 되는 문제점이 있습니다. 이러한 문제를 해결하기 위해서 소속확률기반 언더샘플링(MPU) 알고리즘을 제안했다고 들었다. 앞으로 하게 될 연구에서도 불균형 데이터가 나올 수 있다고 생각하는데, 불균형 데이터를 분석하기 위한 선행연구, 방법들을 배울 수 있어서 좋았습니다.

2.	준지도 학습을 활용한 역사 데이터 기반 권력 메커니즘 추론
 이 발표는 아주대학교 데이터마이닝 연구실 소속의 연구원이 발표한 내용이었습니다. 역사학에 관한 내용을 연구하기 위해서, 사학과 교수님과 공동 연구를 한 것이 인상적이었습니다. 평소에도 역사 데이터에 대해서 분석하는 것에 관심이 있었는데, 데이터를 가져올 수 있는 방법이 쉽지 않다고 느꼈었는데 이번 발표를 통해서 데이터를 구할 수 있는 방법을 알게 되었습니다. 기존의 연구에서는 역사 기록물 텍스트를 기반으로 토픽 모델링을 적용하는 단계였는데, 족보 데이터에서 역사 인물들 간의 네트워크를 바탕으로 권력 양상을 추론한 것이 흥미로웠고 방대한 시간이 걸리는 문헌 연구 과정에서 도움이 될 것이라는 생각이 들었습니다.]]></content>
		<date><![CDATA[20160418164604]]></date>
		<update><![CDATA[20160418164604]]></update>
		<view><![CDATA[2227]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/571490bcf05429826416.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo_03.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[102]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 춘계학술대회 - 김해동]]></title>
		<content><![CDATA[이틀 간 진행 되었던 춘계 산업공학회는 맑은 봄 날씨 덕택에 느낀 포근한 감정부터 다른 연구자들의 연구 결과물을 보면서 느낀 감상과 연구실원들과의 재미난 추억까지 다양하다. 하지만 학술대회를 다녀오고 나서 쓰는 글인 만큼 학술적인 부분에서 느낀 감상을 중점적으로 기술하고자 한다. 본문에서 ‘연구’라는 활동 자체에 대한 감상에서, 다른 학회 참가자들의 연구 결과 발표, 그리고 좀 더 세부적인 사항으로 시야를 좁혀가며 느꼈던 점을 최대한 자세 쓰려 했다. 
    올해 초부터 준홍 형과 같이 키스트로크를 주제로 연구를 진행하였고 이번 학술대회에서 발표도 하였지만 사실 여전히 ‘연구’라는 활동이 어떤 건지 감을 못 잡고 있었다. 학회에서 여러 사람들의 연구결과물을 보고 생각한 연구는 주장과 설득의 과정 이었다. 사람들의 시선을 끌 매력적인 주장을 하기 위해선 기존의 연구 결과물 들을 비판적으로 검토한 내용에 기반하여 대안, 또는 개선점을 제시해야 한다. 기존 연구와 같은 주장을 되풀이 하는 것은 표절로 인한 범죄이기 이전에 흥미롭지 않기 때문에 연구로써 의의가 적다. 새롭고 설명력 있는, 매력적인 주장을 하고 입증하는 것이 연구활동의 본성이라면, 기존 결과물을 베낀 앵무새 같은 연구는 법 체계가 달라 표절이 범죄가 되지 않는 장소가 있어 처벌은 피한다 하여도 어떤 크레딧도 얻을 수 없을 것이다. 말이 기존 연구를 비판적으로 보거나, 또는 기존에 시도 되지 않았던 새로운 분야에 적용하기 위해선 무엇보다 해당 분야에 대한 완벽한 이해가 전제되어야 한다. 선행 연구가 중요한 이유도 이러한 맥락 에서가 아닐까 생각했다.
    다양한 연구자들의 연구결과물들을 보고 든 첫 번째 생각은 참 다양하다는 점이었다. 이틀 간 참석한 비즈니스 애널리틱스 세션에서만 보더라도 데이터 마이닝 기법을 의료와 역사학에 까지 활용한 점을 보더라도 그렇다. 그뿐만 아니라 학회에서 나눠준 팜플렛에 나열되어 있는 다른 발표들을 보면 생각지도 못했던 분야에 산업공학의 기법들이 적용되고 있었다. 활발한 연구활동에서부터 많은 자극을 받았다. 반면 조금 아쉬웠던 점은 여러 세션에 참가해 보지 못했다는 점이다. 다음에는 비즈니스 애널리틱스 외에도 최적화나 물류쪽 세션에도 참여해 보고 싶다.
    앞서 연구는 주장과 설득의 과정이라 하였다. 설득을 위해선 설득력 있고 논리적으로 결함이 적은 논증을 우선 해야한다. 하지만 그에 못지 않게 발표를 통한 전달이 중요하단 점을 느꼈다. 아마 일반적으로 발표가 중요한 영역은 비즈니스나 정치, 연설을 떠올릴 것 같다. 하지만 학문을 할 때에도 비즈니스 못지 않게 발표 능력이 중요했다. 자신의 연구 아이디어가 참신하고 결과가 좋아도 다른 사람을 설득하지 못하면 소용이 없다. 물론 학계에서는 논문, 즉 글을 통해 이러한 과정을 거치기 때문에 구두 발표의 중요성이 조금 떨어져 보일 수도 있다. 하지만 논문으로 출판하기 이전에 미리 관심을 선점하기 위해선 컨퍼런스나 학술대회 자리에서의 우수한 구두 발표가 필요하다. 이는 논문이 매일 같이 쏟아져 나오는 현실을 생각해 봤을 때 구두 발표를 통한 관심의 집중이 더욱 중요하다. 여러 발표를 들으면서 본받아야 할 점도 있었지만 삼가해야 할 점도 눈에 띄었다. 그 중 가장 머릿속에 강하게 남아있는 조심해야 할 점은 너무 많은 내용을 발표에 담으려 하다 핵심 주장이 흐려지는 경우였다. 글과 말의 차이점을 명확하게 인지하고 구두 발표에 걸맞게 준비를 해야겠다고 생각 했다. 이번 학회 참가는 학부생에서 벗어나 직접 연구를 수행해야 하는 대학원생으로 훈련을 받는 과정에 유익한 생각거리와 교훈, 그리고 동기를 부여해주었다.]]></content>
		<date><![CDATA[20160418201353]]></date>
		<update><![CDATA[20160418201353]]></update>
		<view><![CDATA[2255]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/5714c1718762f1150787.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[KakaoTalk_20160418_152422961.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[103]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 춘계학술대회 - 조수현]]></title>
		<content><![CDATA[4월 13일부터 16일 까지 제주도에서 열리는 2016 춘계공동학술대회에 다녀왔습니다. 입학한 후처음 경험하는 학회였던 만큼 기대 반 걱정 반으로 매우 떨렸지만 많이 배우고 연구실 사람들과 추억도 쌓을 수 있는 알찼던 시간이었습니다. 

발표와 관련된 느낌을 서술 하자면 먼저, 지난 겨울부터 동화 오빠와 한 팀이 되어 Multi-co-training을 이용한 문서 분류 라는 주제의 연구를 진행한다고 했지만 사실상 큰 맥락을 파악하지 못하고 헤매고 있었습니다. 하지만 이렇게 직접 경험 해 보고 다른 사람들이 진행해온 연구 결과물을 보면서 대학원생이 하는 ‘연구’라는 것이 어떻게 진행되고 여기에 어떤 태도로 임해야 하는지에 대해 배울 수 있었습니다. 
또한 이번 발표를 통해 누군가에게 나의 아이디어를 전달하기 위해서는 엄청난 준비가 필요하다는 것을 다시 한번 깨달았습니다. 평상시에 발표에 자신있는 학생이 아니었기 때문에 단상에 올라가기 직전까지 계속 연습했지만 막상 올라가니 생각했던 만큼 말이 나오지 않았습니다. 뿐만 아니라 발표 전에 나름대로의 예상질문을 생각하고 대비했지만 좀 더 유연하게 대처하지 못한 것 같아 아쉬움이 많이 남았습니다. 단상에서 내려온 후 차분하게 제 자신을 되돌아보면서 앞으로 더 많이 연습하겠다고 다짐했습니다.

다른 참가자들의 발표를 들으면서 계속 들었던 생각은 데이터마이닝 기법이 적용될 수 있는 분야는 정말 무궁무진 하다는 것이었습니다. 과연 그 한계가 있을까 하는 의문도 들었습니다. 물론 모든 분야에 유용하게 적용될 수 있겠지만 가장 흥미로웠던 연구는 이를 역사 분야에 적용한 것이었습니다. 연구 진행 설정상 불가피한 한계가 있기는 했지만 역사상의 권력구조를 문서 분석을 통해 알아보려고 시도했다는 점이 매우 인상깊었습니다. 저 자신에게 자극을 줄 수 있었고 앞으로의 제 연구분야에 대해 깊게 고민 할 수 있었던 좋은 경험이었습니다. 
마지막으로 ‘아는 만큼 보인다’ 를 체감했던 학회였습니다. 여러 흥미로운 발표 주제들 중에서는 가기 전 세미나 시간에 배웠던 RBM, Autoencoder 등의 것들도 있었는데 아는 부분은 발표를 들을 때 크게 문제없이 재미있게 들었지만 심화 어려운 부분에서는 이해하기 힘들어 종종 놓치기도 했습니다. 이런 제 자신을 보면서 앞서 배운 내용도 잊지 않으려 노력하고 앞으로 배울 새로운 것들도 부지런히 배워야겠다고 다짐했습니다.]]></content>
		<date><![CDATA[20160419134707]]></date>
		<update><![CDATA[20160419134707]]></update>
		<view><![CDATA[2196]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/5715b84b79b3d9464416.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[썸내일.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[104]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 04. 12 회의록]]></title>
		<content><![CDATA[BI데이터마이닝 학회로 불참하여 나현이가 회의록을 만들어주었습니다.]]></content>
		<date><![CDATA[20160420123434]]></date>
		<update><![CDATA[20160420123434]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[105]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 04. 19 회의록]]></title>
		<content><![CDATA[메타변수 정리한 표 및 설명 파일 링크입니다.
https://www.dropbox.com/s/oha2tzd8kafypjf/%EB%A9%94%ED%83%80%EB%B3%80%EC%88%98%20%EC%A0%95%EB%A6%AC.zip?dl=0]]></content>
		<date><![CDATA[20160420123624]]></date>
		<update><![CDATA[20160420123624]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[108]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 춘계학술대회 - 류나현]]></title>
		<content><![CDATA[학회에서 자신의 연구를 발표하는 것은 대학원에 들어오면서 누구나 기대하고 바라는 것 중 하나가 아닐까 생각합니다. 처음 연구 주제를 접했을 때는 어떤 결과가 될지 두려움도 있었지만 어느새 학회에서 발표할 만큼의 결과도 얻게 되어 작지만 보람을 느꼈습니다. 그리고 생각보다 연구실 사람들과 많은 추억을 쌓고 돌아와 이번 학회에는 개인적으로 남다른 기억으로 남을 것 같습니다. 본문에서는 연구를 진행하면서 배우고 느낀 점도 많지만 발표 및 발표의 준비과정 그리고 이번 학회를 통해 배운 점을 중점으로 적고자 합니다. 
사실 직접 실험하여 결과를 보고 발표를 준비하긴 했지만 연구로써 어떤 의의를 부여할 수 있을지 고민했었는데 발표가 끝나고 현업에 종사하시는 분들이 해주시는 말씀을 들으니 실제 그분들이 느끼신 사실과 실험 결과가 유사하여 신기하였고 피드백도 받을 수 있어서 내심 기뻤습니다. 학회에서 이뤄지는 소통의 중요성을 새삼 다시 느낄 수 있었던 시간이었습니다.
발표를 준비하는 과정에서는 짧은 시간 안에 연구 내용을 정확히 전달하기 위해서는 많은 생각과 연습이 필요하다는 것을 느꼈고 주장하고 싶은 내용을 잘 전달할 수 있도록 명료하고 정제된 발표를 준비해야 한다는 것을 알게 되었습니다. 그런데 학회에서 주어진 발표 시간이 짧아서 다급하게 발표를 마쳤고 발표 후 받은 질문에서 연구 전반의 구성을 잘 전달하지 못 한 것 같다는 생각이 들어 아쉬웠습니다. 세션 시간을 좀 더 넘기어서라도 더 차분히 발표하려고 판단했더라면 더 좋았을 걸 하는 아쉬움도 남습니다.
그리고 이전에는 보이지 않던 것이었는데 이번 학회에서 다른 발표를 들으면서 다양한 연구에서 변수선택이나 변수추출과 같은 차원축소 기법이 사용되는 것을 볼 수 있었고 실제 데이터로부터 기인하는 에러가 차원축소에 의하여 줄어들고 유의미한 변수를 사용할 수 있는 것을 볼 수 있었습니다.
끝으로 이번 학회에서 여러 발표를 들으면서 생각지 못한 새로운 분야에서 이뤄지는 데이터를 이용한 연구들을 알 수 있었고 또한 기존 연구를 개선하거나 현업에서 발생하는 문제를 해결하기 위한 다양한 아이디어와 방법론들을 들을 수 있었습니다.]]></content>
		<date><![CDATA[20160421012747]]></date>
		<update><![CDATA[20160712101959]]></update>
		<view><![CDATA[2316]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/5717ae03d91535081420.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[KakaoTalk_20160418_152422961.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[109]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 04. 21 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160421161915]]></date>
		<update><![CDATA[20160421161915]]></update>
		<view><![CDATA[8]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[110]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[Insider Threat Keyword외 20160425미팅자료]]></title>
		<content><![CDATA[Insider Threat Keyword외 20160425미팅자료

[1] Research Trend Keyword

[2] 변수 조사

[3] Data관련 조사]]></content>
		<date><![CDATA[20160425232616]]></date>
		<update><![CDATA[20160425232616]]></update>
		<view><![CDATA[7]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[107]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[형석 김]]></member_display>
		<title><![CDATA[2016 춘계 산업공학회 후기 - 작성자:김형석]]></title>
		<content><![CDATA[화창한 햇살이 비추고 유채꽃이 필 무렵, 우리 연구실에서는 지난해에 이어서 춘계 산업공학회에 참석하였습니다. 본인은 산업공학회에는 이번이 3번째 참석이었습니다. 작년 춘계에는 참관을 하였으며, 지난 추계에서는 첫 학회 발표를, 이번 춘계에는 제 스스로의 연구성과를 발표하는 자리였습니다. 또한 랩실구성원이 많아지면서 자리를 많이 잡은 상태에서의 학회 참석이라 그 의미가 이전과는 많이 남달랐습니다. 

 우리 연구실은 실제로 학회가 시작하는 목요일에 앞서 그 전날 수요일 오후에 제주에 도착하여 간단한 저녁식사를 마치고 숙소에 도착하여 각자의 발표 준비로 여념이 없었습니다. 발표는 금 번이 처음은 아니지만 발표를 앞둔 상황은 언제나 긴장이 되었습니다. 다행히 본인은 발표가 수요일 마지막 세션이라 준비시간에는 부족함이 없었습니다. 

 우리 연구실의 주 참석 세션장은 비즈니스 애널리틱스 세션장으로 다양한 분야에서 기계학습과 데이터마이닝을 통해서 이를 해결하는 발표들이 주를 이루었습니다. 첫 세션에서는 우리연구실의 막내 조수현양의 발표가 있었습니다. 첫 발표임에도 불구하고 차분히 발표를 잘하였습니다. 그외에도 인상적인 발표는 청각 장애를 위한 보청기의 주파수 증폭대역대를 cascade recurrent neural network를 통해서 예측하는 발표하였습니다. Recurrent neural network의 순차적 인풋의 개념을 차용하여 음역대의 증폭을 업다운형식으로 casecade형식으로 학습해 나가는 구조의 뉴럴넷을 사용하였는데, domain의 특성을 고려한 neural network를 사용한다는 점이 평소 neural net에 관심을 가지고 있던 본인에게는 많은 자극을 받았습니다. 
 
 산업공학회 첫날의 마지막 세션에서는 본인의 발표를 진행하게 되었습니다. 사전에 준비뿐만 아니라 많은 리허설을 가졌지만, 준비만큼의 좋은 발표를 하지 못한 점이 아쉬움을 가졌습니다. 다음기회의 발표에서는 그 역치값을 끌어 올릴 수 있도록 노력해야겠다고 다짐하는 계기가 되었습니다. 발표 후 피드백시간에는 건국대학교 산업공학과의 윤장혁 교수님께서 많은 피드백을 해주셨는데 특히 본인이 발표한 토픽모델링 부분에서 topic의 key-phrase를 추출 가능하며, 이를 통해 Topic의 해석력을 가질 수 있다는 점을 알려주셨습니다. 이는 향후 논문이나 프로젝트에서 아주 좋은 idea를 얻고 가는 큰 수확이 였습니다. 학회가 마치고 난 후, 이메일을 통한 조언에서도 친철히 리뷰해주신 윤장혁 교수님께 정말 감사하였습니다. 

 두번째 날에는 다양한 발표를 들었지만서도 가장 인상이 남았던 건, 우리 연구실의 김준홍 박사과정 형의 발표였습니다. 비슷한 주제의 발표주제라서 더욱 이해도가 있던 건 사실이었지만, 청중의 입장인 저에게 발표에 나무랄데가 없이 잘하였다고 생각합니다. 다음 학회 발표에는 저러한 수준의 발표를 해야겠다며 다시 한번 반성의 시간을 가졌습니다.

 학회를 마지고, 집으로 돌아오는 항공편에서 결항 등의 우여곡절을 겪기도 하였지만, 금 번 춘계산업공학회는 저로 하여금 새로이 동기부여의 기회가 되었으며, 랩실 구성원들간의 친목을 더욱더 굳건히 하는 좋은 추억들을 많이 쌓은 학회가 아니었나 생각됩니다. 다음 학회에서는 더욱 더 좋은 연구성과로 만족할 만의 발표로 후련한 학회가 되기를 기원하며 16년 춘계산업공학회 후기를 마칩니다.
감사합니다.]]></content>
		<date><![CDATA[20160420133937]]></date>
		<update><![CDATA[20160420133937]]></update>
		<view><![CDATA[2349]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201604/5717088c8d1556667297.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[KakaoTalk_20160418_152422961.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[114]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[2016/04/29][딥러닝 세미나] Convolution Neural Network]]></title>
		<content><![CDATA[이번 세미나 시간에는 Convolution Neural Network에 대하여 다루었습니다.

먼저 Convolution의 구조와 Filter에 대하여 설명하였고 그 과정에서 convolution이 어떻게 stride가 되는지 설명하였습니다. 그리고 Max pooling에 대하여 설명하였고 Backpropagation으로 어떻게 Update가 되며 기존의 방식과 pooling을 하였기 때문에 크로니커 곱으로 크기를 맞추어 업데이트 한다는것도 다루었습니다.
다음으로는 기존의 ConvNet이 어떻게 진행되어 왔으며 그 구조에 대하여 다루었습니다.
다음으로는 요즘 핫하게 떠오르고 있는 TensorFlow를 이용하여 간단하게 실습하여 보았고 그 과정에서 느낀점과 성능을 비교하는것으로 발표를 마쳤습니다. 앞으로 TensorFlow를 이용하여 실습하면서 실제 연구에 적용할 부분에 대하여서는 적용해볼 계획입니다.

이상입니다.]]></content>
		<date><![CDATA[20160430112810]]></date>
		<update><![CDATA[20161006131501]]></update>
		<view><![CDATA[47]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[4]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[4]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[115]]></uid>
		<board_id><![CDATA[4]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016.04.13 ~ 2016.04.15 대한산업공학회 춘계학술대회]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160430163615]]></date>
		<update><![CDATA[20160430163805]]></update>
		<view><![CDATA[9018]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/4/201604/572460ad52d224930908.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[대한산업공학회 춘계학술대회2.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[116]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 05. 03 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160503195603]]></date>
		<update><![CDATA[20160503211052]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[120]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/05/06][딥러닝 세미나] Recurrent Neural Network]]></title>
		<content><![CDATA[RNN은 순차적인 정보를 처리하기 위한 neural network로 dynamical system의 state와 유사하게 이전 state의 정보를 현재 state로 전달하는 recurrence를 이용하여 순차적 정보를 처리한다. RNN은 recurrence의 종류와 방향, input과 output의 수 등에 따라 다양한 구조를 가지며 speech recognition, handwriting recognition, image captioning 등에 필요한 구조를 적용할 수 있다. RNN의 시간에 따른 backpropagation은 long term dependencies와 같은 문제가 발생하며 이를 해결하기 위하여 gating을 활용한 LSTM이 나타났다. LSTM은 각 gate에서 cell state에 정보를 reset/write/read 할지 결정하므로 기존 RNN보다 더 좋은 성능을 보여주고 있으며 GRU와 같은다양한 구조로도 쓰이고 있다.]]></content>
		<date><![CDATA[20160506140831]]></date>
		<update><![CDATA[20160519131108]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[118]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 05. 04 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160504215159]]></date>
		<update><![CDATA[20160504215159]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[119]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 05. 06 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160506133307]]></date>
		<update><![CDATA[20160506133307]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[121]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 05. 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160510172919]]></date>
		<update><![CDATA[20160510172919]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[122]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16, 05, 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160510195506]]></date>
		<update><![CDATA[20160511165311]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[123]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2016/05/13][딥러닝 세미나] Application: Computer Vision]]></title>
		<content><![CDATA[컴퓨터 비전은 컴퓨터에게 이미지를 인식시키기 위한 기술로 오락, 교통, 보안, 산업, 의료, 과학, 농업, 군사, 모바일, 로봇 등 다양한 분야에서 활용되고 있고 향후의 인공지능 시대에 필요한 기술입니다. 이번세미나에서는 컴퓨터 비전의 기본 개념(영상 처리, 에지 검출, 지역 특징 검출 및 기술, 매칭, 인식)과 컴퓨터 비전에서 쓰이는 자주 쓰이는 딥러닝 기법인 Convolutional Neural Network에 대한 세미나 발표를 하였습니다.]]></content>
		<date><![CDATA[20160515213020]]></date>
		<update><![CDATA[20160520215812]]></update>
		<view><![CDATA[16]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[124]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[데이터보고서20160516_0023_김준홍]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160516002636]]></date>
		<update><![CDATA[20160516002636]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/6/201605/5738952ccd1fe5500274.hwp]]></thumbnail_file>
		<thumbnail_name><![CDATA[데이터보고서20160516_0023_김준홍.hwp]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[125]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 05. 17 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160517172751]]></date>
		<update><![CDATA[20160517172751]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[128]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 05. 20 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160520142332]]></date>
		<update><![CDATA[20160520142332]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[129]]></uid>
		<board_id><![CDATA[4]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016.05.21 춘계학과등산대회]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160524124110]]></date>
		<update><![CDATA[20160524124110]]></update>
		<view><![CDATA[8686]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/4/201605/5743cd563f2f35842773.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[KakaoTalk_20160524_121653618.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[127]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2016/05/13][딥러닝 세미나] Application: Natural language processing]]></title>
		<content><![CDATA[NLP 세미나 발표는 Motivation, Word vector, 그리고 Deep neural net for NLP 3부로 이루어져 있다.

     1. Motivation
자연어 처리가 여타 다른 도메인과 다른점이 무엇인지 그 특징을 소개 하였고 자연어가 가지고 있는 의미론적 특징을 통해 기계학습과 인공지능의 확장 가능성에 대해 고찰해 볼 시간을 가져 보고자 하였다. 또한 많은 기게학습 알고리즘 중 딥러닝이 가지고 있는 독특한 장점을 자연어 처리와 관련지어 설명하고자 하였다.

    2. Word Vector
자연어 처리의 첫 단추인 워드 벡터, 워드 레프리젠테이션, 또는 워드 임베딩의 개념과 의의를 소개하였다. 워드 벡터를 생성하는 방법 중 널리 사용되는 방법 몇 가지를 살펴 보았다,

   3. Deep neural net for NLP
3부의 목표는 자연어 처리를 위해 고안된 특수한 구조의 신경망 몇 가지를 살펴보는 것이었다. 따라서 가장 기초적인 신경망의 구조와 그 학습방법을 먼저 설명하고 콘볼루션과 리컬시브를 사용한 특수한 신경망 두 가지를 살펴 보았다. 

3부에서 연구실원들에게 생소한 리컬시브 뉴럴넷을 설명하는데 좀 더 발표의 방점이 찍혔어야 했다는 아쉬움이 남는다. 다음 세미나 에서는 발표에서 전달하고자 하는 주제를 여유시간에 맞게 선택과 집중을 해야겠다.]]></content>
		<date><![CDATA[20160519131153]]></date>
		<update><![CDATA[20160603092314]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[130]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 05. 24 회의록]]></title>
		<content><![CDATA[금일 회의때 확인한 회의 자료입니다.
https://www.dropbox.com/s/hfyce6hsr49qlzu/160524%ED%9A%8C%EC%9D%98%EC%9E%90%EB%A3%8C.zip?dl=0]]></content>
		<date><![CDATA[20160524223939]]></date>
		<update><![CDATA[20160524223939]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[131]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 05. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160526181750]]></date>
		<update><![CDATA[20160526181750]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[132]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/05/27][PGM seminar] Bayesian Networks fundamentals]]></title>
		<content><![CDATA[Probablistic Graphical Model 세미나의 첫 주차로 Bayesian Networks fundamentals에 대해서 발표를 진행하였습니다. 본 발표에서는 큰 관점에서 보았을 때, Bayesian Networks라는 것의 정의, 확률의 추론 등을 다루었습니다. 중요한 내용으로는 결합확률분포가 확률변수들의 조건부확률분포의 곱으로 표현되었을 때, 해당 식을 Graph로 표현한 것이 Bayesian Networks다 라는 점이고 개인적으로 I-map에 대한 이해도가 낮은 것 같아 공부를 더해야겠다는 생각이 들었습니다. 
결국 앞으로 우리가 해야할 공부는 Bayesian Networks를 토대로 구성된 classifier를 학습하는 문제일 것입니다.이는 결국 class variable에 MAP를 구하는 문제를 베이즈 규칙으로 ML을 푸는 문제로 바꾸는 것으로 볼 수 있고 결국 ML을 한다는 것은 Bayesian Networks에서 class variable과 dependent한 확률변수들의 조건부확률분포를 ML의 목적에 맞게 추정하는 문제로 귀결될 것입니다. 향후 세미나가 기대됩니다.]]></content>
		<date><![CDATA[20160527234213]]></date>
		<update><![CDATA[20160603130644]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[133]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[승가원 스승의날기념나눔 - 고려대학교 DSBA연구실]]></title>
		<content><![CDATA[스승의 날을 맞아, DSBA 연구실에서 장애가족 복지단체인 승가원에 기부를 하였습니다.
아래 링크는 승가원 홈페이지에 올라온 게시글 링크입니다.

http://www.sgwon.or.kr/board/view.php?&amp;addon=&amp;bbs_id=info12&amp;view_list=&amp;page=&amp;doc_num=280&amp;ss[fc]=0&amp;PHPSESSID=7dc5f13bc2dc18e550f1ac84ff3ab5af]]></content>
		<date><![CDATA[20160601063327]]></date>
		<update><![CDATA[20160906201629]]></update>
		<view><![CDATA[2223]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201606/574e03e20666f5870239.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo_03.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[134]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 05. 31 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160601182318]]></date>
		<update><![CDATA[20160601182318]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[135]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[2016/06/03][PGM seminar] Template Models]]></title>
		<content><![CDATA[Probabilistic Model을 현실문제에 적용시킬 때 사용할 수 있는 Graphical Model에 대해 발표하였습니다. 
가장 먼저 시간에 따라 변하는 궤적의 분포를 나타내는 Temporal Model과 이 모델의 기본 가정인 Markov 가정에 대해 설명했습니다. 2개의 시간구조에 대한 2-time-slice Bayesian Network를 기반으로 Dynamic Bayesian Network가 형성되고 이를 확장시켜 Ground Network를 모델링 할 수 있습니다. 
Markov가정에서 더 나아가 Hidden Markov Model을 설명하고 그 응용분야로써 음성인식에 대해 소개했습니다. 
마지막으로 반복해서 사용하는 object에 대해 모델 구조 또한 반복되는 Plate Model에 대해 설명하였습니다. Plate Model의 유형에는 Nested와 Overlapping plate가 있고 이 plate model을 사용하면 좀 더 전체적인 관점에서 근거있는 결론을 제시할 수 있다는 장점이 있습니다.]]></content>
		<date><![CDATA[20160603124947]]></date>
		<update><![CDATA[20160610141221]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[136]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 06. 03 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160603153340]]></date>
		<update><![CDATA[20160603153340]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[137]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 06. 07 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160607172327]]></date>
		<update><![CDATA[20160607172327]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[138]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 06. 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160610151900]]></date>
		<update><![CDATA[20160610151900]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[139]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/06/10][PGM seminar]structured conditional probability distribution]]></title>
		<content><![CDATA[deterministic separation, context-specific independency 등 tabular representation보다 복잡한 변수 간 조건부 독립 관계를 표현하는 여러 조건부확률분포의 구조를 살펴보았습니다(deterministic, context-specific CPD). 또는 입력변수들에 의해 있을 수 있는 redundancy를 해결하기 위해 조건부확률을 이용하여 예측할 수 있는 분포를 살펴보았습니다(noisy version deterministic, logistic CPD). 마지막으로 이산적/연속적 입력변수와 연속적 출력변수를 사용한 선형 가우시안 분포 및 응용 사례와 예시를 살펴보았습니다.]]></content>
		<date><![CDATA[20160610153024]]></date>
		<update><![CDATA[20160629122341]]></update>
		<view><![CDATA[32]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[140]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 06. 14 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160614192253]]></date>
		<update><![CDATA[20160614192253]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[141]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[데이터사이언스와 산업수학 워크샵 참석]]></title>
		<content><![CDATA[2016년 6월 17일 금요일에 DSBA 연구실에서는 서강대학교에서 열린 &lt;데이터사이언스와 산업수학 워크샵&gt; 에 참석하였습니다. 데이터사이언스와 산업수학에 대한 좋은 발표들을 들을 수 있던 시간이었습니다. 이번 워크샵에서는 강필성 교수님이 '머신러닝을 이용한 뉴스기사 극성 평가' 라는 주제의 강연을 하셨습니다.]]></content>
		<date><![CDATA[20160621204233]]></date>
		<update><![CDATA[20160621204233]]></update>
		<view><![CDATA[2225]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201606/57692829a8f6a9154052.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[KakaoTalk_20160617_230540018.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[142]]></uid>
		<board_id><![CDATA[4]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016.06.17 데이터사이언스와 산업수학 워크샵]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160621204358]]></date>
		<update><![CDATA[20160621204358]]></update>
		<view><![CDATA[7775]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/4/201606/5769287e89af34376647.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[KakaoTalk_20160617_230540018.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[143]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 06. 24 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160624213457]]></date>
		<update><![CDATA[20160624213457]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[144]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 06. 28 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160628144901]]></date>
		<update><![CDATA[20160628144901]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[145]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 06. 29 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160629193744]]></date>
		<update><![CDATA[20160629193744]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[149]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16.07.05 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160705154824]]></date>
		<update><![CDATA[20160705154824]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[150]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2016/07/05][PGM seminar] knowledge engineering &amp; variable elimination]]></title>
		<content><![CDATA[Probabilistic Model 중에서 knowledge engineering 과 variable elimination 대해 발표하였습니다. 
knowledge engineering은 문제를 판단하고 어떤 데이터를 사용할지, 문제를 해결할 때 필요한 것을 결정하는 과정을 말합니다. 강의에서 설명한 내용과 그외에 discriminative model과 generative model의 machine learning 에서의 차이점을 발표하였습니다. variable elimination에서는 조건이 주어졌을 때 variable elimination을 계산하는 conditional probability queries와 evidence가 주어졌을 때 확률값을 최대로 하는 방법인 Maximum a posteriro(MAP)와 algorithm에서의 time complexity에 대한 추가적인 내용에 대해 설명하였습니다.]]></content>
		<date><![CDATA[20160710133547]]></date>
		<update><![CDATA[20160712192953]]></update>
		<view><![CDATA[30]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[148]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 07. 05 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160705154724]]></date>
		<update><![CDATA[20160705154724]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[147]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/06/29][PGM seminar] Undirected graphical models]]></title>
		<content><![CDATA[Markov network는 Bayesian networks와 다르게 인과관계가 아닌 상호관계를 나타낼 수 있는 그래프로 undirected edge로 표현된다. factor를 이용하여 parameterizing 할 수 있으며 edge의 방향성이 없기 때문에 cyclic한 특징이 있다. 이런 특징 때문에 변수들이 conditional하게 독립일 수 있다. 따라서 조건부 확률에 의한 discriminated model을 예측하여 전체 모델을 예측할 수 있다. 이런 예측 방법은 변수들 사이에 복잡한 correlation 관계가 있어도 보다 쉽게 예측할 수 있다는 장점이 있다.]]></content>
		<date><![CDATA[20160630001351]]></date>
		<update><![CDATA[20160705172956]]></update>
		<view><![CDATA[31]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[151]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 07. 11 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160711225836]]></date>
		<update><![CDATA[20160711225836]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[152]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[[2016/07/12][PGM seminar] Variable elimination]]></title>
		<content><![CDATA[지난 세미나 시간에 이어서 variable elimination에 대해서 설명하였습니다.
chain-rule, bayesian network, markov network 에서의 variable elimination 절차,  시간복잡도, moralized graph, Induced graph, Elimination ordering에 대해서 알아보았습니다.
data간의 관계를 inference하고 structure을 구하는데 사용하기 위해서 probabilistic graphical models를 사용한다는 점을 교수님의 말씀을 통해서 알게 되었습니다.]]></content>
		<date><![CDATA[20160712132431]]></date>
		<update><![CDATA[20160719125459]]></update>
		<view><![CDATA[25]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[153]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 07. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160712154503]]></date>
		<update><![CDATA[20160712154503]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[154]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[[2016/07/19][PGM seminar]  Belief Propagation]]></title>
		<content><![CDATA[안녕하십니까.  금주 세미나 발표진행자 김형석입니다.
금주 세미나시간에는 지난 시간에 다룬 Variable Elimination을 이어서 cluster graph를 통한 Belief Propagation을 진행하는 방법과 왜 이를 진행하는지 이유에 대해서 알아보았습니다. 더불어 cluster graph의 형태를 갖춘 그래프의 성질 Family reservation과  Running Intersection property에 대해서 알아보았으며 이가 가지는 의미에 대해서 다루었습니다.
마지막으로 cluster graph중  Clique Tree는 어떠한 경우 일컫는지 알아보았으며, 이런 경우의 성질 또한 알아보았습니다.
이상입니다.]]></content>
		<date><![CDATA[20160719125902]]></date>
		<update><![CDATA[20160807205216]]></update>
		<view><![CDATA[30]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[155]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 07. 19 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160719160158]]></date>
		<update><![CDATA[20160719160158]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[156]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 07. 19 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160719162504]]></date>
		<update><![CDATA[20160719162504]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[157]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 07. 22 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160724171634]]></date>
		<update><![CDATA[20160724171634]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[158]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 07. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160726171841]]></date>
		<update><![CDATA[20160726171841]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[159]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 07. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160727075107]]></date>
		<update><![CDATA[20160727075107]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[160]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[[2016/07/26][PGM seminar] MAP Inference]]></title>
		<content><![CDATA[금주 세미나 시간에는 MAP Inference를 다루었습니다.

Optimal solution을 구할 수 없는 MAP inference를 위한 Max-Sum 방법이 어떤 것인지, 그리고 이 방법을 쓰는 이유와 장점을 Sum-product와 비교하여 알아보았습니다.
또한 Tractable MAP Problems에서는 현업에서 쓰이고 있는 Factor와 예시를 발표했습니다.
마지막으로 MAP inference를 좀 더 효율적으로 찾기 위한 Divide and Conquer 방법인 Dual decomposition이 무엇인지, 그리고 이 알고리즘의 원리에 대하여 알아봤습니다.]]></content>
		<date><![CDATA[20160727155554]]></date>
		<update><![CDATA[20160807204522]]></update>
		<view><![CDATA[31]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[161]]></uid>
		<board_id><![CDATA[4]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016.7.16 ~ 2016.7.17 하계 워크숍]]></title>
		<content><![CDATA[7월 16일부터 17일까지 여름 워크숍으로 강원도 영월에 다녀왔습니다. 관련 사진 첨부합니다.]]></content>
		<date><![CDATA[20160729134515]]></date>
		<update><![CDATA[20160801125202]]></update>
		<view><![CDATA[5988]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/4/201608/579ec7621de494128356.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[크기변환_KakaoTalk_20160717_113204727.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[162]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[2016/08/02][PGM seminar] Sampling MCMC]]></title>
		<content><![CDATA[금주 세미나에서는 sampling에 대해 다루었습니다.

통계학에서 다루는 sampling과 그때의 추정치, 그리고 그 추정치의 성능을 파악하는 것을 기본으로 현재 우리가 배우는 PGM에서 어떻게 sampling을 할 수 있는지에 대해 큰 흐름을 잡고 알아보았습니다.
네트워크와 같이 변수가 여러개이고, 종속적인 관계를 갖는 상황에서의 sampling에 기본적인 sampling 방법론을 사용하지 못하기 때문에 MCMC라는 방법론을 적용해야 한다는 사실을 알고, 이후 MCMC의 대표격인 Gibbs sampling과 Metropolis Hastings algorithm에 대해 다루었습니다. 
마지막으로는 Marginal, MAP, Approximate가 가지는 특징과 적용 분야에 대해 정리하였습니다.]]></content>
		<date><![CDATA[20160802114700]]></date>
		<update><![CDATA[20160810135346]]></update>
		<view><![CDATA[31]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[163]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 08. 03 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160803104157]]></date>
		<update><![CDATA[20160803104157]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[164]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 08. 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160810111314]]></date>
		<update><![CDATA[20160810111314]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[168]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[16. 08. 16 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160817140750]]></date>
		<update><![CDATA[20160817140838]]></update>
		<view><![CDATA[9]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[169]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[16. 08. 16 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160817200406]]></date>
		<update><![CDATA[20160817200939]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[170]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 08. 18 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160818143204]]></date>
		<update><![CDATA[20160818143204]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[171]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 08. 18 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160819133612]]></date>
		<update><![CDATA[20160819133612]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[172]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 08. 31 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160831192518]]></date>
		<update><![CDATA[20160831192518]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[173]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2016/08/31][PGM seminar] PGM learning; introduction, parametric learning]]></title>
		<content><![CDATA[이번 세미나에선 PGM 학습의 개괄과 모델 구조와 결측이 없는 완벽한 데이터가 주어졌을때 Bayesian network와 Markove network를 학습 하는 방법을 같이 공부하여 보았다. 

[1] Introduction to learning
PGM이 학습과정을 개괄 하며 여타 다른 기계학습 방법과의 차이점을 강조 하였다. 또한 구조와 데이터의 완전성 여부의 조합에 따라 PGM 학습이 처할 수 있는 상황을 살펴 보았다. 이번 세미나 에서는 이 중 구조가 미리 주어져 있고 데이터가 완벽할 때 학습과정을 배운다.

[2] Learning Bayesian network
베이지안 네트워크가 함축하고 있는 독립성이 MLE를 사용할때 어떻게 활용되는지 중점적으로 살펴 보았다. 로컬 라이클리후드를 기억하자!

[3] Learning Markov network
노멀라이징 컨스턴트 때문에 마코브 네트워크는 배이지안 네트워크 보다 학습의 복잡도가 높다. 로그 리니어 모델로 마코브 네트워크를 새롭게 수식화 하고, 컨케이비티를 가짐을 보임으로써 전역 최적점이 있음을 확인 하였다. 전역 최적점이 있으므로 그래디언트 방법으로 쉽게 학습 가능하다.]]></content>
		<date><![CDATA[20160901003904]]></date>
		<update><![CDATA[20160905191203]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[167]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 08. 11 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160814162847]]></date>
		<update><![CDATA[20160819133522]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[166]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[2016/08/10][PGM seminar] Decision Theory, ML-class Revision]]></title>
		<content><![CDATA[6주차 PGM 강의에서는 ML class로 같이 있으므로 같이 진행을 하였습니다.

[1] 먼저 Bias Variance Decomposition에 대하여 수식으로 이해를 하였습니다.

[2] 과적합을 피할수 있는 방법들에 대하여 열거 하였고 그에 대한 설명을 하였습니다.

[3] R을 이용하여 실제 복잡도에 따른 Bias Variance의 실제 값을 산출하고 이해하였습니다.

[4] Maximum Expected Utility에 대하여 다루었고 Utility function에 대하여 언급하였습니다.

[5] Utility != pay off 라는 것을 다루었고 그에 대한 보험 예제를 들었습니다.

[6] 마지막으로 Action이전에 정보 이득이 있을시의 Utility를 최대화 하는 선택을 할 수 있으며 그에 대한 정보를 효용 함수에 근거하여 수치로 나타낼 수 있다는것을 다루었습니다.

[7]  실제 상황에서 Utility function을 만들수 있는가에 대한 토론을 하였으며 그에 대한 교수님의 첨언과 서로의 생각을 통해 좀더 생각해 보는 시간을 가졌습니다.

이상입니다.]]></content>
		<date><![CDATA[20160811141017]]></date>
		<update><![CDATA[20160818110938]]></update>
		<view><![CDATA[54]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[174]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 08. 31 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160901162425]]></date>
		<update><![CDATA[20160901162425]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[175]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 08. 31 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160902133459]]></date>
		<update><![CDATA[20160902133538]]></update>
		<view><![CDATA[7]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[176]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 09. 05 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160905155459]]></date>
		<update><![CDATA[20160905155459]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[177]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 09. 05. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160905160021]]></date>
		<update><![CDATA[20160905160021]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[178]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[2016/09/05][PGM seminar] Structure learning in Bayesian network]]></title>
		<content><![CDATA[이번주 세미나에서는  full data가 주어졌지만 network 구조를 알지 못할 때, 구조와 parameter를 찾는 structure learning에 대해 공부했습니다. 

1. Overview
Structure learning의 필요성, 중요성, 그리고 여러 learning의 방법들을 간단히 소개하면서 structure learning에 대한 기본 개념에 대해 배워보는 시간이었습니다. 완전하지 않은 network들의 특징을 비교하면서 정확한 structure의 중요성을 강조하였고, 이후에 등장할 내용인 score-based learning에 대해 소개했습니다.

2. Scoring
가능한 candidate 그래프 중 어떤 그래프가 데이터를 설명하는데 적합한지 근거를 찾기 위해 그래프에 점수를 부여하는 방식이 scoring입니다. 크게 likelihood score, Bayesian score, BIC score를 소개했습니다. Overfitting의 문제가 있는 likelihood score의 단점을 극복하기 위해 complexity를 부여하는 scoring 방식이 Bayesian, BIC 입니다.

3. Learning
각 network에 score를 부여한 후 어떤 그래프를 선택할지에 대한 부분이 learning입니다. 적합한 그래프를 찾는 과정에서 edge를 추가, 삭제, 역방향으로 전환하는 조취들을 취하기도 하는데, 여러 조취 중 변화 후의 score가 증가하는  방향으로 움직입니다.]]></content>
		<date><![CDATA[20160905182440]]></date>
		<update><![CDATA[20160918023013]]></update>
		<view><![CDATA[22]]></view>
		<comment><![CDATA[3]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[179]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 09. 05 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160905183312]]></date>
		<update><![CDATA[20160905183339]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[180]]></uid>
		<board_id><![CDATA[10]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[ㅁ]]></title>
		<content><![CDATA[ㅁ]]></content>
		<date><![CDATA[20160905220345]]></date>
		<update><![CDATA[20160905220345]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[183]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 09. 02 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160911153319]]></date>
		<update><![CDATA[20160911153438]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[2]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[184]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 09. 06 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160911155939]]></date>
		<update><![CDATA[20160911155951]]></update>
		<view><![CDATA[8]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[2]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[185]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/09/12][PGM seminar] Learning with incomplete data]]></title>
		<content><![CDATA[[2016/09/12][PGM seminar] Learning with incomplete data

이번 주 세미나에서는 Partially observed data가 주어져있을 때, bayesian network를 학습하는 방법에 대해 설명했습니다. 학습하는 방법론에는 gradient ascent, expectation maxmization 방법이 있고 그외에 기저가 되는 개념으로써observability  model을 설명하였습니다. complete data의 경우에는 likelihood가 concave한 형태의 함수이지만 Partially observed data의 경우 missing value 또는 hidden variable을 marginalize해야하기때문에 local maxima가 존재하는 매우 복잡한 함수가됩니다. 따라서 이러한 경우 optimization 방법론으로 gradient ascent, expectation maxmization 방법론을 사용할 수 있습니다. gradient ascent의 경우 주어진 task가 확률을 학습하는 것이기 때문에 re-parameterization 과정이 필요하지만 em은 처음부터 likelihood에 대해서 설계되었기때문에 그러한 과정이 필요가 없습니다. 또한 gradient ascent의 경우 maximum을 찾아가는 과정이 current estimate에서 gradient를 계산하여 더해가는 과정이나 em은 current estimate에서 local-approximation으로 concave한 function을 만들고 (closed-form) 해당 function을 최대값으로 만드는 estimate로 이동해가는 과정입니다. 이 과정에서 다음 단계의 likelihood가 왜 항상 커지는 지를 증명하였습니다. 또한 em을 해야하는 데에서 문제점을 설명하였는데 parameter convergence와 likelihood convergence가 다름을 설명하였고 이 과정에서 parameter convergence를 하고자한다면 training data에 overfitting 문제를 일으킨다는 점을 설명하였습니다.]]></content>
		<date><![CDATA[20160912173906]]></date>
		<update><![CDATA[20160927210820]]></update>
		<view><![CDATA[33]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[186]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 09. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160912175104]]></date>
		<update><![CDATA[20160926230410]]></update>
		<view><![CDATA[7]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[187]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 09. 12. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160912180422]]></date>
		<update><![CDATA[20160912180739]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[188]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 09. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160913154023]]></date>
		<update><![CDATA[20160913154023]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[189]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 09. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160913154928]]></date>
		<update><![CDATA[20160913154928]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[190]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 09. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160926155734]]></date>
		<update><![CDATA[20160926155734]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[191]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 09. 26. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160926170836]]></date>
		<update><![CDATA[20160926170836]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[192]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 09. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160926232452]]></date>
		<update><![CDATA[20160926232452]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[2]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[193]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 09. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20160927153116]]></date>
		<update><![CDATA[20160927153116]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[194]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[2016/09/26][RL seminar] Introduction to Reinforcement learning]]></title>
		<content><![CDATA[이번 시간에서는 강화학습의 기본 개념에 대해 다루었습니다.

1.  강화학습의 정의
강화학습이란 agent가 주체가 되어 action을 취하고 그에 따른 reward를 기반으로 그 다음 action을 결정하는 학습입니다. 미래에 쌓일 reward의 누적값을 최대화 하는 것이 강화학습의 목표입니다

2.  강화학습 구성 요소
강화학습은 기본적으로 environment, reward, 그리고 action으로 구성됩니다. 이러한 요인들을 쌓아 나열한 것이 history인데 이 history를 매번 다 사용하기엔 무리가 있기때문에 history 정보를 요약한 state를 사용합니다. state는 주로 markov를 적용하여 현재 상태 직전의 정보만을 사용합니다. 

3.  Agent 구성요소
agent를 구성하는 3가지 요인들은 policy, value function, model 세가지 입니다. policy는 agent의 behavior,  value function은 미래 reward의 평균값을 예측하는데 사용됩니다. 마지막으로 model은 environment가 다음단계에 무엇을 할지 예측하는 역할을 합니다.

4. 강화학습 문제
강화학습에서는 학습 방식보다는 강화학습 문제인지를 파악하는 것이 중요합니다. 강화학습 문제의 여러 종류로써 reinforcement learning VS planning, Exploration VS Exploitation, Prediction VS Control을 알아보았습니다.]]></content>
		<date><![CDATA[20160928133800]]></date>
		<update><![CDATA[20161007012320]]></update>
		<view><![CDATA[39]]></view>
		<comment><![CDATA[11]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[195]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[16. 09. 26 회의록]]></title>
		<content><![CDATA[1.	일시 및 장소 : 2016-10-04 16:30 (NC soft 사옥) 
2.	회의 참석자: 강필성 교수님, 김형석, 김보섭, 서덕성, NC 관계자
3.     회의록 작성자 : 김형석, 서덕성]]></content>
		<date><![CDATA[20161005200924]]></date>
		<update><![CDATA[20161005200924]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[196]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 09. 29 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161007134954]]></date>
		<update><![CDATA[20161007134954]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[197]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[[2016/10/10][RL seminar] Markov Decision Processes]]></title>
		<content><![CDATA[이번 시간에는 강화학습의 핵심인 Markov Decision Processes에 대해 다뤘습니다.

1. 주요 용어 정의
상태(state) : 에이전트가 환경을 관측한 결과 (마코프 가정)
행동(action) : 에이전트의 의사결정
보상(reward) : 에이전트가 행동을 취할 때 환경으로부터 피드백 받는 스칼라 실수값
감쇄계수(discount factor) : 미래 보상값을 현재가치로 할인하기 위한 값
정책(policy) : 상태 s가 주어졌을 때 행동 a를 할 확률값을 반환하는 함수

2. 상태가치함수 (state value function)
t시점의 상태가치를 나타내는 함수, Gt의 기대값을 뜻함
t시점의 상태가치는 즉시보상과 t+1 시점 이후의 모든 미래 보상을 현재가치로 환산한 값으로 분해 가능
상태가치함수가 곧 벨만방정식이며 MDP문제는 이 방정식의 해를 구하는 것을 뜻함

3. 행동가치함수 (action value function)
상태s에서 행동a를 취했을 때 기대되는 현재 및 미래보상의 총합
행동가치에는 t+1시점의 상태가치 포함
에이전트는 행동가치가 최대인 행동을 하게 됨

4. 벨만방정식의 최적화
최적 상태가치, 최적 행동가치, 최적 정책은 서로 물고 물리는 관계
어느 하나를 정확히 알면 모든 문제가 풀림
벨만방정식은 닫힌 해가 없기 때문에 반복적인 방식으로 해를 찾아야 함]]></content>
		<date><![CDATA[20161010160510]]></date>
		<update><![CDATA[20161012105607]]></update>
		<view><![CDATA[36]]></view>
		<comment><![CDATA[11]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[198]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 10. 10. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161010162825]]></date>
		<update><![CDATA[20161010162825]]></update>
		<view><![CDATA[7]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[199]]></uid>
		<board_id><![CDATA[6]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[16. 10. 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161010165035]]></date>
		<update><![CDATA[20161010165035]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[200]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 10. 10 회의록]]></title>
		<content><![CDATA[1. 일시 및 장소 : 2016-10-10 15:30 (교수님 연구실) 
2. 회의 참석자 : 강필성 교수님, 김형석, 류나현, 김보섭, 서덕성
3. 회의록 작성자 : 서덕성]]></content>
		<date><![CDATA[20161010204703]]></date>
		<update><![CDATA[20161014133326]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[201]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 10. 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161011005058]]></date>
		<update><![CDATA[20161011005058]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[202]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 10. 14 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161014133228]]></date>
		<update><![CDATA[20161014133403]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[203]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 10. 14. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161014152000]]></date>
		<update><![CDATA[20161014152308]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[204]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[[2016/10/17][RL seminar] Planning by Dynamic Programming]]></title>
		<content><![CDATA[이번 시간에는 Dynamic Programming을 이용한 Planning에 대해서 다뤘습니다.

1) 용어 정의
Dynamic Programming 용어의 유래에 대해 살펴 보고, Optimal Substructure와 Subproblem이 Nesting(중첩)된다는 특징이 있다는 것을 살펴봤습니다. Planning은 Environment에 대한 Model을 알고 있을 때 문제를 푸는 것이고, Learning은 Model에 대한 정보 없이 Environment와 상호 작용하며 문제를 푸는 것을 말합니다.
Planning은 Prediction과 Control 두 단계로 나뉨을 알아보았습니다.

2) Policy Evaluation
Bellman Expectation Backup을 반복해서 현재 State에 대한 가치를 얻을 수 있음을 알아보았습니다.
Synchronous Backup을 이용하여 한꺼번에 가치 함수를 업데이트하고 작은 Gridworld 예제에서 어떻게 Policy Evaluation을 할 수 있는지 보았습니다.

3) Policy Iteration
정책에 대해 Evaluate하고 Improve하는 과정을 반복하는 것을 말하며, Bellman Expectation 알고리즘으로 Evaluate하고 Greedy Algorithm으로 Improve를 계속 반복합니다. 반복회수를 줄이기 위해 종료 조건으로 e-convergence를 사용하거나 반복회수(k)를 지정할 수 있음을 살펴 보았습니다.

4) Value Iteration
Bellman Optimality Backup을 반복적으로 계산하며, Synchronous Backup을 사용합니다. Policy Iteration과는 다르게 명시적인 정책이 없으며 중간의 가치함수는 어떠한 의미도 가지지 않습니다.

5) Asynchronous Dynamic Programming
Synchronous DP의 계산 복잡도 문제를 해결하며, In-place Dynamic Programming, Priortised sweeping, Real-time dynamic Programming 방법에 대해서 알아보았습니다. Full-Width Backup의 계산복잡도가 높아 Sample Backup이 있음을 언급하였습니다.

6) Contraction Mapping
Policy Evaluation&amp;Iteration과 Value Iteration이 결국 수렴하고 하나의 유일한 값을 가짐을 Contraction Mapping 을 통해 증명해보았습니다.

감사합니다.]]></content>
		<date><![CDATA[20161017145935]]></date>
		<update><![CDATA[20161018224335]]></update>
		<view><![CDATA[47]]></view>
		<comment><![CDATA[11]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[205]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 10. 21 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161021132054]]></date>
		<update><![CDATA[20161021132120]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[207]]></uid>
		<board_id><![CDATA[11]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[asdf]]></title>
		<content><![CDATA[asdf]]></content>
		<date><![CDATA[20161021141047]]></date>
		<update><![CDATA[20161021141047]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[208]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 10. 21. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161021164126]]></date>
		<update><![CDATA[20161021164126]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[206]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016 빅스타 선발대회 우수상 수상]]></title>
		<content><![CDATA[2016년 10월 20일 목요일, DSBA 연구실 학생 3명(김준홍, 김해동, 서덕성)이 경기콘텐츠진흥원 주관의 2016 빅스타 선발대회에서 우수상을 수상하였습니다. SNS상에 무차별적으로 등장하는 스팸을 막는 알고리즘 제안으로 대회에 출전했습니다.]]></content>
		<date><![CDATA[20161021133658]]></date>
		<update><![CDATA[20161021180927]]></update>
		<view><![CDATA[2104]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201610/58099b6a400025590087.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[빅스타 수상.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[209]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 10. 31 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161101145302]]></date>
		<update><![CDATA[20161101145302]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[210]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 11. 01 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161101210716]]></date>
		<update><![CDATA[20161101210716]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[2]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[211]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 11. 01. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161102124457]]></date>
		<update><![CDATA[20161102124457]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[212]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 11. 04. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161104151803]]></date>
		<update><![CDATA[20161104151803]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[213]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 11. 04 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161105191651]]></date>
		<update><![CDATA[20161105191651]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[214]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[2016/10/17][RL seminar] Model-free prediction]]></title>
		<content><![CDATA[금일 세미나에서는 Model-free prediction에 대해 다뤘습니다.

MDP는 현실세계에서 Environment를 모두 알 수 없는 경우가 많아 적용하기 어렵다는 한계점이 있습니다.
이를 극복하기 위해 sample에 기반한 value function을 update를 이용해 취하는 방법론을 소개했습니다.
sample을 이용해 value를 update하는 방식에 따라 크게 Monte-Carlo와 Temporal-Difference 방법론이 있습니다.

[1] Monte-Carlo learning
episode를 끝까지 얻을 수 있는 경우에 가용한 방법론이며 이는 bootstrapping이 없이 unbiased estimate를 할 수 있다는 장점이 있습니다. value를 update할 때 좀 더 편리하게 update하는 방법으로 incremental mean 방식을 취했습니다. MC 방법론에서 issue 사항으로 볼 수 있는 것이 두번 이상 방문하는 state를 어떻게 다룰지 입니다. 처음 방문한 state만 인정하는 방법인 first-visit와, 모든 방문을 인정하는 every-visit이 있었습니다.

[2] Temporal-Difference learning
episode를 끝까지 가져갈 필요 없이 online으로 즉각적으로 다음 step을 이용해서 update하는 방식입니다. MC와는 다르게, TD target이라는 값을 이용해 update를 진행하는데, 이때 다음 step이후의 value function을 추정해서 사용합니다.

MC는 Markov structure를 구축 불가한 경우 효과적이며, TD는 그 반대의 경우 효과적입니다.

[3] TD(lambda)
본 방법론은 1-step 앞만을 고려하는 TD(0)에서 확장된 개념으로, n-step까지 고려하며, weighted sum을 하는 방식입니다. 이를 적용하는 방법으로 가장 쉽게 생각할 수 있는 Forward-view TD(lambda)가 있는데, 이는 episode를 끝까지 봐야한다는 점에서 MC와 같은 한계를 가지고 있습니다. 본 한계를 극복하기 위해 Backward-view TD(lambda)를 소개하는데 여기서는 중요한 state를 frequency heuristic, recency heuristic 기반으로 정의를 하게 됩니다.]]></content>
		<date><![CDATA[20161107162947]]></date>
		<update><![CDATA[20161109133306]]></update>
		<view><![CDATA[36]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[215]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 11. 09 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161109173401]]></date>
		<update><![CDATA[20161109173401]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[216]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 11. 07. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161110174331]]></date>
		<update><![CDATA[20161110174331]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[217]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 11. 11 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161111142914]]></date>
		<update><![CDATA[20161111142914]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[218]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 11. 11. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161111152309]]></date>
		<update><![CDATA[20161111152309]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[219]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 11. 15 회의록 (NC soft 사옥 미팅)]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161116171451]]></date>
		<update><![CDATA[20161116171514]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[220]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2016 대한산업공학회 추계학술대회 참석]]></title>
		<content><![CDATA[2016 대한산업공학회 추계학술대회

1.장소 및 기간
2016/11/19, 고려대학교

2. 참가인원
박사과정 : 김준홍, 김창엽
통합과정 : 김형석, 박민식
석사과정 : 류나현, 김보섭, 김해동, 조수현, 서덕성, 이기창, 모경현, 박재선

3. 발표자
김준홍, 김형석, 박민식, 조수현, 서덕성

4. 수상
김보섭, 박민식, 조수현 연구원 산업융합논문공모전 우수상 수상

5. 사진첨부
개별 연구원의 발표사진 첨부합니다.]]></content>
		<date><![CDATA[20161121152752]]></date>
		<update><![CDATA[20161121153441]]></update>
		<view><![CDATA[1357]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201611/583293e8767525733734.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[DSBA logo.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[221]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회 – 서덕성]]></title>
		<content><![CDATA[2016 추계학술대회 에세이
서덕성

이번 학술대회는 고려대학교에서 진행됐습니다. 따라서 학회가 진행되기 위한 여러 작업들을 실제로 도우며 학술적인 부분 뿐만 아니라 산업공학회 자체를 좀 더 바라보는 기회가 되었습니다.

춘계때는 BI데이터마이닝학회에 가서 Multi-co-training에 대해 발표를 해서 산업공학회에서 발표를 하지 못했는데, 이번 추계 학술대회에서는 제 연구를 통해 발표를 했다는 점에서 개인적으로 기억에 남는 학회였습니다.

제가 발표한 주제는 “Word sentiment score evaluation” 즉, 도메인에 기반한 감성사전을 워드 임베딩을 통해 얻은 저차원상의 공간에서 네트워크기반의 준지도학습을 이용해 얻는 방법론이었습니다.
특히 기억에 남는 사건으로는 본 연구를 진행하며 결과를 내기 위해 앞만 보고 달렸는데, 학회준비 시즌이 되어 돌아보니, 우리가 만든 감성사전과 비교할 대상이 없다는 것이었습니다. 급히 Lasso-regression을 이용해서 모형을 만들었지만, 학회 발표에 활용하지는 않았습니다. 아마도 6개 영화사 데이터를 모두 사용할 때 비교 대상으로 사용할 것으로 생각됩니다.
그리고 발표 후에 질문을 많이 받았는데, 그런 질문들을 답변하며 우리의 연구가 어떤 부분에서 기여점이 있고, 어떤 부분은 부족한 것인지 알 수 있었으며 추후 연구에 보탬이 될 것이라 생각합니다. 또한 제가 발표하는 부분을 연구실원이 촬영해줘서 그것을 기반으로 제 발표에 관해 고민해볼 수 있는 것 또한 좋은 경험이었다 생각합니다.

여러 발표 중 관심을 가졌던 발표는 Denoising Auto-Encoder를 이용한 발표였습니다. 2016년 겨울방학에 진행했던 Deep learning 세미나에서 배웠던 Unsupervised learning 방법론 중 하나로 Auto-Encoder를 알게 되고 흥미를 가졌었는데, 이를 이용해서 조금 더 general한 모형을 만드는데 이용하는 것이 인상깊었습니다. 아직까지는 Deep learning의 기본적인 부분 까지만 이해한 상황이지만, 최근 가장 Hot한 방법론이므로 다시 위고 교수님의 Deep learning 강의를 들어볼 계획에 있습니다. 처음 들었을 때는 정말 많이 어려웠지만 지금은 그래도 조금 배경지식이 생겼으니 이해도 잘 되고 더 재미있게 들을 수 있을 것이라 생각합니다.

통합하여 느낀점은 역시 제 연구가 텍스트 관련 주제라는 점 때문에 텍스트 관련 연구에 눈길이 갔는데, 생각보다 적은 연구실에서 진행하고 있음을 알게 되었습니다. 물론, 산업공학과만이 모인 것이기에 적은 양의 발표가 진행되었다고 생각할 수 있겠지만 NLP관련 연구를 더 만날 수 있었다면 개인적으로 더 흥미를 갖고 보지 않았을까 생각했습니다.

다음 학회에서도 재미있는 연구들을 많이 볼 수 있을 것 같아 기대가 됩니다.]]></content>
		<date><![CDATA[20161121164618]]></date>
		<update><![CDATA[20161121164618]]></update>
		<view><![CDATA[1317]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201611/5832a64af01a15115753.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[DSBA logo.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[222]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 11. 21 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161121174624]]></date>
		<update><![CDATA[20161121174624]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[223]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 11. 21. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161121174742]]></date>
		<update><![CDATA[20161121174742]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[224]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회 – 김보섭]]></title>
		<content><![CDATA[- 발표후기
발표제목 : 관광지 평가 댓글을 활용한 만족도 요인 추출과 관광지 평가

 지난 학기 비정형 데이터분석의 수업의 결과물을 제 5회 산업융합 활성화 방안 연구 및 사례연구 논문 공모전에 투고하여 수상을 하는 과정에서 나름대로 느꼈던 점들을 기술해보고자 한다. 첫째로, 논문의 결과물을 발표자료를 구성하는 과정에서 느꼈던 점은 장표의 구성의 중요성이다. 장표를 작성함에 있어 꼭 필요한 것만 남기고 그 이외의 것은 버리는 것이 가장 중요하다는 생각이 들었다. 실제로 연구를 진행한 연구자에 입장에서는 연구의 세세한 모든 절차가 중요하다는 생각이 들지만 해당 연구를 평가하는 사람의 관점에서는 그러한 세세한 것보다는 연구의 아이디어와 큰 줄기가 훨씬 이해하는 데에 있어서 중요하기 때문일 것이다. 실제로 이러한 점은 비단 연구 결과의 발표뿐만 아니라 사람과 사람사이의 소통에 있어서 가장 중요한 것일 수 있다는 생각이 들었다. 둘째로, 직접 발표를 하진 않았지만 심사위원들의 질의에 대답하는 과정에서 느낀 점으로 경청의 중요성을 느끼게 되었다. 실제로 다른 참가자의 발표와 질의응답을 지켜보면서 느낀 점은 참가자가 심사위원의 질의의 의도와는 다른 응답을 주는 경우가 많은 경우가 많았는 데, “긴장해서 그런 걸 수도 있지만 경청을 하지못했기 때문이 아닐까?”라는 생각이 들었다. 이는 비단 발표와 평가라는 측면 뿐만 아니라 연구 결과의 공유에서도 문제가 될 소지가 있다고 생각한다. 따라서 앞으로 연구하는 데에 있어서도 같이 연구하는 사람들과 오해의 소지가 없도록 핵심을 공유하기위해 나부터 다른 연구자들의 질문의 의도를 파악할 수 있도록 경청하는 자세를 가져야겠다는 생각이 들었다.

- 청취후기

 이번에 고려대학교에서 열린 추계학술대회에서는 개인적으로 발표일정이 없어 개인적으로 세션을 집중해서 들을 수 있는 좋은 기회였다. 인상 깊었던 발표 내용은 두 가지로 하나는 고려대학교 데이터마이닝 및 품질관리 연구실의 박성호 박사과정이 발표한 “복합적 표본선택 기준을 활용한 반 교사 능동학습 기법”이라는 내용이다. 해당 발표내용은 과거에 접해보지 못했던 새로운 내용이었으며 그 내용의 이론적 아이디어는 특수한 목적을 만족하는 모형을 학습하기위해 학습 데이터를 능동적으로 고르자는 내용이다. 예로써 모형의 분산을 줄이는 방향으로 학습을 하는 것도 위 방법론의 일종이라고 생각할 수 있다. 해당 발표 내용은 주로 선형회귀분석의 비용함수에 새로운 제약식을 더함으로써 학습에 필요한 데이터 마저 선별할 수 있다는 내용이었고, 비단 회귀분석 뿐만 아니라 분류기의 학습에도 응용할 수 있다. 다른 하나는 동 연구실의 박영준 박사과정이 발표한 “Convolutional Autoencoder with Hybrid Learning”이라는 내용이다. 해당 내용은 어떠한 이미지를 feature extraction할 때, image의 label 정보도 고려하여 feature extraction을 할 수 있도록 DNN의 hidden layer와 autoencoder의 hidden layer를 결합시켜 동시에 학습하는 방법이다. 학습을 위해서 loss function을 autoencoder의 reconstruction error와 dnn의 cross-entropy를 적당히 가중치를 주어 정의하였고 hidden node의 값들을 추출하였다. 실험을 위한 데이터는 이미지 분야에서 유명한 데이터인 mnist 데이터를 기반으로 진행하였다. 연구의 결과물은 물론 훌륭하였지만 내가 느낀 이 연구자의 가장 훌륭한 점은 자신의 연구 결과물을 얼마나 효과적으로 보여줄 지 고민하였고 그 것을 토대로 청중에게 확실히 잘 전달했다는 점이다. 결과를 보여주는 방식에 너무나 깊은 고민이 들어가 있는 것을 느꼈다. 다음 연구를 진행하고 발표할 때에는 내 연구를 다른 사람에게 효과적으로 전달하는 방식을 깊이 고민하여 연구를 진행해야겠다는 필요성을 느꼈다.]]></content>
		<date><![CDATA[20161121214413]]></date>
		<update><![CDATA[20161121214601]]></update>
		<view><![CDATA[1329]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[225]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[[2016/11/22][RL seminar]Model-free Control]]></title>
		<content><![CDATA[지난 세미나에 이어 model-free contro에 대해 다뤘습니다. DP와 마찬가지로 prediction에서 policy를 evaluation하고 control 단계에서  policy를 개선시킵니다. 이러한 iteration 과정을 통해 optimal policy에 도달하게 됩니다. 

1) Model-free
model-free가 model-base와 다른 점은 sample을 통해 학습하기 때문에 reward와 transition matrix를 알 수 없으므로 state value 함수가 아닌 action value 함수를 이용하여 업데이트합니다. 또한 학습하면서 local optimum에 빠지지 않도록 epsilon-greedy를 사용하여 학습합니다.

Model-free learning은 크게 on-policy와 off-policy로 나뉩니다. 
2) On-policy
On-policy는 현재 policy 상에서 학습하면서 optimal policy를 구하는 방식으로 Monte-Carlo 방식과 action value 함수를 이용하는 TD인 sarsa로 구분됩니다. 

3) Off- policy
Off- policy는 학습하고 있는 policy에 대해 실제 구하고자하는 policy와의 차이가 있는 경우 사용하는 방법으로 on-policy보다 regret이 낮아 실제 적용하기 더 편리하며, off-policy에는 importance sampling과 Q-learning이 있습니다. importance sampling은 variance가 높고 학습시간이 오래 걸려 실제 잘 사용되지 않습니다. Q-learning은 off-policy TD의 단점을 보완하여 target policy와 behavior policy를 달리하여 학습하며, 이로 인해 regret을 낮추면서 학습을 더 빠르게 마칠 수 있습니다.]]></content>
		<date><![CDATA[20161122001709]]></date>
		<update><![CDATA[20161207074811]]></update>
		<view><![CDATA[38]]></view>
		<comment><![CDATA[14]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[226]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회, INFORMS 학회 후기 – 김준홍]]></title>
		<content><![CDATA[[1] 산업공학회 발표 관련

     [1-(a)] 제목 
인스타그램에서 학습된 단어와 게시글의 메타 정보를 이용한 페이스북 텍스트 스팸필터링 알고리즘

     [1-(b)] 내용
  본 연구는 학기 중 수업의 프로젝트 결과물중 하나이며, 평소에 SNS의 스팸 문제를 해결하고 싶었던 것을 재미로 시작한 과제이다. 해당 발표에서는 Image에 대한 결과는 나중을 위해 제외하였다. 먼저, 해당 연구의 문제점중 하나는 Facebook에 있는 크롤링된 데이터에 대하여 Supervised learning의 한계점 중 하나인 목표변수의 부재이다. 이를 위하여 약 180만건의 Facebook자료와 약 80만건의 Instagram 데이터를 사용하였다. Instagram기반의 모델을 통한 Transfer learning을 행하였으며 그 사후 확률값과 여러 가지의 메타변수를 통하여서 가변적인 SNS의 특성을 고려한 분류기 모델을 최종적으로 생성하였고 F1-measure기준 약 96%의 성능을 보였다. 본 연구의 시사점은 본 연구 주제에서 만큼은 실제 클래스간의 불균형이 극심한 스팸 주제에서 이를 효과적으로 대처 할 수 있다는 점이며, 누구나 사용할 수 있는 즉, 데이터의 취득권이 필요없는 프레임워크라는 점으로 생각된다.  

    [1-(c)] Q&amp;A
   Q1. Facebook label의 전체 자동화가 아닌 어느정도의 라벨링 작업이 필요한 것인가?
   A1. 그렇습니다. 하지만 그 경우의 수가 확연하게 적은 것을 확인할 수 있으며, 이 특성을 통하여서 라벨링을 현실적          으로 행할 수 있었습니다.

   Q2. Instagram의 사후확률을 어디서 사용하였는가?
   A2. Facebook classifier에서 다시 input variable로 사용하였습니다.

Q3. Model B(Facebook meta variable만으로 구성된 분류기)와 Model A(Instagram에서 생성된 분류기)를 제한한      알고리즘(Model C)와의 성능 평가는 어떻게 되었는가?
A3. 6가지의 Valid Index를 기반으로 T-test를 하였으며 이를 기반하여 통계적으로 유의미하게 성능이 좋다고 말 할      수 있습니다. 특히나 해당 도메인에서 중요한 지표인 Spam 클래스가 Ture라고 가정하였을 때 Sensitivity와 F1-measure에서 성능 개선인 드라마틱하게 있는 것이 특징이며, 재미있는점은 Model B가 오히려 Model A보다 성능이 좋다는 것은 SNS에서 정보를 얻기위해서 Bag of Words기반의 취약점이 있는데, 이를 해결하게 위하여 오히려 해당 분야에서는 텍스트의 구조적으로 다름이 더 유의미 하다는 것입니다.

   [2] 산업공학회 청취 관련

      [2-(a)] 다중 속성 네트워크를 이용한 내용기반 필터링 영화 추천시스템
Item-based CF모델에서 Network의 특성을 기반으로 한 추천 알고리즘을 설명하여 주셨다. Item based CF방법은 User Based CF에 비하여 Cold Starter와 초기 User들의 불확실성에 대한 문제를 비교적 해결할 수 있는 방법이다. 실제로 high dimension에서의 Item별 similarity를 구할때의 문제점은 O(n^2)의 복잡도도 문제지만 명목변수의 수치화이며, 특정 기준의 세분화적 모델이라고 생각한다. 이를 위하여 min-hash jaccard와 명목변수의 수치화, 책 분야에서는 종교책과 자기개발서의 시간적 감가상각이 다르므로 시간적 penalty function을 다르게 하여 모델링을 하는 것을 볼 수 있으며 이를 몇개를 추천할것인가? 화면상에서 어떻게 효과적으로 전달할것인가?등의 연구가 많이 진행되고 있다. 해당 발표에서는 network centrality measure등을 이용하였다. 보통 가장 기본적인 User, Item, Hybrid CF기반의 추천에서는 특정 종합정보를 이용하여 network기준으로 해당 노드와 연결되어 있는 하나의 edge정보를 이용하여 best k를 찾아서 추천하지만, 해당 방법론은 좀더 많은 정보를 이용할 수 있다는점에서 유용하며, 재미있게 청취하였다. 해당 정보를 발전시킨다면 범용적으로 많은 도메인에서 성능 향상을 기대할 수 있을것이라고 생각된다.

      [2-(b)] Convolutional Autoencoder with Hybrid Learning
Autoencoder의 Output Node에서 class node를 추가하여 각각의 loss function의 가중치를 주어서 전체 loss function을 만든 뒤 생성된 차원축소된 정보를 기준으로 MNIST, CIFAR-10의 결과물을 기반으로 기존 방법론보다 좋다는 결론으로 이해하였다. 재미있는 연구라고 생각되었고, 발표도 이해하기 쉽게 작성하고 풀이해 주셨다. 만약의 input size가 크고 class가 많은 경우 차원죽소기준 적은양의 hidden node가 된다면 class node와의 연결되어 있는 weight의 개수가 늘어나기는 하지만 GPU memory 할당 범위안에 충분히 가용할 수 있으며 해당 방법론을 시행해 볼 수 있겠다는 생각을 하였다. 향후 필요한 상황이 생긴다면 연구에 실제적으로 도움이 될 것 같다고 생각하였다.

[3] INFORMS 발표 관련

     [3-(a)] 제목 
Strengthening free-text keystroke dynamics based user authentication based on user-adaptive feature construction for one-class classification

     [3-(b)] 내용
  본 연구는 PC keyboard기반의 한글 영어 기준 약 13,000이상의 keystroke 데이터를 기반으로 기존 방법의 손의 위치를 기반으로 한 변수대신 각 사용자마다의 di-graph의 순위를 기반으로 한 8가지의 변수를 할당한 rule을 생성하여 5가지 One-class classification을 기반으로 사용자 인증 알고리즘을 만들었으며 EER기준 T-test결과 기존 방법보다 유의미한 결론을 얻을 수 있었다.

   [4] INFORMS 청취 관련
    INFORMS 2016 annual meeting이 Nashville에서 개최 되었다. 개인적으로 처음 참석하는 학회였는데 큰 규모에 시간별로 듣고 싶은 session을 결정하는 것도 쉬운일이 아닐 정도였다. 그로인해 각 session에서는 상당히 세부적인 연구들로 이루어져 있는 것을 확인 할 수 있었다. 큰 맥으로 연구가 되는 것은 data analytics와 healthcare, uncertainty등에서 청취자들의 집중을 받고 있다는점을 느낄수 있었다. CS학회가 아니기 때문에 vision관련 등의 DNN관련 연구는 거의 없는 것도 큰 특징으로 다가왔다. 개인적으로 흥미있게 다가왔던 것은 머신러닝 기반의 알고리즘에 대한 hyper parameter의 최적화였다. 기존의 사용하고 있는 알고리즘들에서 파라미터 서치가 필요한 알고리즘들이 많은데 많은 연구에서 이를 최적화를 하려고 노력하는 것이 보였으며 이에 대한 청취자들의 열기가 높은 것을 확인 할 수 있었다. 또한 불확실성에 대한 데이터에 대한 것을 어떻게 효과적으로 사용할 것인지, Network의 similarity를 계산할때의 cut-off에 따른 구조적 변화를 고려하여 사용하는 것, 이미 기술적으로 많이 사용하고 있는 hyper space에서의 data를 나누어서 최적의 알고리즘을 할당하는 향상된 방법론, 실제 회사에서의 제약사항에 대한 최적식에 대한 것들을 볼 수 있는 좋은 자리 였으며, 특히 MIT에서 발표한 Optimal tree관련 연구는 이해하기가 쉽지 않았지만, 결과를 고려하였을때 차근차근 다시 한번 곱씹으면서 실제 구현을 통하여 개인적으로 적용해 보아야 되겠다고 생각하였다. 상당히 다양한 분야가 있어, 뇌파에 대한 연구가 흥미로워 보여 몇가지를 청취하였는데, 방법론적인 것 이외에 domain knowledge가 부족하여 이해하기가 쉽지 않았다. 이뜻은 각 session에서의 세부화가 그정도로 잘 되어 있기 때문이라고 느껴졌다. 그리고 같이 참석하였던 DMQM연구실의 두 형님분의 발표도 유심히 청강하였다. Random Jungle의 경우, 핵심 아이디어는 Randomforest 자체가 워낙 memory를 많이 사용하는 알고리즘이니 time complexity를 늘리는 penalty를 감수하게 되면 memory를 효과적으로 줄이면서 비슷한 성능을 낼 수 있다는 것이다. 이는 high dimesion데이터에 records도 많은 경우에 상당히 효과적으로 사용가능할 것 같다는 생각이 들었다. 그리고 많은 실제 많은 발표에서 느꼈던 점은 상당히 구조화가 잘되어있는 발표 슬라이드와 자신이 발표하기 쉬운 것이 아닌 처음듣는 사람도 이해시킬려고 PPT를 만들었으며 그에대한 발표력이 상당하신 분들이 많았다는 것이다. 예를들어 실제 공정 라인에서 어떠한 문제가 발생하고 어떠한 장비가 있으며 그 장비를 기반으로 하였을 때 어떤 문제점이 발생하고 그 문제를 풀기위해서 왜 이 프레임워크를 하였는지에 대한 것을 차근차근 설명하여 발표에 집중을 할 수 있게 도와주는 경우가 많았는데, 이런경우는 꼭 차용하여 개인적으로 개선해야 될 사항이라고 생각이 되었다. 지금 하고 있는 연구들 중 잘 정리하여 의미있는 발표가 되도록, 연구를 잘 세일즈 할 수 있도록, 항상 노력해야 되겠다는 생각을 가지게 되었다.]]></content>
		<date><![CDATA[20161122014709]]></date>
		<update><![CDATA[20161122144351]]></update>
		<view><![CDATA[1398]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201611/583327c93d5a29897003.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo_03-e1478242892681.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[227]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 11. 21 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161122183903]]></date>
		<update><![CDATA[20161122183903]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[228]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회 - 김창엽]]></title>
		<content><![CDATA[우선 이번 추계 학술대회에서는 발표를 하지 않아 편한 마음으로 세션을 들을 수 있었다. 오전에는 우리 연구실 연구원들이 참여한 제 5회 산업융합 활성화 방안 연구 및 사례연구 논문 공모전에 참석하였다. 발표자의 발표를 들으면서 다음 학술 대회 발표 준비를 어떻게 해야할 지에 대해 고민해 볼 수 있는 기회였다. 여러 세션을 들으면서 좌장이 공통적으로 확인하는 부분이 있었는 데 "해결하고자 하는 문제를 정의하는 것"과 "모델의 성능을 평가"하는 부분이었다. Q/A 세션에서 각 분야의 전문가 앞에서 발표한다는 것도 긴장이 되겠지만 대부분 질문에 대한 정확한 대답을 줄 수 있는 발표자는 드물었던 것 같다. 우리 연구실 내부 세미나나 작은 발표(회의)등을 통해 전달하고자 하는 내용을 다른 사람에게 정확하게 전달하고 다른 질문자의 의도를 파악할 수 있도록 많은 연습이 필요할 것 같다. 세션 중에는 아래 두 발표가 가장 흥미로웠다. 

[1. 다중 속성 네트워크를 이용한 내용기반 필터링 영화 추천시스템 - 손지은]

기존의 추천 시스템 개념에 대해서 설명하고 협업 필터링과 컨텐츠 기반 필터링으로 나눌 수 있음을 설명하였다. 협업 필터링은 특정 사용자와 유사한 선호도를 갖는 사용자 집합을 찾아내고 다른 사용자가 선호 하는 아이템을 추천해 주는 것을 의미하고 컨텐츠 기반 필터링은 사용자가 과거에 좋아했었던 아이템을 분석해서 유사한 아이템을 추천해 주는 것을 말한다. 컨텐츠 기반 필터링은 아이템의 콘텐츠들의 유사성을 분석하여 아이템을 추천해주는 방식인데 유사하는 기준을 정의 하기 어렵고 한 개의 특징으로 분류할 경우, 특징에 대해 비슷한 아이템들이 반복적으로 추천이 되는 문제점을 가지고 있다. 발표자는 다양한 속성을 동시에 고려하여 유사도를 계산하고 아이템간의 관계를 분석할 수 있는 방법을 제안하였다. 다양한 아이템들을 네트워크로 표현하고 Centrality를 정의하여 이 Centrality와 선호도 간의 관계로 추천을 해주는 시스템을 발표하였다. Modularity analysis는 네트워크에서 커뮤니티 구조를 찾아내는 알고리즘이며 높은 Modularity를 갖는 것은 dense한 연결을 갖고 서로 다른 클러스터에 속한 노드끼리는 sparse한 연결을 갖는다는 것을 설명하였다. 먼저 다중 속성 네트워크에서 아이템의 Centrality를 계산하고 사용자의 선호를 다른 아이템의 Centrality들과의 거리를 사용하여 추천하는 시스템을 소개하였다. 발표자는 영화 도메인에서 이 시스템을 적용하였는 데, 거리를 기반으로 분류하거나 군집화하는 다른 문제에도 적용하면 하나의 특징에 대해 분류 되는 특수화 문제를 해결하고 다중 속성을 이용하여 얻는 강점도 많을 것으로 생각되었다. 관련해서 이전 발표자가 발표하셨던 자료와 커멘트 들을 찾아 보면서 유사도 분석 알고리즘에 대해 더 학습할 수 있는 좋은 계기가 되었다.  

[2. 이상치 탐지 기법을 활용한 내부자 위협 탐지 방법론 개발 - 박민식]

길지 않은 직장생활이었지만 그 기간 중 가장 오랫동안 진행한 업무가 침해 사고(해킹 사고) 분석이었다. 만약 해커가 금전적인 요구를 하며 해킹되었다는 사실 하나만 가지고 해킹 사고 분석을 시작하는 것이 대부분이었다. 이런 경우 어떠한 시스템을 먼저 점검해야 하는 지, 이 시스템에 접근할 수 있는 사용자는 누구이며 어떠한 권한을 갖고 어떠한 업무를 맡고 있는지, 이 시스템을 통해서 다른 시스템에 접근을 할 수 있는 범위는 어디까지인지 등을 판단하는 것은 매우 어려운 문제였다. 또한 가장 분석해야 할 데이터가 많은 부분은 바로 사용자 행위에 대한 로그들이었다. 아침에 출근한 뒤 컴퓨터에 로그인을 하고 수 많은 웹 접근 로그를 남기며 이메일을 보내고 외장 장치를 연결하고 수 많은 응용 프로그램을 실행 시켜 로그를 남긴다. 이런 사용자 행위들을 모델링하여 Gaussian, Parzen Window, PCA reconstruction error 등을 통해 이상치를 탐지할 수 있다는 것은 매우 흥미로운 아이디어였다. 이 아이디어는 해킹 사고 분석을 할 때 모든 계정에 대해 모델을 적용한 뒤 먼저 분석해야 할 범위를 줄여 나가는 데에 큰 도움이 될 것으로 판단된다.]]></content>
		<date><![CDATA[20161122192004]]></date>
		<update><![CDATA[20161122192112]]></update>
		<view><![CDATA[1282]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201611/58341c18950a96691192.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[583327c93d5a29897003-130x87.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[229]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회 – 김형석]]></title>
		<content><![CDATA[- 발표후기
발표제목 : 토픽모델링 기반의 뉴스기사 평가방법론

현재 연구실에서 NCsoft와 진행하고 있는 ‘Information Quality 평가 기술 개발’ 과제의 연장선으로 ‘토픽모델링 기반의 뉴스기사 평가방법론’이라는 주제로 발표를 진행하였다. 본 과제의 내용에 대한 이해와 슬라이드에 대한 구성은 본인이 직접 참여하고 작성하여 100%이해를 하고 있음에도 불구하고, 얼마전 참석하였던 INFORMS 2016 annual meeting에서 좋은 발표들을 듣고 왔기에 의욕도 앞서고, 부담도 많았던 발표였다. 매번 발표를 진행하고 나서 남는 아쉬움은 항상 있지만, 이번 발표에서는 크게 다음과 같은 아쉬움이 남았다. 첫번째, 스피치의 속도에 대한 아쉬움이 크게 남았다. 발표를 함에 있어 주어진 시간을 고려하여 많은 내용을 모두 다루려고 하다 보니, 스피치가 너무 빠르게 진행되 버린 것이다. 물론 주어진 시간에 많은 내용을 전달하는 것도 중요하지만, 청중입장에서는 짧은 시간에 모든 발표를 한번에 받아들이기 힘들 것이다. 따라서 추후의 발표에서는 스피치의 속도를 유동적으로 조절하여 중요한 내용에서는 정확한 스피치를 통해서 잘 전달하며, 예시와 그림과 그래프와 같은 비교적 덜 중요한 부분에서는 이를 skip하거나 빠른 전달하는 발표를 가지도록 하겠다. 두번째로, 금번 발표는 청중으로 하여금 발표에 대한 호기심이나 관심을 끄는데 부족하지 않은 않았나 생각된다. 이는 발표가 진행된 후, 질의응답시간에 청중에게서 발표에 대한 질문이 없어 조금은 당혹스러웠다. 이는 청중에게서 본 발표에 대한 관심을 끄는데 부족하였다고 생각된다. 따라서 추후 발표에서는 청중으로 하여금 발표에 대한 관심을 가질 수 있도록 도입 부분을 신경 써서 준비하도록 할 필요가 있다고 생각된다. 위와 같은 아쉬움을 거름 삼아서 다음 발표에서는 좀더 일목요연하고 흥미로운 발표를 진행할 수 있도록 준비하고, 연습하도록 하겠다.

- 청취 후기
본 학회는 고려대에서 진행된 만큼 고려대학교 산업경영공학과 학생들의 발표가 많은 비중을 차지하고 있었다. 따라서 실제로 같이 대학원 생활을 같이 하고 있지만, 잘 모르던 다른 연구실 동료들의 연구들을 알고, 그들의 발표를 들어볼 수 있는 좋은 시간이었다. 특히 박영준 학생의 ‘Hybrid learning’이라는 컨셉의 네트워크 관련 발표 주제는 흥미로우면서도 청중으로 하여금 충분한 설득력을 가지는 연구발표라고 생각된다. Deep neural network에 대한 이해를 기반으로 Feature extraction까지 이끌어가는 연구방향은 발상의 전환을 통해서 좋은 결과를 보여주었다. 또한 포항공대의 이정혜 양의 발표도 매우 인상적이었다. 발표 과정과 질의응답에 대한 자신감, 목소리의 피치와 Diction 등 자극을 받는 발표였었다. 그 외에도 흥미로운 발표들이 몇몇 있었다. 매번 학회를 마치고 나면 많은 동기부여를 받게 되는데, 이러한 동기부여가 계속해서 이어져 좋은 연구방향으로 이끌어 갈 수 있도록 노력 할 수 있도록 하겠다.
이상으로 2016 추계 산업공학회에 대한 후기를 마치도록 하겠다. 
작성자:김형석]]></content>
		<date><![CDATA[20161124133112]]></date>
		<update><![CDATA[20161124133112]]></update>
		<view><![CDATA[1363]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[230]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회 – 박민식]]></title>
		<content><![CDATA[이번 2016년 추계 산업공학학회는 고려대학교에서 진행되었습니다.
지난 학술대회에서 발표하지 않았던 고려대학교 학생들의 발표를 많이 들을 수 있었고
데이터마이닝 세션을 주로 참석하였는데 다른 학교 학생들의 발표를 들으면서
생각하지 못했던 연구 주제들과 연구방법들을 알게 되는 계기가 되었습니다.

[발표 후기]
-발표 제목- : 이상치 탐지 기법을 활용한 내부자 위협 탐지 방법론 개발

 작년 추계대회, 올해 춘계대회를 참석하고 이번에 세번째로 산업공학학회에 참석하게 되었습니다.
지난 두 대회에서는 다른 사람들의 발표를 듣기만 했었는데 이번에 처음으로 학회에서 발표를 하게 되었습니다.

 이번에 발표를 한 주제는 올해 봄부터 국가보안기술소와의 과제로 진행했던 내부자 위협 탐지 방법론 개발에
대한 발표였습니다. 연구의 목적, 데이터 소개, 실험결과, 기대효과에 대한 발표를 진행하였습니다.

 발표 진행을 하면서 느낀점은 작은 강의실이나 적은 수의 학생 앞에서만 발표를 하였는데 대형 강의실과 많은 사람들 앞에서 발표를 하니 끝까지 발표를 마쳤지만 긴장했다는 것을 느꼈고, 발표 슬라이드가 잘 안보이는 무대위에서 발표를 해서 설명하는데 어려움이 있었는데 무대 아래에서 발표를 했으면 좋았겠다는 생각이 들었습니다.

 질문으로는 탐지율을 구하는 부분에서 "false alram rate"에 대해서 질문을 하신 분이 계셨습니다. 저희가 탐지를 하는데 있어서 confusion matrix 같은 오분류에 대한 연구 내용은 진행하지 않았는데 이 부분에 대해서 개선하면 실제 application에 적용되는데 도움이 될것이라 생각했습니다.

 학회 발표 기회가 주어질때마다 매번 발표를 하고 싶다는 생각이 들었고, 개인 연구를 진행하고 평가받고 싶어졌습니다.

[발표 청취 후기]
 이번 학회를 참석하면서 들었던 발표 중에서 인상깊었던 발표들은 다음과 같습니다.

 첫번쨰는 포항공대의 이정혜 학생이 발표한  "Development of a Federated Patient Similarity Learning Framework with Privacy Protection" 입니다. 전자의료기록 정보들에 대한 데이터의 양과 접근성이 점점 증가를 하고 있는데, 이러한 EHR 데이터들은 개별적으로 연구를 하기 때문에 연합된 데이터 분석 프레임워크를 구축하는데 문제가 있고, Privacy 문제 같은 한계점이 있습니다. 이러한 문제를 해결하기 위해 환자들 간의 유사도를 이용하는데 Hashing 이라는 방법을 이용해서  유사도를 측정하는 발표를 들었습니다. 처음보는 주제이고 수식이 많아 이해하기 힘들었지만 흥미로운 주제여서 관심을 가지게 되었습니다.
 두번째 발표는 NC와의 프로젝트를 통해서 진행하고 있는 김형석 학생의 '토픽 모델링 기반의 뉴스기사 품질 방법론' 발표였습니다. 이전 야구 선수에 대한 평가를 하였던 지난 프로젝트와 비슷한 방법으로 경제면 기사에 대한 뉴스기사들에 대해서 뉴스 기사를 분류, 평가하는 내용이었습니다. 토픽모델링, Novelty detection등의 방법을 활용했는데 뉴스기사를 분류하는데 좋은 결과를 얻을 수 있음을 알 수 있었고, 텍스트 데이터를 이용해서 주가 예측을 하는 연구가 국내, 해외 연구실들에서 많이 연구가 되고 있는데 저희 연구실의 연구내용이 의미있는 결과를 낼 수 있을 것이라 기대가 됩니다.
 세번째 발표는 고려대 박영준 학생의 'Convolutional Autoencoder with Hybrid Learning' 이라는 발표였습니다.  요즘 유행하고 있는 딥러닝에 대한 발표였고 Autoencoder 방법을 supervised learning 방법을 결합해 일반화 성능을 향상시킨다는 주제였습니다. 차원축소 뿐만 아니라 분류를 수행할 수 있다는 점이 인상적이었습니다.  

 이번 학회에서는 딥러닝에 대한 발표들이 많았는데 제가 개인 연구를 하게 된다면  Recurrent Neural Network를 application 하는 연구에 대한 주제에 대해서 연구를 하고 싶다는 생각이 들었습니다.]]></content>
		<date><![CDATA[20161128114939]]></date>
		<update><![CDATA[20161128114939]]></update>
		<view><![CDATA[1409]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[231]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[2016년 대한산업공학회 추계학술대회 – 이기창]]></title>
		<content><![CDATA[이번에 처음으로 대한산업공학회 학술대회에 참가하게 됐습니다. 무엇보다 지난 3개월간 준비한 'Word sentiment score evaluation based on semi-supervised learning in a distributed representation' 발표가 저에게 뜻깊게 다가왔습니다. 팀장인 서덕성 학우 이외 팀원들 각고의 노력 끝에 나온 결과물이어서 뿌듯했습니다. 그동안 팀을 이끌고 발표를 맡아준 서덕성 학우 외 모든 팀원들 고생 많으셨습니다.
'이상치 탐지 기법을 활용한 내부자 위협 탐지 방법론 개발' 발표를 맡은 박민식 학우, '관광지 평가 댓글을 활용한 만족도 요인 추출과 관광지 평가' 발표를 맡은 조수현 학우도 리허설 때 대비 훌륭한 발표를 해줬습니다. 특히 산업융합활성화 방안 및 사례 연구 공모전 우수상을 받은 김보섭 조수현 박민식 학우 축하드립니다.
이번 학회에선 이웃 연구실인 DMQM의 발표들이 인상적이었습니다. 연관규칙분석에 네트워크를 접목시킨 ''다중 속성 네트워크를 이용한 내용기반 필터링 영화 추천시스템', 오토인코더에 딥러닝을 결합한 'Convolutional Autoencoder with Hybrid Learning' 등이 신선한 시도였다고 생각합니다. 포스텍의 'Develeopment of a Federated Patient Similarity Learning Framework with Privacy Protection'은 수학적 엄밀함,  울산과기원의 '이벤트 발생 시간 간격을 고려한 자동차 엔진 고장 징후 분석'은 꼼꼼하고 성실한 연구 진행 과정이 돋보였습니다. 이번 학회가 첫 참석인데 스스로에게 채찍질이 되는 좋은 발표들이 많았습니다.
다만 발표를 위한 발표, 문제의식 설정이나 방향성에 동의하기 어려운 연구, 수식이나 기법에 매몰된 나머지 당초의 목표를 잃은듯한 연구, 내용은 좋으나 발표나 설명이 부족한 프리젠테이션 등도 일부 눈에 띄었습니다. 이번 학회 참석을 계기로 성실하고 훌륭한 연구자로 거듭나겠습니다. 마지막으로 연구실 식구들에게 부담주지 않으시려고, 이번 학회 준비를 묵묵히 도맡으신 강필성 교수님 특히 고생 많으셨습니다.]]></content>
		<date><![CDATA[20161128122021]]></date>
		<update><![CDATA[20161128122503]]></update>
		<view><![CDATA[1272]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[232]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회 - 조수현]]></title>
		<content><![CDATA[지난 춘계 학술대회에 이어서 이번 추계 대회에서도 발표를 하게 되었습니다. 이번에는 홈그라운드인 고려대에서 개최되었고 지난 춘계 때 한 번 발표한 경험이 있어서 발표를 좀 더 자신 있게 잘 할 수 있지 않을까 하는 생각이 잠깐 들었었지만 여전히 떨리고 부담스러웠으며 많은 것을 배울 수 있는 기회였습니다.

산업 융합 활성화 방안 및 사례 연구 논문 공모전 발표
이번 발표를 준비하면서 끊임없이 고민했던 부분이 ‘어떻게 하면 연구한 부분에 대해 압축해서 잘 전달할 수 있을까’ 였습니다. 이에 대해 충분히 고민했고 발표자료와 관련해서도 나름 잘 구성했다고 생각했는데 발표가 끝난 후 방법론과 관련된 질문이 생각보다 많이 나와 놀라웠습니다. 심지어 활용방안에서 충분히 설명했다고 생각한 부분을 또 질문 받아 제 전달력에 대해 고민하게 되었습니다. 15분동안 발표할 것이라 예상하여 준비했고, 연습할 때도 크게 시간 문제를 못 느꼈지만 막상 발표장에서 10분도 채 안돼 끝내는 저 자신을 보고 실전 경험이 부족하다고 느꼈습니다. 이뿐만 아니라 질문을 받았을 때 미리 예상했던 질문이었고 답변도 어느 정도 준비했었는데 막상 발표장에선 아무 생각이 나지 않아 평상시 수업시간이나 세미나 같은 기회를 통해 더 연습해야겠다고 느꼈습니다.

청취 후기
각 세션장에서 발표를 들으며 느낀 바는 비록 학회를 많이 경험해보진 못했지만, 딥러닝은 정말 뜨거운 관심을 받고 있고 관련된 연구 결과물이 꾸준히 나오고 있다는 것입니다. 그 중 가장 기억에 남는 연구는 convolutional autoencoder with hybrid learning이라는 연구였습니다. Autoencoder와 Deep learning을 결합한 방식을 이용하여 feature를 뽑은 후 이를 지도학습, 준지도학습에 적용한 연구였습니다. 지난 겨울 딥러닝을 이론으로만 배운 저에겐 이를 활용하여 연구를 한고 의미 있는 결과를 산출했다는 것이 그저 대단해 보였습니다. 언젠가 저도 딥러닝을 활용해 어떤 의미 있는 결과를 내보고 싶다는 생각이 들었습니다.

평상시 수업시간 때 방법론들을 배울 때마다 이를 어떻게 활용할 수 있을지에 대한 고민을 하였는데 학회를 다녀올 때마다 조금이나마 해소되는 기분입니다. 학기 동안 부지런히 배우고 다음 학회 때 더 많은 새로운 연구를 접해보고 싶습니다]]></content>
		<date><![CDATA[20161128122239]]></date>
		<update><![CDATA[20161128122259]]></update>
		<view><![CDATA[1255]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[233]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 11. 28. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161128124930]]></date>
		<update><![CDATA[20161128124930]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[234]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[[2016/11/28][RL seminar] Value Function Approximation]]></title>
		<content><![CDATA[이번 세미나 시간에는 Value Function Approximation을 공부했습니다.

지난 두시간 동안 배웠던 Model-free의 MC, TD, Sarsa, Q-learning은 모두 state와 action space가 finite 할 때에만 적용할 수 있었던 방법론이였습니다. 따라서 이 Value function을 우리가 익숙한 지도학습을 통하여 추정하는 방법을 다루었습니다. 
그 중에서도 다루기 편한 미분가능한 함수를 중점적으로 살펴봤습니다.


[1] Incremental methods
각 에피소드에서 한 step마다 stochastic gradient descent로 파라미터 w를 업데이트합니다.

State value function과 Action value function 모두 MC, TD, Q-learning 등의 방법을 적용하였으며, Linear combination 모델을 예시로 소개했습니다.

[2] Batch methods
기본적인 Batch 방식은 충분한 데이터를 모은 후 이를 이용하여 파라미터 w를 찾습니다.

이에 나아가 Experience replay의 방식은, 적절한 양의 데이터를 모은 후 이 데이터셋으로부터 sampling을 합니다.
이를 이용하여 stochastic gradient descent로 파라미터를 업데이트합니다.

이 방법론을 Atari games에 응용한 DQN을 비롯하여 여러 예시를 살펴보았습니다.]]></content>
		<date><![CDATA[20161129161328]]></date>
		<update><![CDATA[20161208152945]]></update>
		<view><![CDATA[39]]></view>
		<comment><![CDATA[10]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[235]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 12. 02 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161202174157]]></date>
		<update><![CDATA[20161202174157]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[236]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 12. 02. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161202180712]]></date>
		<update><![CDATA[20161202180712]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[237]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 12. 02. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161202183724]]></date>
		<update><![CDATA[20161202183814]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[239]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[11]]></member_uid>
		<member_display><![CDATA[류 나현]]></member_display>
		<title><![CDATA[2016년 대한산업공학학회 추계학술대회 - 류나현]]></title>
		<content><![CDATA[우선 이번 학회에서는 우리 연구실의 수상의 기쁨과 그동안 노력의 결실이 있어 더욱 뜻 깊은 학회로 다가왔습니다. 리허설보다 더욱 안정적이고 좋은 발표를 보여주어 연구실원으로써 뭔지 모를 뿌듯함도 느껴졌습니다.

가장 기억에 남는 연구 중 하나로 convolution Autoencoder with Hybrid Learning이었습니다. 변수추출 모델로써 기존 convolution Autoencoder의 경우 독립변수만을 이용하기 때문에 분류문제에서의 성능을 기대하기 어려우므로 이 연구에서는 종속변수도 이용하여 지도/준지도 학습의 방법론을 선보였습니다. 요즘 각광받는 딥러닝에 feature extraction의 단점을 보완하는 아이디어를 더해 흥미로운 주제였고 이론으로 배웠던 autoencoder와 convolution network에 대해 다루어 매우 인상적이었던 발표였습니다. 다소 어려울 수 있는 내용을 청자로 하여금 쉽게 이해할 수 있도록 설명하여 효과적으로 연구 목적과 발표내용을 전달하여 이 또한 배울 점이 많았습니다.

이번 학회에서 들어간 세션에서는 발표 후 연구의 목적과 연구 결과의 평가방법에 대한 질문이 전반적으로 많았던 것 같습니다. 연구자의 입장에서 주로 연구 방법이나 그 과정을 고민하기 때문에 연구 발표나 설명이 이에 치우치기 쉬운 것 같습니다. 하지만 청자의 입자이 되어 보니 연구의 목적이나 동기가 더 명확해야 연구자의 의도를 이해하기 쉬웠습니다. 제 발표에서도 부족한 부분인데 이런 점을 유념하여 고쳐 나가겠습니다. 또한 준홍오빠의 발표가 뒤에서 두번쨰 발표였는데 청자와 소통하면서 하루동안 지쳤을 청자의 흥미와 참여를 유도하여 더욱 전달력 있었습니다.  이 또한 제가 본받아야 할 부분이라 생각했습니다.

이번 학회에서는 연구의 교류뿐 아니라 고려대에서 학회가 열리면서 학회 준비과정도 살펴보게 되었고  다른 연구실 사람들과도 친분을 더욱 쌓는 계기도 되어 다른 때와는 조금 다른 의미를 갖는 학회가 되었습니다. 끝으로 이번 학회를 준비하신 분들과 교수님께 감사드립니다.]]></content>
		<date><![CDATA[20161205043948]]></date>
		<update><![CDATA[20161205050440]]></update>
		<view><![CDATA[1219]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201612/58447104863d25909515.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo_03-e1478242892681.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[240]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[18]]></member_uid>
		<member_display><![CDATA[모경현]]></member_display>
		<title><![CDATA[[2016/12/05][RL seminar] Policy Gradient Method]]></title>
		<content><![CDATA[이번 세미나 시간에는 Policy Gradient Method를 공부했습니다.

지금까지 강화학습 세미나는 Value based 였다면 오늘 다룬 내용은 Policy based 였습니다.
Policy를 직접 update하면 수렴이 더욱 잘되며 continues한 문제를 다룰 수 있고 stochastic한 부분도 다룰 수 있습니다.
다만 local optimal에 빠지거나 inefficient한 문제가 있을 수 있습니다.
그리고 Policy를 업데이트 하기 위해서 objective function을 정의했습니다.
start value, averages value, average value per time step으로 각각 시작 시점과 time step에 따라 정의하는 방식이 달라졌습니다.

[1] Monte Carlo Policy Gradient
episode별로 theta를 업데이트하는 방식입니다.
gradient방식으로 theta를 찾아 policy를 업데이트 시킵니다.

[2] Actor-Critic Policy Gradient
time step별로 theta와 w를 업데이트하는 방식입니다.
gradient 방식으로 theta와 w를 찾아 policy를 업데이트 시킵니다.

[3] Finite Difference Policy Gradient
naive,numerical하게 policy를 업데이트하는 방식입니다.
parameter vector를 지정하여 각 parameter를 업데이트하는 방식으로 policy를 업데이트 시킵니다.]]></content>
		<date><![CDATA[20161205143709]]></date>
		<update><![CDATA[20161219123217]]></update>
		<view><![CDATA[35]]></view>
		<comment><![CDATA[10]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[241]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 12. 09 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161209173505]]></date>
		<update><![CDATA[20161209173505]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[242]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 12. 09. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161209174234]]></date>
		<update><![CDATA[20161209174234]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[243]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 12. 09. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161209213737]]></date>
		<update><![CDATA[20161209213737]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/9/201612/584aa591959a33981689.docx]]></thumbnail_file>
		<thumbnail_name><![CDATA[회의록_161209.docx]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[244]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 12. 19 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161219155538]]></date>
		<update><![CDATA[20161219155538]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[245]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 12. 19. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161219175939]]></date>
		<update><![CDATA[20161219175939]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[246]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2016/12/19][RL seminar] Integrating Learning and planning]]></title>
		<content><![CDATA[이번 강화학습 세미나 시간에는 Integrating Learning and planning에 대해 발표를 하였습니다.

지난 시간까지 배웠던 내용들은 learning과 planning에 대해 각각 배웠었는데 이를 결합하여
강화학습문제를 해결하는 방법을 알아보았고 experience로 부터 모델을 학습하는
model-based 방법에 대해서 중점적으로 다루었습니다.


[1] Introduction
지난 시간까지 배웠던 Full backup과 sample backup 방법들을 복습하고
Model-based 학습 방법에 대해서 소개를 하였습니다.

[2] Model-Based Reinforcement Learning

model-based 방법은 experience로 부터 model에 대해서 직접 학습을 하고
구축된 정보들을 바탕으로 value function/policy를 planning을 통해 구할 수 있습니다.
model learning을 통해 state와 action을 통해서 reward나 다음 state를 구할 수 있습니다.

[3] Integrated Architectures

경험에는 real experience와 simulated experience가 있습니다.
두가지 모든 경험으로부터 model에 대해서 학습하고 learn과 plan 방법을 동시해 사용할 수 있는
방법이 dyna 입니다.
dyna-Q 알고리즘을 통하여 미로찾기를 하는 방법에 대한 예제를 보았습니다.

[4] Simulation-Based Search

simulation-based의 대표적인 방법인 monte carlo tree search 방법에 대해 알아보고
대표적인 예시인 바둑게임을 강화학습하는 방법에 대해서 보았습니다.
monte-carlo대신 temporal difference를 사용하는 TD search 방법에 대해서도 알아보았습니다.]]></content>
		<date><![CDATA[20161220153044]]></date>
		<update><![CDATA[20170101142920]]></update>
		<view><![CDATA[29]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[247]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 12. 19. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161222214717]]></date>
		<update><![CDATA[20161222214717]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[248]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 12. 23 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161223170000]]></date>
		<update><![CDATA[20161223170000]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[249]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 12. 23. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161223173502]]></date>
		<update><![CDATA[20161223173502]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[250]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 12. 23. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161223175816]]></date>
		<update><![CDATA[20161223175816]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[251]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[[2016/12/28][RL seminar] Exploration &amp; Exploitation (김형석)]]></title>
		<content><![CDATA[금일 세미나에서는 Reinforcement Learning 문제에서 직면하는 Exploration &amp; Exploitation의 Trade-off Learning  problem 을 해결하는 Solution 방법론들을 다루었습니다. 
  Exploration &amp; Exploitation 을 해결하는 문제는 이전부터 Multi-Armed-Bandit-Problem이라는 이름으로 많은 연구가 선행된 분야로 Regret analysis등과 같은 Decision Process를 해결하기위해 다양한 방법론들이 제안되어 왔습니다. 크게는 단순히 Random action을 허용하는 Naive Exploration과 최적(High reward action)의 Initial을 통한 Optimistic Initialization, 추가적으로 action value, Q(a) 의 Uncertainty 를 활용하여 Regret을 최소화 시키는 방향으로 Upper Confidence Boundary를 활용하는 Exploration을 장려하는 Optimism in the Face of Uncertainty 방법론, 그리고 Bayesian Approach를 통해서 실제 Q(a)의 distribution을 Posterior Computation을 통해서 추정하고 이에 따른 PayOff를 최대화 시키는 방법으로 Exploration을 수행하는 Probability Matching 그리고 Thompson sampling등이 있습니다. 또한 마지막으로 현재 주어진 Information state를 정의하고 이를 MDP를 통해서 문제를 해결하도록 하는 Information State Search 방법론등이 있었습니다. 
 세미나를 마치면서 Exploration을 위해 Uncertainty를 활용하여 Optimistic하게 적용하려는 이러한 다양한 방법론들은 추후 다른 연구분야에서의 Uncertainty를 활용한다면 재미있는 연구주제가 되지 않을까 생각됩니다.

감사합니다.]]></content>
		<date><![CDATA[20161228165351]]></date>
		<update><![CDATA[20170117125137]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[252]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2016/12/28]RL seminar] Classic games]]></title>
		<content><![CDATA[금주 세미나에서는 board games에서의 RL 적용방법과 구축한 게임 프로그램이 상대방과 대국 하는 방식으로 minimax search와 variants 들을 설명하였습니다. board games에서의 강화학습은 multi-agent이므로 여태까지배운 single-agent의 문제와는 다른 식으로 environment가 정의되며 value function 자체도 game theory에 기초한 방식으로 정의됩니다. RL의 value function approximation 방법으로 TD(lambda), TD, MC 등을 활용할 수 있고 게임의 context (e.g. chess의 checkmate-position)를 value function approximation을 할 때 반영하는 방으로 TD-root, TD-leaf, TreeStrap 등을 설명하였습니다. 개인적으로 이번 세미나는 Game AI의 큰 틀을 이해하는 데 도움이 되었습니다. 향후 AI관련해서 발표자료의 reference를 토대로 공부해 보는 것도 좋을 것 같습니다.]]></content>
		<date><![CDATA[20161228170604]]></date>
		<update><![CDATA[20170117125831]]></update>
		<view><![CDATA[35]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[253]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[16. 12. 30 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161230164742]]></date>
		<update><![CDATA[20161230164742]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[254]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[16. 12. 30. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20161230193021]]></date>
		<update><![CDATA[20161230193021]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[255]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[[2017/01/03]RL seminar] Deep Q-Network and AlphaGo]]></title>
		<content><![CDATA[이번 세미나에서는 Reinforcement learning(RL)과 Deep learning(DL)을 성공적으로 결합한 사례로 Deep Q-Network(DQN)과 AlphaGo를 다루었습니다.

[1] Deep Q-Network
CNN으로 action-value function을 Q-learning으로 approximate하여 49개의 Atari game 중 대다수의 게임에서 사람 이상의 점수를 획득했습니다.
RL과 DL을 결합하기 위해서는 수렴성이 매우 중요합니다. 이를 위하여 experience replay와 fixed Q target 기법을 고안했습니다.
 - Experience replay: 전후 관측치 간의 correlation을 약화시키기 위하여 최근 충분한 데이터를 모아서 그 안에서 random batch를 추출하여 SGD 적용
 - fixed Q target: target value와 fitted value의 correlation을 약화하기 위하여 target value에 사용되는 parameter의 업데이트 빈도를 줄이는 방법

[2] AlphaGo
바둑 AI에서 혁신적인 성능을 보였던 AlphaGo의 기작을 알아보았습니다.
(1) Traing
 - Rollout policy, Tree policy, SL policy를 프로 대국 기보를 이용하여 Supervised learning으로 학습
 - SL policy를 기반으로 self-play로 improve하는 RL policy
 - RL policy를 이용하여 value function을 approximate하는 value network
 - 위의 트레이닝된 함수들을 기반으로, 더 정밀한 성능을 위한 APV-MCTS

DQN으로 DL과 RL이 이미지에 대하여 강건하게 학습할 수 있다는 것을 알 수 있었고, AlphaGo로 보아 보다 더 뛰어난 성능을 발휘하기 위하여 어떤 기법들이 사용될 수 있는지 파악하였습니다.]]></content>
		<date><![CDATA[20170104132236]]></date>
		<update><![CDATA[20170117135546]]></update>
		<view><![CDATA[34]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[256]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[16. 12. 30. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170104170631]]></date>
		<update><![CDATA[20170104170631]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[257]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 01. 05. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170105162050]]></date>
		<update><![CDATA[20170105162050]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[258]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[17. 01. 05. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170105171648]]></date>
		<update><![CDATA[20170105171648]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[259]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 01. 05 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170106103141]]></date>
		<update><![CDATA[20170112162627]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[263]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[18]]></member_uid>
		<member_display><![CDATA[모경현]]></member_display>
		<title><![CDATA[[2017/1/10][CS231n seminar] Introduction]]></title>
		<content><![CDATA[Stanford 대학의 이미지 강의 CS231n으로 세미나를 진행하게 되었습니다.
오늘은 간략한 강의 소개와 Vision의 역사에 대해서 알아보았습니다.

비전은 생물학적으로 눈을 통해 보는것을 시작하여 그 메커니즘이 어떻게 이루어지는지를 파악, 공학적으로 다루어가는 학문입니다. 우리의 눈이 세상을 바라보는 모습을 따라 이미지를 3D Modeling하거나, 이미지의 부분을 Segmentation, Grouping 하는 등의 해결해야 할 과제가 있었습니다.
그리고 이런 문제점들을 해결하기 위한 Convolutional Neural Network 등 최신 알고리즘까지 흐름을 살펴보았습니다.
향후에는 이미지의 전체 labeling, captioning 등 vision intelligence로 나가가야 함을 이야기하였습니다.]]></content>
		<date><![CDATA[20170111125334]]></date>
		<update><![CDATA[20170119153235]]></update>
		<view><![CDATA[32]]></view>
		<comment><![CDATA[11]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[262]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[2017/01/05][Deep Learning 2017 seminar] Introduction to Machine Learning]]></title>
		<content><![CDATA[이번 세미나에서는 machine learning에 대해 전반적으로 알아보는 시간이었습니다.

[1]Machine learning은 data의 원리를 파악하고 알고리즘을 전개하여 비슷한 형태의 새로운 data가 들어왔을 때 잘 다루는 것입니다. Machine learning은 크게 supervised learning, unsupervised learning, reinforcement learning으로 나뉩니다.

[2] Machine learning의 개념과 더불어 학습 방법, 생성한 알고리즘을 평가하고 그 중 best 알고리즘을 찾는 방법에 대해서도 알아보았습니다. 전체 data를 Training, validation, test data로 나눈 후 training data로는 model을 train하고, validation data로는 model을 평가한 후 test data로 generalization 성능을 평가합니다. Model의 capacity를 조절하는 과정에서 bias와 variance의 딜레마도 알아보았습니다

[3] 마지막으로 model의 bias와 variance를 줄이기 위해 ensemble 방식에 대해 알아보았습니다. 여러 가지 ensemble중 bagging과 boosting을 소개하였고 linear predictor로 nonlinear predictor를 구축하여 feature representation을 다양하게 표현할 수 있다는 것을 배웠습니다.]]></content>
		<date><![CDATA[20170108222719]]></date>
		<update><![CDATA[20170117140844]]></update>
		<view><![CDATA[34]]></view>
		<comment><![CDATA[10]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[261]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[[2017/01/05][Deep Learning 2017 seminar] Theoretical Motivation - 김형석]]></title>
		<content><![CDATA[금일(2017-01-05) 세미나 시간에는 앞서 준비한 조수현학생의 Machine Learning Introduction에 이어서 본 study seminar의 주제인 Deep Learning이 가지는 여러가지 장점들을 이론적 Background를 통하여 설명하는 Theoretical Motivation을 다루었습니다. 최근 Deep learning이 각광을 받고 있는 이유와 좋은 설명력을 가질 수 있는 배경들을 다루었습니다. 크게 5개의 특징을 통해서 딥러닝의 강점을 소개하였습니다. 이는 흔히 딥러닝이라고 부르는 Deep Neural Network의 layer구조를 통해서 가지게 되는 Distributed representations와 Deep composition of non-linearities의 성질을 통해서 Higher level abstraction이 가능해지며 이는 딥러닝을 Represent Learning으로 작용하도록 가능케 합니다. 이는 Vision과 NLP분야와 같은 다양한 분야에서 많은 Variation들을 통해서 그 성능을 입증해 주고 있습니다. 또한 과거부터 제기되어온 Local minimal문제에 대해서도 실험적으로 문제가 되지 않음을 대표적인 3편의 논문을 통해서 증명해주고 있습니다. 마지막으로 Representing Learning을 수행하면서 실제 Unsupervised model로서 학습이 가능하여, Semi-supervised approach가 가능하며, 다양한 multi-task를 동시 적용가능하여 많은 장점을 가지고 있다.
본 세미나 시간에는 왜 Deep Learning이 각광받고 좋은 성능을 보여주는지에 대한 수리적이기보다는 이론적인 Background를 통해서 소개하고 있다. 본격적인 Deep Learning에 대한 설명은 차주부터의 세미나를 통해서 알아보고자 한다. 처음 딥러닝을 접하는 이를 위해 최대한 무겁지 않도록 설명을 하고자 노력하였으며, 더 좋은 자료와 그림을 통해서 더 쉽게 이끌어가지 못해 아쉬움이 남는 발표였다.
이상으로 후기를 마친다.]]></content>
		<date><![CDATA[20170108190517]]></date>
		<update><![CDATA[20170117140503]]></update>
		<view><![CDATA[28]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/1/201701/58720edd30b7f8669555.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[티모 표지.PNG]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[264]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[2017/1/12][CS231n seminar] Image classification]]></title>
		<content><![CDATA[[1] 이미지 데이터의 형태를 살펴봤습니다. 가로, 세로 픽셀의 개수와 RGB의 채널을 갖는 형태임을 확인하였고, 0에서 255까지의 값을 가질수 있으며 이는 각 채널에서의 밝기임을 살펴봤습니다.

[2] 가장 먼저 살펴본 데이터 기반 분류기는 k-NN 기반 이미지 분류기였습니다. 두 객체간 거리를 L1, L2 등으로 정의하고, 주변의 여러 이웃들을 기반으로 voting하여 test data의 label을 예측하는 것입니다. 강의에서는 CIFAR-10 데이터를 이용해서 예를 들지만, python3버젼에서는 돌아가지 않으므로, hand-written digits 데이터를 이용해서 코드를 짰습니다. 간단한 시각화와, class정의, 그리고 최종 output까지 살펴봤습니다.

[3] 다음으로 살펴본 분류기는 선형 이미지 분류기였습니다. weight인 W와 절편 b를 정의하고, input data에 대해 선형 분류를 소개하였습니다. 본 방법론은 training 단계에서 k-NN보다 시간이 오래걸리지만, test data에 대해서는 시간이 훨씬 적게 든다는 장점이 있었습니다. 실제로 test data 적용 단계에서 시간이 매우 적게드는 CNN까지 가는 단계로서 그 흐름이 적절하다고 생각합니다.

[4] 본 세미나에서는 W등을 optimize하는 것까지는 다루지 않았습니다. 앞으로는 충분한 데이터를 기반으로 W를 최적화 하기 위해 loss function을 정의하고, 최적화 방법론을 적용할 것입니다.]]></content>
		<date><![CDATA[20170112144841]]></date>
		<update><![CDATA[20170124124430]]></update>
		<view><![CDATA[32]]></view>
		<comment><![CDATA[12]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[265]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 01. 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170112163818]]></date>
		<update><![CDATA[20170112163922]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[266]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 01. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170112164141]]></date>
		<update><![CDATA[20170112164141]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[267]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[17. 01. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170115111556]]></date>
		<update><![CDATA[20170115111556]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[268]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 01. 12. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170117124315]]></date>
		<update><![CDATA[20170117124315]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[269]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 01. 16 회의록(NC 정기 미팅)]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170117124618]]></date>
		<update><![CDATA[20170117124618]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[270]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[[2017/1/18][CS231n seminar] Cost Function &amp; Optimization]]></title>
		<content><![CDATA[[1] 먼저, 지난 시간에 배웠던 Data-Driven 접근 방법 중 KNN과 Linear classifer의 Score Function 개념을 복습하였습니다.

[2] 임의의 Weight W로 계산한 Score Function이 주어진 학습 데이터를 잘 표현하고 있는 지 정량화하기 위해 Loss Function을 정의하였습니다. Loss Function 중 Multiclass SVM loss(Hinge loss)와 Softmax(Cross entropy loss)개념에 대해 살펴보고, Regularization Term의 역할에 대해 다뤘습니다.

[3] Loss Function의 Loss를 최소화하는 W를 찾기 위해 Random Search, Numerical Approach, Analytic Approach가 있음을 살펴 보았고, 각각의 코드에 대해 살펴 보았습니다. Gradient Descent와 Mini-batch Gradient Descent 개념을 다뤘습니다.

[4] 끝으로 기존의 연구는 Image Feature Extraction에 집중되어 있음을 살펴보았고, Alexnet이 2012년에 발표된 이후로 패러다임이 바뀌었음을 살펴보았습니다.

감사합니다.]]></content>
		<date><![CDATA[20170118230828]]></date>
		<update><![CDATA[20170125233724]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[271]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2017/1/18][CS231n seminar] Backpropagation &amp; Neural Network]]></title>
		<content><![CDATA[저는 Backpropagation과 Neural Network 소개 부분의 강의를 맡아서 발표했습니다.
핵심요약은 다음과 같습니다.

1. Backpropagation의 정의를 통해 복잡한 합성함수를 재귀적인 방법으로 Graidient를 구할 수 있음을 보았습니다.

2. Backpropagation을 Computational 방법으로 살펴보았습니다.

3. Neural Network의 구조를 살펴보고 디자인할 때 유의해야 할 점을 알아보았습니다.]]></content>
		<date><![CDATA[20170119215540]]></date>
		<update><![CDATA[20170126122452]]></update>
		<view><![CDATA[27]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[272]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 01. 20 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170120104227]]></date>
		<update><![CDATA[20170120104227]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[273]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 서덕성]]></title>
		<content><![CDATA[NC soft AI center workshop
 2017. 01. 23 판교 NC soft(NC) 사옥에서 NC와 산학을 진행중인 6개의 연구실 교수님 및 대학원생, 그리고 NC 관계자들이 모여 workshop을 진행했다. 내용은 각 연구실이 주로 관심있어 하는 연구가 무엇인지, 그리고 NC와는 어떤 일을 진행하고 있는지였다.

 처음으로 우리 연구실이 발표를 맡았으며, 대부분이 아는 내용이며 NC와 진행하는 프로젝트 팀원이므로 발표 내용은 대부분 아는 내용이었다. 다만, 삼성전자와 진행중인 프로젝트 관련된 Bin2Vec은 참 신선했다. Word2Vec이라는 텍스트 관련 방법론을 웨이퍼에 적용시켜 insight를 도출했다는 것이 정말 배울 점이 많다고 생각했다. 하나의 방법론에 대해 변화를 주어 성능 향상을 도모하거나, 전혀 어울리지 않는 것 같은 데이터에 대해서도 그 본질에 깔린 idea를 통해 적용하는 시도가 정말 멋있다고 생각했다.

 두번째로는 K-MOOC 강좌로 이미 개인적으로는 알고 있었던 KAIST오혜연 교수님의 발표가 진행됐다. 해당 연구실은 Topic modeling을 10년간 하고 있다고 소개되었으며, 교수님의 발표에서 정말 많은 것을 느낄 수 있었다. 가장 충격적이었던 부분은 Document를 Graph로 표현하고, Edge를 Topic에 할당하는 방법론을 시도했다는 것이다. 어떤 연구로부터 영감을 얻어서 진행한 연구라고 하셨는데, 나는 왜 저런 깨어 있는 생각을 먼저 하지 못했나 하는 반성을 할 수 있는 시간이었다. 어찌 보면 성능에 대한 보장이 정말 확실하지 않은 상황에서 도전한 것이므로 좀 더 과감해질 필요가 있을 것 같다는 생각을 했다.

 세번째로는 강원대학교 김학수 교수님의 발표가 진행되었다. NLP 연구실이며, 현재 진행중인 연구는 챗봇 개발이었다. 막연하게 챗봇에 관심을 가지고 있으면서 기본 you-tube 자료등을 보는 중이었는데, 김학수 교수님의 발표를 통해 아직 제 내공이 부족해도 한참 부족하다는 사실을 알 수 있었다. NLP의 가장 밑단까지 고려하기 위해 POS tagger와 NER 파트를 결합하여 실험을 진행한 부분이나, 성능을 높이기 위해 직접 데이터셋에 태깅을 해준 노력등을 통해 그 열의를 느낄 수 있었다. 졸업까지 1년간 챗봇이라는 것을 완성시키지는 못할지 몰라도, 그 배경에 깔린 idea들과, 기저까지는 진행을 한 채로 졸업하고 싶다는 생각을 했다.

 네번째로는 연세대학교 황승원 교수님의 발표가 진행되었다. 황승원 교수님은 Botification 이라는 슬로건을 내걸으시며 연구 성과들을 말씀하셨는데, 모든 것을 봇 형태로 만들고자 하시는 분이었다. 유쾌한 발표 속에서 말씀하고자 하시는 부분은 궁극적으로는 개인의 웨어러블 디바이스를 통해 얻은 데이터로 사용자의 기초통계량만을 뽑아 주는 것이 아니라, 트렌드를 분석하고, 사용자와 대화할 수 있는 기술이 완성되기를 바라신다고 하셨다.

 다섯번째로는 고려대학교의 임희석 교수님의 발표가 진행되었다. 본 연구실은 AI 연구실로, 여러 연구를 진행중이었는데, 특히 education 관련 연구를 진행중이셨다. 기존의 교육은 학생들에게 선생님의 지식을 주입하는 방식이었다면, 현재는 정말 많은 컨텐츠가 있으므로, 그것들을 개인에게 적절하게 소개해주고, 평가하는 방법이 진행되어야 된다고 하셨다. 또한 머리에 패치를 붙여서 뇌파를 연구하기도 하셨는데, 재미있는 점은 코딩을 잘하는지, 못하는지 코딩 시험을 통해서 아는 것도 하나의 방법이 될 수 있는데, 추가로 뇌파를 통해서도 알 수 있다는 점이었다. 코딩을 잘하는 사람은 언어쪽이 많이 활성화 된다는 연구 결과를 들을 수 있었다.

 여섯번째로는 서울대학교 강유 교수님의 발표가 진행되었다. 2016년 NC 과제를 함께 진행하는 연구실이라 시작 전부터 친근함이 느껴졌다. 본 연구실에서는 tensor라는 것으로 주가 데이터를 정의하고, 여기서 패턴을 찾아 주가를 예측해보는 프로젝트를 먼저 진행해 보고, 현재는 그 주가 데이터에 추가로 뉴스기사 데이터 텐서를 붙여서 실험을 진행하고 있다고 하였다. 우리 연구실에서 진행하는 연구랑 큰 틀에서는 비슷한 것 같으면서도 그 세부 방법론이 다른 것을 확인할 수 있었다. 또한 사용하는 용어가 각 과마다 다른 것을 느꼈는데, 협업시에는 용어 정의가 필수적이겠구나 라는 생각을 했다.

 이번 워크샵은 많은 영감을 얻을 수 있는 귀한 시간이었다. 특히 개인 연구주제를 선정하는데 있어서 많은 도움이 되었다. 앞으로도 이런 자리가 있다면 꼭 참석해서 idea를 얻을 수 있도록 해야겠다.]]></content>
		<date><![CDATA[20170124124545]]></date>
		<update><![CDATA[20170124124951]]></update>
		<view><![CDATA[1416]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201701/5886cedfe2a3c9767913.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[DSBA logo.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[274]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[2017/1/24][CS231n seminar]  Backpropagtion1]]></title>
		<content><![CDATA[오늘은 CS231n의 갓파씨의 backpropagtion에 대하여 말하였습니다.

먼저 저번시간의 warmup을 하였고 gate에서 gradient at braches에 관하여 설명하였습니다.

그리고

1. Jacobian matrix
2. Skipp connection based on resnet
3. Backpropagaion equation
4. Full connected, Locally connected shared-weight network
5. Backpropagation of CNN
6. Transferlearning from Imagent
7. ANN history
8. CNN history
9. Why Nonliear actvation function and varint
10. Weight initialization (xavier and he initialization)
11. Batch normalization and interpret of gate network and numpy code
12. Batch norm and conv and tf code
13. method of check network based on number of output node and add regularization
14. Hyperparameter in ResNet
15. Hyperparameter optimizatino method (random search and grid search)
16. TLDRs from 갓파씨님]]></content>
		<date><![CDATA[20170124155009]]></date>
		<update><![CDATA[20170126123925]]></update>
		<view><![CDATA[46]]></view>
		<comment><![CDATA[10]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[278]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[7. 01. 23 NC soft AI center workshop - Haedong Kim]]></title>
		<content><![CDATA[NC Soft에서 주최하는 첫 번째 Workshop에서 나는 기대 이상의 수확을 거두었다고 생각한다. 추운 겨울 아침 만원 버스를 타고 판교로 가는 일이 상당히 곤혹스러웠지만, 저명한 교수님들의 발표를 들으면 들을수록 그런 수고로움으로 경망스럽게 투덜댔던 내가 벼룩같이 한심스러웠다. Workshop의 발표를 모두 듣고 나서 느낀 생각은 파고들어갈 특정 분야가 있으면 좋겠다는 것이다. 각 연구실 마다 집중하고 있는 특정 분야가 있었다. 구체적으로, KAIST는 토픽 모델링, 강원대는 NLP, 연세대는 봇, 서울대는 그래프와 선형 대수와 같은 이론을 실제 문제를 푸는데 적용하는 것에 집중하고 있다고 생각했다. 한 분야를 깊게 연구한 이들의 발표에서는 발화의 뒷면에 느껴지는 노련함과 같은 것이 있었다. 여러 분야에 관심을 가지되, 내가 집중해야 할 한 가지 분야는 최소한으로 만들어 두어야 한다는 생각을 가지게 되었다. 모두 주옥같은 발표였지만, 특히 감명 깊게 들었던 발표들에 대해서 몇 가지 코멘트를 아래와 같이 정리하였습니다.
1.	KAIST
  개인적으로 발표를 하신 KAIST의 오혜연 교수님은 K-MOOC에서 기계학습 강좌를 통해 강의를 하시는 모습을 본적이 있는데 실제로 만나보니 감회가 새로웠다. 이분이 10년이상 토픽 모델링을 연구하신 대가라는 사실은 처음 알게 된 사실이다. 발표의 첫 파트로 Dual Context Topic Modeling이란 방법을 소개해 주셨는데, 그래프를 활용한다는 점이 매우 인상깊었다. 듣고 나면 간단한 아이디어라 생각할 수도 있을 것 같다. 콜럼버스의 달걀과 같은 사례가 아닌가 싶다. 쉬운 것 같지만 처음이 어려운 그런 문제이다. 그래프로 단어 간의 관계를 표현하고 모델링한다는 아이디어는 다른 기계 학습 분야에도 확장시킬 수 있는 스키마가 아닌가 생각한다. 부가적으로 느꼈던 점은, 연구를 소개할 때 Research Hypotheses를 문장으로 명시한다는 것이다. 이 방식이 연구의 목적을 한 눈에 파악하는데 굉장히 도움이 되었다. 앞으로 차용할 생각입니다.
2.	강원대
  NLP를 아주 오래 전 부터 파고들고 계셨다는 것이 인상깊다. 발표의 후반부에 설명해 주신 응용 분야보다 전반부의 이론적인 연구들을 주의 깊게 들었다. 가장 기억에 남는 것은 통합 언어 모델이다. NLP의 단계에서 앞의 오류가 뒤로 갈수록 전파되면서 점점 커진다는 것은 이전까지 크게 고려하고 있지 않던 사항이었다. 이번 발표를 들으면서 이것이 얼마나 큰 문제가 될 수 있는지 실감하였다. 그리고 오류 전파 문제를 통합 언어 모델로 접근한 시각이 재미있었다. 지금은 각 단계별 모형을 단순히 쌓은 것으로 보인다. 하지만 앞으로 개선 사항이 많을 것이라 생각되고, 이런 지점에서 새로운 연구 아이디어를 얻을 수 있지 않을까 하는 기대를 품어본다.
3.	서울대
  저에게 이번 Workshop의 화룡정점은 단언컨데 이 발표였습니다. 긴 Workshop 시간, 평소보다 한참이나 이른 기상시간으로 집중력이 단조감소하고 있던 마지막 발표에, 제 머리에 신비로운 육각수가 쏟아지는 듯한 신선함을 받았습니다. 소셜 네트워크가 가지고 있는 특징을 날카롭게 분석해 내었고, 그것을 이용해서 역행렬을 아주 효율적으로 구하는 연구는 관찰, 분석, 그리고 문제 해결과 같이 과학자가 갖추어야 할 덕목을 아주 맛 좋게 버무린 비빔밥 같았습니다.]]></content>
		<date><![CDATA[20170125031124]]></date>
		<update><![CDATA[20170125031124]]></update>
		<view><![CDATA[1322]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[277]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 김창엽]]></title>
		<content><![CDATA[학생이라는 신분으로 다른 회사에 방문해서 세미나를 듣는 느낌이 새로웠고 바로 옆건물이었지만 알 수 없었던 내부 근무 환경을 조금이나마 경험해 볼 수 있었던 점이 좋았습니다. 서울대, KAIST, 강원대, 고려대의 각 연구실과 NC 담당자들이 참석하였고, 발표에 관련된 점은 우리 연구실에 대한 내용은 제외하고 작성하였습니다. 

가장 인상적이었던 발표는 서울대학교 강유 교수님의 발표였습니다. 예전에 컴퓨터 보안이라는 도메인에서 공부를 하고 있었고 그 당시에 강유 교수님이 번역하신 책을 여러권 보고 공부를 하였습니다. 서울대학교 해킹동아리 가디언의 초대 회장 정도로만 알고 있었는 데 머신 러닝과 컴퓨터 보안을 접목한 내용을 발표해 주셔서 많은 도움이 되었습니다. 제가 연구해 보고 싶었던 주제도 Malware Classification인 데 그 부분을 Belief Propagation 알고리즘을 이용하여 접근한 것을 발표해 주셨습니다. "친구 9명이 다 나쁜놈이면 나도 나쁜놈일 확률이 높다"라는 설명으로 컴퓨터 보안 쪽의 평판 개념을 쉽게 설명해 주었습니다. 각 안티바이러스 업체들은 백신이 설치된 컴퓨터로 부터 모든 파일을 수집하고 그 파일을 검사함으로써 해당 시스템의 평판 정보를 구할 수 있고, 이 정보를 통해 해당 시스템 뿐만 아니라 해당 네트워크에 대한 평판 정보도 구할 수 있으며 시만텍의 경우 이 점수를 구할 수 있는 식을 가지고 있다라고 설명해 주셨습니다. 관련 논문 "Polonium: Tera-Scale Graph Mining for Malware Detection; https://www.ml.cmu.edu/research/dap-papers/dap-chau.pdf"울 소개해 주셔서 많은 도움이 되었습니다. 수집된 악성 코드를 처리할 때 수집된 곳의 평판 정보를 이용하여 하나의 Feature를 더하면 많은 도움이 될 것 같았습니다. 그 외 Tensor Decomposition에 대한 내용을 주로 설명해 주셨습니다. 네트워크 또는 그래프 분석을 통해서 사회의 많은 문제점을 해결하거나 분석해 볼 수 있는 데, Random Walk with Restart(RWR), Tucker, PARAFAC, ND-DTW 등의 실험  결과를 쉽게 설명해 주셔서 향 후 그래프 기반 분석작업을 할 때 꼭 시도해봐야 겠다는 생각이 들었습니다. 

다음으로 인상적이었던 발표는 Alice라는 교육프로그램을 운영하고 계셨던 KAIST 오혜연 교수님의 발표였습니다. NLP 분야에서 굉장히 오랫동안 연구를 하셨었고 이 분야에서 대가 이시라는 것은 소문을 통해서 알고 있었습니다. 발표 내용 중에 인상적이었던 부분은 이전에 노드 간의 관계를 분석하던 것을 엣지 간의 관계로 분석해 보자는 발상의 전환이었습니다. 주변에서 너무 당연하게 생각해서 놓치고 있는 부분이 많이 있겠구나 라는 생각을 하게되었습니다. 그 점외에도 LOL 게임의 문제를 사회의 문제로 투영해서 해석한 점도 놀라웠고 연구 주제를 정한 부분도 우리가 쉽게 접하면서 정말 즐길 수 있는 주제를 선택하였다는 느낌을 받았습니다.  

세번째는 생각해볼 만한 주제를 던져 주신 고려대학교 임희석 교수님의 발표였습니다. 연구 외적인 내용으로 삶의 목표(이쁜 마누라, 학위, 명예? 등등)와 직장을 그만두고 대학원생이 된 이유, 그리고 이루고자 하는 목표를 다시 되짚어 볼수 있는 기회였습니다. 교육과 관련된 주제 외에도 장애인 들을 도와주는 데에도 활용해서 인간 사회에 이바지할만한 주제에 대해 다시 생각해 볼 수 있었습니다.

이번 세미나를 통해서 우리 연구실에서 수행하는 연구 외에 다른 경험과 시각에 대한 발표를 들을 수 있어서 큰 도움이 되었습니다. 다른 연구실과 네트워킹할 수 있는 기회가 더 많았으면 좋겠다는 생각이 들었고 이런 기회를 만들어 주신 NC소프트 분들께 감사 드립니다.]]></content>
		<date><![CDATA[20170124200723]]></date>
		<update><![CDATA[20170124200909]]></update>
		<view><![CDATA[1415]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201701/588735d531aba7974945.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[58872e19759e81526519.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[276]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 김준홍]]></title>
		<content><![CDATA[연구실에서 산학과제를 하고 있는 NC soft의 판교에 위치한 사옥에서 2017년 1월 23일자로 총 6개의 연구실 5개 학교에서 현재 행하고 있는 연구 주제와 NLP기반의 산학과제를 어떤것을 하고 있는지에 관하여 공유하는 자리를 가졌습니다.

  각 연구실을 운영하고 계시는 교수님께서 발표해 주셨으며 본 연구실을 제외한 발표요약과 느낀점은 아래와 같습니다.

  [1] KAIST 오혜연 교수님꼐서는 Topic modeling에 관하여 오래 연구하셨으며 그 variant들에 관하여 말씀하여 주셨습니다. Network기반의 Topic modeling을 처음 접하였는데, 질문에서 나온 것처럼 최소 단위를 어떻게 설정하느냐에 따라서 network의 density가 달라질 수 있겠지만 typical LDA에 비하여 얻는 장점이 명확한 것을 알 수 있는 시간이었습니다. 그리고 Topic 간의 branch역할을 할 수 있는 term들을 다시 시각화 하여 주셨는데 이 부분에 대한 논문을 찾아서 읽어봐야 되겠다고 생각하였습니다. 개인적으로 LDA 적용관점에서 사용하고 깊게 variant들에 대하여 공부해본적이 없는데 이번기회에 찾아봐야 되겠다고 생각하였습니다.

  [2] 강원대학교 김학수 교수님께서는 한글기반의 분석에 대하여 많은 연구를 하고 계시다고 생각이 들었습니다. 말씀 하신 부분 중에서 보통 NLP의 4단계 분석에서 POS tagger가 잘못되어지면 상위 단계로 갈때의 문제점이 증폭되는 효과가 나온다는 부분에서 항상 생각하고 있었던 부분이라서 주의 깊게 청강 하였던것 같습니다. 그런 부분에서 새로운 POS tagger를 생성 하신다는것이 재미있었고 valid index를 통한 비교도 재미있었습니다. 실제로 한글 분석을 하게 되면 오픈되어 있는 형태소 분석기의 성능이 크게 만족이 되지 않는 경우가 많다는 것은 항상 느끼는 부분일 것입니다. 꼭 필요한 연구를 위하여 장기적인 관점으로 연구를 하신다는 점에서 감명 깊었으며 여러 application과 ‘NER’등의 문제에서 발생하는 에로사항들을 하나하나 말씀해 주셔서 재미있게 청취하였습니다.

  [3] 연세대학교 황승원 교수님께서는 botification이라는 단어로 발표를 이어주셨습니다. 챗봇 부분에서 상당히 공감되는 부분이 있었는데, 챗봇이 업무적인 관점이 아니라 개인단위의 수준까지 가려면 담화분석(discourse analysis)이 제대로 되어야 하며 이를 위해서 어떤 문제점에 대하여서도 설명해 주셨으며 지식의 계층적 구조에 대한 부분도 재미있게 청강하였습니다. 그리고 마지막에 Google PlaNet에 관하여 설명하여 주셨는데 너무 재미있는 내용이었습니다. 데이터가 쌓이게 되고 image particle을 이용하여 전체 사진을 만드는 것은 상당히 재미있는 결과라고 생각하였습니다. 

  [4] 고려대학교의 임희석 교수님께서는 교육에 관련된 연구와 학문에 근본적 기저에 대하여 발표하셨습니다. EGG를 통하여 들어온 Input에 관하여 프로그래머의 숙련도를 측정하는 방식을 말씀하여 주셨는데 재미있는 주제였고 더 얻게 된 점은 발표 후 질문자가 '프로그램을 하는 것으로 EGG input을 받는 것은 어떠한가?'라는 질문을 하였을 때 실제로 몸의 근육이 움직이는 환경에서는 여러가지 뇌파가 같이 활성화되는 것에 대하여서는 전처리가 쉽지 않다고 말씀하셔서 재미있었습니다. 예전에 EGG관련 연구에서 거짓말을 탐지하지 못한다는 결과를 언듯 본 기억이 있는데 EGG의 효용성이 관하여서도 관심을 가지게 되었습니다

  [5] 마지막으로는 서울대학교의 강유 교수님께서 발표하여 주셨으며 graph 기반의 계산 효용성과 그에 대한 application에 대하여 설명하여 주셨습니다. 개인적으로 정말 재미있는 강연이었습니다. 먼저 inverse matrix를 구축할때의 연구하신 결과물이 너무 훌륭하다고 생각이 들었고, 실제 데이터로 Network를 구축하여 보면 말씀해 주신 것처럼 적은수의 노드가 대부분의 엣지를 독식하고 있는(large degree)구조가 상당히 많게 나타나게 되며 이는 쇼핑몰 SNS 등등의 데이터에서 항상 나타나는 결과입니다. 이를 통하여 계산효율성을 증대하셨다고 말씀하셔서 이번기회에 꼭 이해하여야 되겠다고 생각하였습니다. 그리고 Tensor decomposition &amp; applications에 대하여 말씀하여 주셔서 정말 재미있게 청강했던 강연이었습니다.

  [*] 이렇게 산학을 하고 있는 연구실에서 하고 있는 연구와 결과물을 공유하여 한자리에서 청취하게 된 점에 대하여 NC soft측에 감사드리며 앞으로 분석을 할시의 생각할수 있는 범위를 넓히는데 큰 도움이 될 것 같습니다.

감사합니다.]]></content>
		<date><![CDATA[20170124193609]]></date>
		<update><![CDATA[20170125153629]]></update>
		<view><![CDATA[1365]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201701/58872e197d6cf7619262.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[logo.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[279]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 김보섭]]></title>
		<content><![CDATA[이번 workshop에서는 NCsoft와 협업하고하는 각 대학별 연구실의 교수님들과 학생들이 모인 뜻깊은 자리였습니다. 개인적으로 이번 workshop을 통해서 사고의 전환을 할 수 있는 좋은 계기였습니다. 새삼스레 Machine learning에서도 다양한 분야를 연구하는 연구실 각각의 프로젝트와 연구내용을 보고나니 정말 Machine learning이라는 학문이 정말 큰 학문임을 느낄 수 있었습니다. 인상깊었던 연구실의 발표내용을 듣고 느낀점을 정리하면 아래와 같습니다.

[1] KAIST
개인적으로 이번 workshop에 참석하기전 프로젝트에서 현재 topic modeling을 가장 많이 활용하고 있으므로 가장 관심이간 연구실은 kaist의 오혜연 교수님의 연구실이었습니다. 해당 연구실은 topic modeling의 활용보다는 알고리즘을 개선하는 방향에 목적을 둔 연구실로 오혜연 교수님의 발표내용을 들으면서 기존에 존재하는 알고리즘에 매몰되지않고 다른 관점에서 생각해봐야한다는 것을 느꼈습니다. 해당 연구실의 연구내용 중 하나로 topic modeling을 함에있어 document 내의 word에 topic을 assign하는 방식이아니라 context를 고려하여 document 내의 word간의 edge에 topic을 assign하여 document를 graph로 표현한다는 발상은 실로 참신하였습니다. 최근에 sentence를 image로 표현하여 CNN으로 classifier를 학습하는 방법을 처음 알고 충격을 금할길이 없었는데, 이번에 발표를 듣고 또 한단계 사고가 확장됨을 느꼈습니다. 또한 이를 통해서 느낀점은 연구란 것이 처음부터 답을 알고 진행하는 것이아니라 가정과 직관을 통한 여러 시행착오로써 이루어진다는 것을 다시 한번 뼈저리게 깨달은 발표였습니다. 

[2] 강원대학교
 강원대학교의 김학수 교수님이 이끄시는 자연어처리 연구실은 자연어처리에 집중하고 있는 연구실이었습니다. 해당 연구실에서 발표현 NER을 고려하여 pos tagging을 하는 방식의 연구는 개인적으로 최근에 관심있게 봤었던 metric learning과 큰 틀에서는 비슷하다는 느낌을 받았습니다. 또한 어렴풋이 알고있었던 자연어처리에 대해서 pos tagging, chatbot, ner 등 여러 방법론의 큰 틀을 상세하게 설명해주셨기때문이 해당 분야를 공부함에 있어 큰 기초를 얻었습니다. 

[3] 연세대학교
  연세대학교 황승원 교수님의 발표에서는 NCsoft와 협업한 내용을 발표한 부분에서 깊은 인상을 받았습니다. 우리 연구실과 같은 주제로 연구하였지만 조금 다른 시각을 가지고 연구하신 내용을 말씀하셨는데 classifier를 만드는 데 집중하신 것이아니라 야구의 기록으로 나타나는 structured data의 공간과 야구에 관한 기사로 나타나는 unstructured data의 공간의 관련도를 분석하신 방법론이 개인적으로는 참신하다고 생각하였습니다. 문제를 정의하는 방법에서 많은 내용을 배웠습니다.

 임희석 교수님과 강유 교수님의 발표에서는 개인적으로 아직 해당 분야에 대한 기초지식이 부족하여 많은 것을 느끼지는 못했지만 세상에는 아직 machine learning으로 풀어야할 문제가 정말 많다는 것을 느낄 수 있는 워크샵이었습니다. 다음에도 이런 워크샵이 진행되었으면 좋겠습니다.]]></content>
		<date><![CDATA[20170125163148]]></date>
		<update><![CDATA[20170125163148]]></update>
		<view><![CDATA[1314]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[280]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 조수현]]></title>
		<content><![CDATA[1월 23일 월요일 NC에서 주최하는 워크샵을 다녀왔습니다. 6개의 연구실이 한자리에 모여 연구 분야를 공유한 자리인만큼 뜻깊었고 배운 점 또한 많았습니다.  6개 연구실 모두 의미있는 연구를 하고있었지만 가장 인상깊었던 연구들은 다음과 같습니다. 

[1] KAIST 
KAIST의 연구를 들었을 때는 약간 망치로 머리를 맞은듯 한 느낌이 들었습니다. Topic modeling을 할 때 corpus 통채가 아니라 문서를 그래프로 구축하고 이 그래프를 input으로 넣는 발상은 굉장히 센세이션하게 다가왔습니다.  word사이의 edge에 topic을 할당한다는 개념이 새로웠고 좀 더 알아보고 싶다는 생각을 하게 되었습니다. LOL과 관련된 연구를 들었을 때도 비록 LOL 게임에 대한 이해가 부족하여 완전히 이해하는데 한계가 있었지만 게임 이론과 같이 전략으로써 유용하게 적용할 수 이는 연구라는 생각이 들었습니다. 이외에도 Hawkes process, narrative modeling 등 흥미로운 개념들을 많이 접할 수 있었고 나름대로 이를 어디에 적용할 수 있을지 고민해보는 계기가 되었습니다. 마지막으로 오혜연 교수님이 예전에 들었던 K MOOC 강좌의 그 교수님이란 것을 나중에 깨닫고 굉장히 놀라웠습니다.  

[2] 강원대
강원대 김학수 교수님의 강연에서는 자연어 처리의 A to Z까지 배운 듯 하였습니다. 한국어 자연어 처리에 관한 연구를 많이 해본 적도 없고 주로 주위 랩원들이 하는 연구로 자연어 처리를 배워 잘 모르는 제가 보기에도 차원이 다른 깊이의 연구인 듯 했습니다.  특히 NER을 고려하여 POS tagging한 부분 등 연구를 위해 험난한 노동을 아끼지 않았다는 것을 느낄 수 있었습니다.

[3] 연세대
황승원 교수님의 botification 강연에서는 bot의 목적이 가장 공감되었습니다.  인간이 어떤 정보를 물어봤을 때 fact를 전달해주는 봇이 중요한 것은 이미 널리 알려져 있고 저 또한 JAVIS, 챗봇같은 로봇은 인간이 필요한 정보를 전달하거나 인간을 대신하여 정확히 일처리하기 위한 대용품 정도로 생각했습니다. 하지만 황승원 교수님께서 말씀하셨듯이 인간이 "저런 거 별로지 않아?"라고 감정을 나타내는 문장을 던졌을 때  "응 별로야~" 이 정도만이라도 감정을 공유할 수 있는 챗봇이 개발 되면 외로움을 느끼는 현대인들이 조금이나마 위로를 받을 수 있을 것 같다는 생각을 했습니다. 개인적으로 황승원 교수님이 재미있게 강연해주셔서 봇의 필요성, 취지 등에 더 잘 공감할 수 있었습니다. 

이러한 연구들을 들으며 스스로 동기부여 하는 계기가 되기도 했지만 안그래도 많이 부족하다고 느꼈던 제 자신이 더 아무것도 아닌 존재가 되는 것 같았습니다.  하지만 얻어가는 것이 많은 워크샵이었던 만큼 남은 방학동안 시간을 알차게 분배하여 더 열심히 배워야겠다는 생각을 하게되었습니다. ]]></content>
		<date><![CDATA[20170125175900]]></date>
		<update><![CDATA[20170125175900]]></update>
		<view><![CDATA[1261]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[281]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 김동화]]></title>
		<content><![CDATA[17. 01. 23 NC soft AI center workshop - 김동화

저희 교수님 세션에서 이번에 작성한 논문에 관한 피드백을 받은 것같아 유의미한 시간이었습니다. 다중공동학습시 라벨을 어떻게 레이블링 하는지, 각 특징들이 나타내는 성능은 어떠한지 대한 질문이었는데 교수님께서 말씀하신데로 특정 진술을 일반화하기에는 다소 실험 데이터 적었지만 지금 현재 나온 결과에 대해서는 LDA모델은 멀티 클래스들을 다소 구분을 잘 했고, Doc2vec모델은 불균형적인 데이터에 대해 강건성을 나타내었다고 생각합니다. 하지만 이는 기회가 되면 실험  데이터나 횟수를 늘려 확인해 봐야 될 사항 인 것 같습니다.
토픽모델링에 관현 오래 연구를 하신 오혜연 교수님께서는 네트워크 기반 토픽모델링을 보여주셨는데 저는 그 연구자체가 이렇구나 저렇구나라는 생각보다 어떻게 그러한 논문아이디어를 낼 수 있었을까에 대해서 생각해 보았습니다. 문서각의 토픽모델링, 문서의 간의 네트워크, 토픽간의 연결성을 구현한 것인데 일반적으로 우리는 문서를 활용한 토픽모델링, 상호연결성을 고려한 네트워크 이를 독립적이라고 생각하고 한번도 이를 같이 엮어 보려고 하는 생각을 해보지 않았기 때문에 이러한 아이디어의 가능성을 생각해 보지 않았다고 생각합니다. 앞으로 다양한 수업을 듣게 되는 계기가 많아질 텐데 하나를 듣더라도 새로운 것을 수강하면서 기존에 것에 접목시키는 도전을 해보고 싶습니다. Point process 특정이벤트에 관련된 내용을 결합시켜 한 팀의 효율성에 대한 주제를 발표를 하실 때 이 부분은 학교에서 배운 응용확률 과정 수업에서 배운 내용과 비슷하다고 생각했습니다. 이러한 내용은 고전적이고 최신식 기법들이 아니끼 때문에 등한시하기 쉬운데 우리가 특정 알고리즘 방법론을 개발할 때 기저가 되는 개념이므로 이미 수강을 했지만 백프로 다 이해한 것이 아니기 때문에 여유가 있을 때 틈틈이 이것과 관련된 확률적이고 통계적인 기본적인 것부터 심화적인 것 까지 다루워 보려고 합니다.
종합적으로 모든 세션의 공통 키워드를 도출하자면은 그것은 ‘네트워크’ 같습니다. 네트워크란 개인, 집단, 사회의 관계를 네트워크로 파악하는 개념, 인공신경망은 시냅스의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 가리키는 것 처럼 이번 컨퍼런스의 키워드는 네트워크 였던 것 같습니다. 범용적이며 모든 알고리즘을 적용할 수 있다는 점에서 다음학기에 그래픽컬 모형에 관련된 수업이 있다면 수강하려고 합니다.
마지막을 서울대 강유 교수님께서 발표하신 Random walk 기반 belief network를 통한 악성 바이러스를 탐지하는 기법들은 우리 연구와 다소 다른점들이 많았지만 새로운 개념들을 접하고 이해하는데는 유익한 시간이었습니다.]]></content>
		<date><![CDATA[20170125184740]]></date>
		<update><![CDATA[20170125184740]]></update>
		<view><![CDATA[1265]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[282]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[18]]></member_uid>
		<member_display><![CDATA[모경현]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 모경현]]></title>
		<content><![CDATA[판교의 NC 사옥을 방문한다는 것부터 저는 이번 workshop에 대한 기대가 많았습니다. NC 근무환경을 체험해본다는 점과 더불어 NC라는 기업이 어떤 머신러닝 분야에 관심이 있는지 그리고 다른 연구실은 그런 연구주제에 어떤 방법으로 접근하고 있는지 접해볼 기회라고 생각했기 때문입니다. 다만 실제 발표를 들으면서 제가 가지고 있는 기초지식이 부족하여 대부분의 연구가 어떻게 진행되고 있는지 이해하는데 어려움이 많았습니다. 제게는 기초 공부를 더 탄탄히 해야겠다는 마음가짐을 심어준 workshop이었습니다.

작년부터 연구실에서 word senti 실험을 진행하면서 선배들에게 들었던 아이디어가 있었습니다. postagging을 진행하기 전에 같은 단어지만 표현이 다르게 된 것들을 미리 보정한다면 더 실험 결과가 좋을 것 같다는 아이디어였습니다. 이런 처리를 하지 않은 채 postagging을 진행했을 때 실제로 아쉬운 결과를 보기도 했어 공감하고 있던 아이디어였으나 그 방법에 대해서는 수작업 이외에 자동화된 방법을 고민해보진 않았습니다. 그런데 이번 workshop에서 강원대학교의 김학수 교수님께서 postagger의 한계와 그 이전의 단계에서 처리의 중요성을 말씀해 주셨습니다. 그리고 이 문제에 대해서 어떻게 접근하면 더 효율적일지 오랜 시간 고민하고 실험해오신 부분을 이야기해 주셨습니다. 앞쪽에서 발생된 오류가 NLP실험을 진행하면서 뒤로 갈수록 점차 커지게 되기 때문에 이를 보정하기 위해 통합언어모델을 사용한다는 것으로 이해했으나 디테일한 부분을 이해하지 못한 것이 아쉽습니다.

우리가 이제 배우고자 하는 분야가 있다면 학습하는데 필요한 자료는 대부분 인터넷에서 찾을 수 있습니다. 오히려 너무 많아서 나에게 필요한 것이 무엇인지 판단하는 것이 어렵기도 합니다. 그런 상황에서 개인에게 적절한 콘텐츠를 소개해주는 내용은 기본적인 추천시스템의 아이디어이기도 하지만 여전히 필요한 연구라고 생각합니다. 연구를 함에 있어 연구의 필요성과 취지에 대해 깊은 고민을 해야 함을 고려대학교 임희석 교수님의 발표에서 느낄 수 있었습니다. 또한 다양한 응용 애플리케이션 부분에 관해 생각해 볼 것들이 많이 있었습니다. 그리고 외적으로 머신러닝 분야를 공부하는 이유에 대해서 더 생각해 볼 기회도 있었습니다. 왜 이 분야를 업으로 삼고자 했는지, 이 분야를 통해 어떤 점으로 사회에 기여하고 싶은지 고민해보게 되었습니다.

우리 연구실에서도 많은 연구원들이 관심 있어 하고 고민해보고 연구해보고 싶었던 분야(그래프 기반의 NLP, 챗봇, Melware classification 등)에 대한 내용이 많았습니다. 저 또한 이번 workshop 동안 내가 머신러닝 분야에서 깊게 연구하고자 하는 것이 무엇일지 계속 생각해보았습니다. 다른 연구자들의 연구방향을 듣는 것은 이런 고민에 플러스 요인이 될 수 있다고 믿습니다. 이번 workshop에서 저의 부족함을 느꼈고 해당 분야를 이해하기 위해 더 깊은 공부를 해 나가겠습니다.]]></content>
		<date><![CDATA[20170125210718]]></date>
		<update><![CDATA[20170125210718]]></update>
		<view><![CDATA[1256]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[283]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 서승완]]></title>
		<content><![CDATA[17. 01. 23 NC soft AI center workshop - 서승완 

17.01.23 판교에 위치한 엔씨소프트에서 본 기업의 AI연구팀과 함께 산학 연구를 진행중인 6개 연구실에 대하여 각각의 연구실이 진행중인 연구 주제들과 산학 연구 주제에 대하여 공유하는 자리를 가졌습니다.

우선 본 연구실에 다음 학기부터 함께 참여를 하게되어 어떠한 연구들을 하고있는지 홈페이지에 있는 설명들만 접할 수 있었는데 이번 기회를 통하여 보다 자세하게 설명을 들을 수 있어서 좋았습니다.

KAIST에서는 Topic modeling분야를 오래 연구하신 오혜연 교수님께서 Users &amp; Information Lab의 연구들에 대하여 알려주셨습니다. 오락거리로만 생각했던 게임을 통해서도 여러가지 유의미한 정보들을 찾아낼 수 있다는 점도 상당히 흥미로웠지만, 문서를 분석함에 있어서 각각의 word들을 인풋으로 사용하는 것이 아니라 word들의 네트워크를 구하여 word 사이의 edge를 인풋으로 사용한다는 개념이 가장 신선하고 충격적이었습니다. 해당 방법을 통하여 기존에는 할 수 없었던 의미 추출을 할 수 있을것이라 생각했습니다. 또한 마지막에 설명하신 Hawkes process를 통하여 문서들 사이의 상관관계를 학습 할 수 있다면 멀지 않은 미래에 텍스트만으로도 앞으로 벌어질 일을 예측할 수 있는 세상이 오지 않을까 생각하였습니다.

강원대학교 김학수 교수님께서는 자연어 처리를 통한 텍스트 분석의 퍼포먼스를 높이는 연구를 주력으로 하신다는 생각을 하였습니다. 한글의 경우 영어에 비해 접두사의 범위가 한정적이지 않기때문에 분석을 함에 있어서 자연스레 문제가 됩니다. 김학수 교수님께서는 이러한 문제를 지금까지와는 다른 POS tagger(제가 이해하기로는 형태소 단위로 쪼개서 POS tag를 하는게 아니라 단어 단위로 해주겠다 라는 방식이었습니다.)를 생성하여 유의미하게 퍼포먼스를 향상시키고 있으며, 향후 수년간 연구를 진행할 것이라 하셨습니다. Capstone design을 하면서 web crawling과 text mining을 수행하였을 때, 영어 문서에 비하여 한글 문서의 퍼포먼스가 낮을 것을 보며 이런저런 해결 방법들을 생각해보곤 했었는데 조만간 한글도 높은 수준의 분석을 할 수 있지 않을까 기대가 되었습니다.

연세대 황승원 교수님께서는 botification에 관하여 말씀을 해주셨습니다. 지금은 챗봇을 비롯하여 사람과 의사소통을 하는 기계들의 한계점이 너무나 명확하지만 담화를 학습함으로써 앞으로는 사람과 수다를 떨 수 있는 로봇이 나올것이라 기대하신다고 말씀하셨습니다. 최근 1가구 1로봇 시대가 올 것이라는 뉴스들이 종종 보이는데 언젠가는 사람과 사람이 사는 집이 아니라 사람과 로봇이 사는 집이 만연해질까 약간은 무섭기도 했습니다. 또한 교수님의 생각을 이미지에 확장시켜 사진들을 연결시켜 만든 건물 전체를 보여주는 이미지에 각각의 사진들에 붙어있는 캡션들을 정제하여 보여준다는 생각은 정말 인상깊었습니다. 

이번 워크샵을 통하여 조금이나마 어떠한 연구들이 진행되고 있고, 학계에서 어떠한 주제에 관심이 있는지 알게되었습니다. 앞으로는 지속적으로 질문을 던지는 태도로 주위 연구들에 관심을 가져야겠다는 생각을 하였습니다.]]></content>
		<date><![CDATA[20170125232434]]></date>
		<update><![CDATA[20170125232434]]></update>
		<view><![CDATA[1243]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[284]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop - 박민식]]></title>
		<content><![CDATA[1월 23일에 NC소프트에서 기계학습, 자연어처리 워크숍에 있어서 다녀오게 되었습니다. 관련 연구를 하는 6개의 연구실의 교수님들이 연구실 소개와 진행하고 있는 연구들을 소개 하셨습니다. 발표를 들으면서 느끼게 된 점은 데이터 분석이라는 비슷하고 공통된 주제를 연구실마다 연구를 하고 있는데 데이터를 바라보는 관점이나 분석방법이 많이 다르다는것을 느꼈고 이 분야에 대한 외연의 폭이 넓어지는 느낌을 받았습니다.

저희 교수님이 연구실 소개와 프로젝트 진행된 것들을 발표하셨는데, 다른 연구실 구성원이 진행하고 있는(진행했었던) 프로젝트를 알 수 있어서 좋았습니다.

다음으로는 kaist의 오혜연 교수님께서 발표를 해주셨는데 elice라는 교육 프로그램과 ocw 강의에서 뵌적이 있는 분이어서 궁금했습니다. 토픽 모델링쪽으로 연구를 많이 하신다고 들었는데 저희 연구실과 다루는 분야가 비슷하다는 느낌을 받았고 word의 네트워크를 구해 word간의 edge를 인풋으로 사용하는 토픽모델링이 흥미로웠습니다.  Proficiency - Congruency Dilemma 라는 주제로 리그오브레전드 게임에서 챔피언 선택시 개인이 잘 하는 챔피언을 선택할지 팀의 조화를 고려한 챔피언을 선택할지에 대한 문제였습니다. 게임이론과 유사한 연구주제라고 생각이 들었고 관심이 가는 내용이었습니다.

강원대의 김학수 교수님은 자연어처리를 연구하시는 교수님이었습니다.,  저희 연구실에서 진행하는 텍스트 분석은 주어진 실제 문제를 해결하는데 텍스트 분석 기법을 사용하는것과 다르게 강원대에서는 문장의 구조적인 측면까지 분석해서 연구하는 점이 인상적이었습니다. 챗봇, 문서요약-제목생성등의 활용사례들도 동영상으로 보여주셨는데 공상과학 영화속에서 나오는 사람과 소통하는 로봇을 만들기 위해서는 많은 데이터와 노력이 필요하겠다는 생각이 들었습니다. 

서울대의 강유 교수님은 Graph Mining과 대용량 데이터 처리를 다루는 연구를 발표하셨습니다. Belief Propagation 개념을 이용해서 Malware detection 하는 연구를 발표하셨는데 제가 진행했었던 insider threat detection 연구도 belief propagation을 적용할 수 있겠다는 생각이 들었습니다. 서로 다른 시계열 데이터간의 거리관계를 보는 Dynamic Time Warping에 대한 내용 소개도 하셨는데 이번에 진행하려고 하는 음악 데이터 분석과 연관이 있어서 관심을 가지게 되었습니다.

흥미로운 주제가 있다면 연구실 간의 공동 연구를 했으면 좋겠다는 느낌을 받는 워크숍이었고, 좋은 발표를 해주신 교수님들과 자리를 마련해주신 엔씨소프트 관계자 분들께 감사드립니다.]]></content>
		<date><![CDATA[20170126092607]]></date>
		<update><![CDATA[20170126092607]]></update>
		<view><![CDATA[1292]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[285]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop – 이기창]]></title>
		<content><![CDATA[17. 01. 23 NC soft AI center workshop – 이기창

텍스트 마이닝 연구와 관련 많은 아이디어를 얻을 수 있는 자리였습니다. 각 연구실의 성과들을 한자리에서 듣게 돼 좋은 기회였던 것 같습니다. 인상 깊은 점 몇 가지 정리해봤습니다.

[1] 데이터 인풋 관련 상상력을 높이자 : 문맥을 고려한 그래프 매트릭스를 토픽모델링의 인풋으로 넣는다(카이스트 오혜연 교수님 연구실)든지, 구문분석 결과 나오는 자질까지 임베딩한다(강원대 김학수 교수님 연구실)든지 여러 가지 제안들이 있었습니다. 현실을 잘 반영하는 인풋일수록 모델링에 큰 도움이 될 것 같은데, 실제 정보를 가장 잘 반영하는 인풋의 형태가 무엇일지 앞으로 계속 고민해보겠습니다.

[2] 팩트보다는 관점, 나열보다는 해석! : 지식의 구조와 특징에 관해 강연해 주신 연세대 황승원 교수님에 따르면 사람들은 데이터의 단순 나열보다는 데이터를 관통하는 트렌드나 데이터 전체를 설명해주는 해석을 원한다고 합니다. 이와 관련 Word2Vec을 활용해 수량 형용사의 의미적 특징을 잡아내는 시도가 무척 흥미로웠습니다. 기계학습 모델을 구축하는 것만큼 그 모델이 내놓은 결과를 해석하는 것도 중요하다는 생각이 들었습니다.

[3] 오묘한 소셜네트워크의 세계 : Random Walk with Restart를 연구하고 계시는 서울대 강유 교수님이나 Social Graph에서 계층관계를 추출하려는 연세대 황승원 교수님 등등 여러 분들이 소셜네트워크를 들여다보고 계셨습니다. 21세기에 접어들면서 뿌리부터 바뀌고 있는 인류의 관계맺음 방식을 반영한 연구 트렌드가 아닐까 합니다. 트위터가 내리막길을 걷고, 페이스북 데이터는 크롤링이 어려워서 소셜네트워크 연구에 대한 큰 관심이 없었는데 여전히 중요한 연구주제라는 생각이 들었습니다.]]></content>
		<date><![CDATA[20170126120424]]></date>
		<update><![CDATA[20170126120424]]></update>
		<view><![CDATA[1273]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[286]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[2017/1/26][CS231n seminar]Training Neural Network2]]></title>
		<content><![CDATA[오늘은 저번시간에 이어서 NN을 트레이닝 하는것에 대하여 다루었습니다.

[0] Batch normalization 
[1] Optimization method
[2] Dropout
[3] Applications
[4] Ensemble

에 대하여 다루었습니다.]]></content>
		<date><![CDATA[20170126163807]]></date>
		<update><![CDATA[20170201110948]]></update>
		<view><![CDATA[51]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[287]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 01. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170126163903]]></date>
		<update><![CDATA[20170126163903]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[288]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop – 장명준]]></title>
		<content><![CDATA[1월 23일 NC soft와 협업하고 있는 6개의 연구실이 한 자리에 모여 각 연구실에 대한 소개와 진행중인 연구를 공유하는 워크샵을 다녀왔습니다. 이제 막 학부를 졸업한 신입생이기에 실제로 기업과 연구실에서 데이터 분석과 관련된 어떤 프로젝트와 연구를 진행하는지에 대해 막연한 궁금증이 있었는데 이 자리를 통해 많은 부분을 해소할 수 있었습니다. 또한 같은 데이터 분석 분야임에도 연구실마다 정말 다양한 방법으로 문제에 접근하는 것을 보고 많은 것을 느꼈습니다.

 우선 사고의 폭을 넓혀야겠다는 생각이 들었습니다. KAIST의 오혜연 교수님께서 Topic modeling에 대한 발표를 듣고 상당히 충격을 받았습니다. 각각의 word를 input으로 사용하는 것이 아니라 word사이에 네트워크를 구성하고 그 edge를 input으로 사용하는 방법은 굉장히 인상깊었습니다. 또한 저도 야구를 매우 좋아하기에 학부를 다니면서 야구에 대한 팀 프로젝트를 많이 진행했었습니다. 그때마다 항상 KBO 사이트에서 제공하는 타율, 타점, 방어율 같은 structured data만 사용하는 굉장히 틀에 박힌 분석을 했었습니다. 교수님께서 진행하신 기사를 분석해 선수의 능력치를 뽑아내는 연구 발표를 듣고 사고의 폭을 넓혀서 기존의 생각의 틀에서 벗어나야겠다는 마음을 갖게 되었습니다.

또한 가까운 생활에서도 연구 주제를 찾을 수 있다는 것을 알게 되었습니다. KAIST의 리그오브 레전드에 대한 Proficiency - Congruency 사이의 딜레마 연구 발표를 듣고, 항상 제가 좋아하고 잘하는 챔피언을 선택할지, 아니면 팀의 조합을 위해 재미없지만 필요한 챔피언을 선택할지 고민하던 저의 모습이 떠올랐습니다. 이런 일상 생활에서 발생하는 아주 사소한 일도 재미있는 연구 주제가 될 수 있다는 것을 느꼈습니다.

이번 워크샵을 통해 실제로 어떤 연구들이 진행되고 있는지를 알 수 있었지만 기초지식이 부족하여 많은 부분을 이해를 하지 못했습니다. 이 점이 한편으로는 아쉬웠지만 이를 통해 저는 정말 아무것도 모르고 있구나 라는 것을 느꼈습니다. 앞으로 부족한 부분을 메우기 위해 정말 많이 노력하겠습니다.]]></content>
		<date><![CDATA[20170126170454]]></date>
		<update><![CDATA[20170126170454]]></update>
		<view><![CDATA[1271]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[289]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[17. 01. 23 NC soft AI center workshop – 박재선]]></title>
		<content><![CDATA[방학에 접어든 지도 한 달, 규칙이 없어진 생활에 빠져 이른 아침에 먼 길을 나서는 일은 편하지 않았습니다. 집, 연구실, 집, 연구실의 생활에 혼자만의 생각에 빠져있었고, 다른 사람, 다른 분야의 의견을 듣는게 성가셔지고 있었습니다. 큰 기대 없이 들었던 Workshop에서 신선한 에너지를 받았습니다. 
#1 재해석
K-MOOC와 Elice project로 알고 있던 오혜연 교수님이 어떤 연구를 하는지 구체적으로 알 수 있었습니다. 아직 Topic modeling을 모르기에 어떤 의의를 가지는지 피부에 와닿지는 않았습니다. 하지만 일반적으로 단어 단위로 수행하던 알고리즘의 input을 edge로 재해석한 것이 논리적으로 합당하다고 생각하였고, 인상적이었습니다. 
강유 교수님의 Tensor decomposition도 흥미로웠습니다. 이 또한 어떤 기작을 거치는지 정확하게 알 수는 없었지만, 수학적 공식을 이용하여 새로운 의미를 도출하였습니다. 대 Deep learning 시대에, 이번 Workshop에서도 Deep learning의 판일 것이라 생각하였지만, 낯선 개념을 마주하여 재미있었으며, 탄탄한 수학적 베이스가 얼마나 중요한 것인가에 대해서 곱씹어보았습니다.
삼성전자와 함께하는 우리 연구실의 프로젝트도 마찬가지지만, 같은 알고리즘을 다른 관점으로 바라봄으로써 새로운 insight를 얻을 수 있다는 것을 다시 상기하였습니다.

#2 소재
 오혜연 교수님 연구실의 League of Legend 데이터 연구도 흥미로웠습니다. 평소 League of Legend를 즐겨 봅니다. 그래서 더 재미있게 봤습니다. 우리도 항상 팀워크와 자신의 특기 중 고민을 합니다. 이를 숫자로 풀어낸 멋진 사례였습니다. 이 문제를 전통적인 심리학에서 풀어내기 위해서는, 설계된 실험환경에서 적은 수의 사례밖에 구하지 못할 것이라고 생각합니다. 이 문제를 게임 기록을 이용하여 명확하게 밝혀냈습니다.
황승원 교수님의 Botification 또한 현실에 밀접한 소재였습니다. 이미 많이 알려진 기술인 챗봇의 application 가능성을 재미있게 이야기해 주셨습니다. 저는 기술이나 알고리즘을 현실 application과 접목하여 생각하는 것이 아직 부족한 것 같습니다. 

 몰두하여 연구를 하는 것도 중요하지만, 다른 연구를 보고 영감을 받는 것이 매우 중요하다고 느낀 하루였습니다. 자주는 아니더라도, 이런 자리가 또 있었으면 좋겠습니다.]]></content>
		<date><![CDATA[20170126222227]]></date>
		<update><![CDATA[20170126222227]]></update>
		<view><![CDATA[1314]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[290]]></uid>
		<board_id><![CDATA[9]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[17. 01. 26. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170127024803]]></date>
		<update><![CDATA[20170127024803]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[291]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 01. 26. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170130190804]]></date>
		<update><![CDATA[20170130190804]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[292]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[5]]></member_uid>
		<member_display><![CDATA[김 보섭]]></member_display>
		<title><![CDATA[[2017/1/31][CS231n seminar]Convolutional Neural Networks]]></title>
		<content><![CDATA[금일 세미나에서는 Convolutional Neural Network의 basic architecture인 conv layer와 pooling layer 등의 여러 layer 들, ILSVRC의 주요 모델들에 대한 리뷰 등을 하였습니다. 먼저 Convolutional Neural Network는 기존의 Artificial Neural Network와 달리 hidden layer의 hidden node (feature map의 하나의 요소값)을 receptive field(filter size)라는 개념을 도입해 input 전체에 가중치를 연결하는 것이아니라 sub-region에 가중치를 연결합니다. 또한 parameter sharing이라는 개념을 도입, feature map에 있는 hidden node는 가중치를 공유하여 가중치의 개수를 일반 neural net에 비해 현저히 줄입니다. 결론적으로 Convolutional Neural Network에서 하나의 filter는 하나의 feature map을 생성하고 feature map안의 각각의 원소값은 neural network의 hidden node와 매칭시켜 생각할 수 있습니다.
 Conv layer는 filter를 input에 적용하는 단계로 filter size, stride, zero-padding, # of filter 등의 hyper-parameter가 존재하고 최근에는 filter size를 작게 stride를 작게하여 Conv layer를 지날 때의 feature map의 크기 (weight, height)를 보존하여 layer를 deep한 구조로 만듭니다. zero-padding은 input의 크기 (weight, height)의 크기를 완전히 보존하고자 할 때 사용하기도 합니다.
 Pooling layer는 feature map의 적용하는 것으로 feature map의 크기를 가중치를 관리(모형복잡도 관리)할 수 있는 수준으로 만들 수 있습니다. max-pooling이 많이쓰이며 최근에는 Pooling layer를 넣지않고 feature map의 크기를 줄이고자 할 때, Conv layer의 filter size, stride를 크게 잡아 크기를 줄이는 경향이 있습니다.
  Normalization layer로는 사용하지않는 경향이 있었으나 최근에는 대부분 batch normalization layer를 활용합니다. 또한 원래 분류문제로 풀어내기위해 FC(Fully-Connected) layer를 사용하였는데 이마져도 계산을 줄이는 효과를 얻기위해 Conv layer로 대체하여 사용하는 추세입니다.
  ILSVRC 2012년 이후의 대체적인 CNN 모델들은 Conv layer를 쌓는 깊이가 늘어나고 layer 펼치는 구조(GoogLeNet)를 보입니다. ZFNet은 AlexNet의 hyper-parameter를 개선한 모델로 개선할 때, Deconvnet을 활용하여 feature visualizing을 통해 개선하였고, VGGNet은 layer를 깊이 쌓는 구조, GoogLeNet은 layer를 깊게 쌓으면서 펼치는 구조를 Inception이라는 개념을 통해 도입하였고, ResNet은 layer를 매우 깊게 쌓으면서 학습을 잘되게하기위해 skip connection이라는 개념을 도입하였습니다.
금일 세미나로 개인적으로 토론을 통해 많은 것을 배울 수 있었습니다. 감사합니다.]]></content>
		<date><![CDATA[20170131165624]]></date>
		<update><![CDATA[20170207125935]]></update>
		<view><![CDATA[28]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[293]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2017/02/02][CS231n seminar]Convnets spatial localization and object detection]]></title>
		<content><![CDATA[이번 세미나에서는 computer vision task 중에서
localization 문제와 object detection을 Convolutional Neural Network를 이용하여 해결하는 연구들에 대해 살펴보았습니다.

localization 대한 개념 및 출력값에 대한 정의를 보았고 overfeat이라는 알고리즘에 대해 알아보았습니다.

object detection 부분에서는 convolutional neural network로 인하여 computer vision 분야가 활성화 된 후 연구가 되었던 R-CNN, SPPNet, Fast R-CNN, Faster R-CNN 등에 대해 발표를 하였습니다.

발표를 진행하면서 듣는 질문들과 토론을 들으면서 발표준비에 부족했던 점을 알게 되었습니다.
다음 발표에는 YOLO 논문에 대한 소개와 시간이 된다면 다른 detection 알고리즘이나 weakly supervised localization 개념에 대해서도 다뤄볼 생각입니다.

참고로 mean average precision에 대한 자세한 설명은 아래 동영상 2분 31초를 보시면 될 것 같습니다.
https://www.youtube.com/watch?v=pM6DJ0ZZee0]]></content>
		<date><![CDATA[20170202194318]]></date>
		<update><![CDATA[20170210175907]]></update>
		<view><![CDATA[22]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[294]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[2017/02/07][CS231n seminar ] Visualization]]></title>
		<content><![CDATA[오늘 세미나에서는 Conv net이 얼마나 잘 학습이 되는지 어떠한 feature들을 가지고 있고 이것을 어떻게 해야 잘 표현 할수 있을까에 대해서 공부해 보았습니다.

1. How to project the feature activations back to the input pixel space
 : 특정 원하는 유닛에 대한 activation의 gradient를 1를 주어 이미지 밑단까지 reconstruction
2. Visualizing and understanding CNN
 : occlusion의 의한 클래스 분류 변화 향상, correspondence 분석을 통한 일관성 확임
3. How can we find an image that maximizes some class score
 : 픽셀마다 클래스 스코어 값을 계산해 객체를 탐지할수 있음
4. Visualizing via Regularized optimization
 : 정교하게 시각할수 있도록 regularization term 을 사용 
5. Deep dream
 : 특정 레이어, 뉴런에 활성함수 값을 촉진시켜 꿈을 꿈
6. Neural Style
 : content 정보, style 정보를 융합하여 합성사진을 구할 수 있음
7. Adversarial Examples
 : input형태가 거의 비슷하지만 결과값이 반대로 나오게 하도록 하는 examples 생성]]></content>
		<date><![CDATA[20170207163840]]></date>
		<update><![CDATA[20170307082954]]></update>
		<view><![CDATA[23]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[295]]></uid>
		<board_id><![CDATA[4]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[4]]></member_uid>
		<member_display><![CDATA[관리자]]></member_display>
		<title><![CDATA[2017. 02. 05 ~ 2017. 02. 06 동계 워크숍]]></title>
		<content><![CDATA[2월 5일부터 6일까지 겨울 워크숍으로 강원도 정선에 다녀왔습니다. 관련 사진 첨부합니다.]]></content>
		<date><![CDATA[20170208225420]]></date>
		<update><![CDATA[20170208225420]]></update>
		<view><![CDATA[1302]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/4/201702/589b230cc82ae9865692.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[연구실 워크샵.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[296]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[[2017/02/13][CS231n seminar ] RNN and LSTM]]></title>
		<content><![CDATA[금일 세미나에서는 Recurrent Neural Networks와 Long Short-Term Memory Models에 대해 배워보는 시간이었습니다. 두 모델 모두 히든 노드가 directed cycle을 형성하는 인공신경망의 한 종류로 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 것으로 알려져 있습니다. 인풋과 타겟 길이에 상관이 없기 때문에 다양하고 유연하게 네트워크를 구성할 수 있으며 최근 NLP 분야를 중심으로 각광받고 있습니다. 
LSTM이 RNN과 다른 점은 hidden state에 cell state가 추가됐다는 점입니다. LSTM의 cell state는 RNN의 vanishing gradient 문제를 해결하기 위해 도입된 것으로, 학습 과정에서 가벼운 선형 연산만 이뤄지기 때문에 gradient가 RNN 대비 비교적 잘 전파가 되는 것으로 알려져 있습니다.
금일 세미나에서는 RNN과 LSTM의 forward, backward pass 계산 방법을 중점적으로 다루었습니다. 카파시가 제안한 계산그래프를 토대로 순차적으로 foward pass를 계산하고 마지막 state에서 loss값까지 구한 뒤 forward pass를 따라 차례대로 backpropagation을 수행합니다. 여기서 중요한 점은 RNN, LSTM 모두 hidden state가 recurrent하게 구해지기 때문에 backpropagation 수행시 직전 계산에서 구해진 gradient가 재귀적으로 더해진다는 사실입니다.
cell state가 추가돼 backpropagation 계산과정이 다소 복잡해졌다는 점 빼고는 RNN과 LSTM이 본질적으로 같습니다.]]></content>
		<date><![CDATA[20170210164755]]></date>
		<update><![CDATA[20170306191524]]></update>
		<view><![CDATA[29]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[297]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 02. 03 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170212000422]]></date>
		<update><![CDATA[20170212000422]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[298]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 02. 09 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170212000449]]></date>
		<update><![CDATA[20170212000449]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[299]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[2017/02/13][CS231n seminar] Training ConvNets in practice]]></title>
		<content><![CDATA[(이사 / 휴가 관계로 늦게 업로드 합니다.)

세미나 시간에는 주로 ConvNets을 다룰때 실전에서 어떤 문제점이 있고, 그것들을 어떻게 해결해나갔는지 다루었습니다. 조금 오래되었지만, 최대한 상세히 적어서 리뷰해보겠습니다.

1. 데이터 부족 현상
 - Data argumentation이라는 방법을 이용해서 데이터를 부풀렸습니다. 대칭, 평행이동 등의 간단한 방법부터 시작해서 resize and cropping이라는 방법을 통해 여러개의 cropped image를 생성하여 학습하는 방법을 주로 다루었습니다. test단계에서도 당연히 test image를 crop하여 성능 측정을 하였는데, 5개의 size를 정해서 resize 한 뒤 그 안에서 10개의 cropped image를 만들어내고 그 성능을 평균내는 방법을 취했습니다. 이 뿐만 아니라, color jittering이라는 방법으로도 데이터를 부풀릴 수 있었는데, 이는 가장 쉽게는 명도, 채도등의 이미지의 특성을 변환하는 방법이 있습니다. 주로 PCA를 이용한 jittering을 사용하고 있으며, 모든 픽셀의 색정보에 대해 주성분을 구하고 색을 흩어지게 만들어서 데이터를 부풀렸습니다.

2. 가져다 쓰기
 - 여기에서는 데이터의 양과 imageNet의 데이터와의 이질성 정도로 내가 학습할 형태를 고려해보고, 가이드라인을 제시하는 것을 배웠습니다. 주로, VGGNet(이 뿐만 아니라 다른 Net도 가능할 것)을 가져와서 학습시키는 내용을 추천했습니다. 주의할 점은 fully connect layer나 CONV layer의 최상층을 학습시키고자 할 때, learning rate를 잘 설정해줘야 구조가 무너지지 않는다고(학습이 잘 이루어지지 않음) 설명하였으며, 강의에서 추천한 사항으로는 FC는 원래 learning rate의 1/10을, CONV layer 최상층은 1/100을 추천하였습니다. 

3. 잘 쌓는 방법
 - conv layer를 임의로 쌓을 것이 아니라, 정교하게 잘 쌓기를 고민해보는 파트였습니다. 지난 외부 세미나에서 황상흠 박사님께서 말씀하시기로는 구조만으로도 학습이 탄탄하게 되게 만들 수 있지 않을까라고 내부적으로 추측하고 있다고 했는데, "잘" 이라는 말 자체에서 의미가 일맥상통한 것이 있지 않을까 생각합니다. 잘 쌓아야 되는 이유에 관해서는 3x3 filter를 3층 쌓으면 7x7 filter와 같은 정보양을 담고 있으며, 이때 사용되는 파라미터가 적고, multiply 연산이 적다는 것을 근거로 잘 쌓으면 더 빠르고 좋다! 라고 설명하였습니다. 뿐만아니라, 1x1 filter를 사용한 bottleneck sandwich 구조라던가, 3x1 filter와 1x3 filter를 이어서 쌓는 구조에 관해서도 얘기했습니다. 

3. 빠르게 작동시키는 방법
 - 잘 쌓더라도, 너무 느리게 학습하면 무용지물이 될 수도 있습니다. 이에 더 빠르게 연산하는 방법론에 대해 고민한 흔적들에 대해 소개해보겠습니다. 첫 번째로 설명한 방법은 im2col입니다. 이는 입력으로 오는 feature map과 weight 역할을 하는 filter를 벡터로 쭉 풀어서 내적연산을 하는 방법입니다.중간에 재선이가 질문하기로는 어차피 똑같이 연산되지 않겠느냐 했는데, 이 부분은 컴알못이라 정확히 설명하지 못했습니다. 다만, 이것을 통해 filter가 움직인다는 개념으로 접근하는 것 뿐만 아니라, 이를 빠르게 하기 위해서 어떻게 노력하고 있고, 이를 수학적으로 표현하는 방법을 연구하고 있다라는 것까지만이라도 알아둔다면 추후 공부하는데 어느정도의 도움이 되지 않을까 생각합니다. 두 번째로 설명한 방법은 Fast Furier Transform입니다. 이 부분은 Furier Transform이 나오는데 간단히 mapping으로 설명하였습니다. 이를 통해 더 빠른 연산이 가능하다고 설명하고 있습니다. 세 번째로 설명한 방법은 fast algorithm입니다. 이는 행렬곱 연산에서 곱하기를 더 적게 사용하는 방법인데, 아직은 많이 사용되지 않지만 앞으로 많이 사용할 것이라고 예상하고 있습니다.

4. 더 빠르게 작동시키는 방법
 - 먼저, 다들 아시다시피 GPU가 쉬운 반복연산을 하는데 더 수월합니다. 이에, 행렬연산이 많은 딥러닝 구축에 GPU를 사용하면 더 속도가 빨라지는 것을 확인할 수 있었습니다. 컴퓨터에서 실수 정밀도를 32bit를 사용하지만, 최근 추세는 더 적은 bit를 사용하면서 성능 손실을 최소화 하는데 있습니다. 16bit를 사용하면서 반올림 방법론을 새롭게 만들어서 성능손실이 최대한 없게 하는 stochastic rounding이 있음을 소개하였습니다. 더 최근으로는 1bit를 사용한 딥러닝까지 나오고 있습니다.]]></content>
		<date><![CDATA[20170225230204]]></date>
		<update><![CDATA[20170307083434]]></update>
		<view><![CDATA[29]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[300]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[[2017/02/24][CS231n seminar] Segmentation, Soft Attention, Spatial Transformers]]></title>
		<content><![CDATA[이번 세미나 시간에는 Classification, Localization, Object Detection에 이은 Segmentation 등의 유연한 vision task에 대하여 다루었습니다.

[1] Segmentation
(1) Semantic Segmentation: 이미지의 각 pixel에 label을 할당하는 task
 - Learning Hierarchical Features for Scene Labeling: 이미지로부터 여러 Scale을 추출한 뒤 CNN과 CRF를 이용하여 segmentation
 - Recurrent Convolutional Neural Networks for Scene Labeling: 이미지의 여러 scale을 recurrent하게 CNN을 적용하여 segmentation
 - Fully Convolutional Networks for Semantic Segmentation: 기존 CNN의 Fully connected layer를 1x1 convolution으로 대체한 뒤, 마지막 output을 upsampling하여 arbitrary size의 이미지를 한번 CNN으로 segmentation.
          Skip architecture 사용
 - Learning Deconvolution Network for Semantic Segmentation: VGG-16의 구조에 거울상의 Deconvolution network를 배치하여 순차적으로 upsampling하고 segmentation

(2) Instance Segmentation: 각 object를 다르게 인식하는 segmentation
 - Simultaneous Detection and Segmentation: MCG를 이용하여 Region proposal하고 Bounding box와 Foreground로부터 각각 추출한 feature를 이용하여 segmentation
 - Hypercolumns for Object Segmentation and Fine-grained Localization: FCN의 Skip architecture와 유사한 architecture를 Instance segmentation에 사용
 - Instance-aware Semantic Segmentation via Multi-task Network Cascades: Faster R-CNN과 유사하게 Region proposal을 Network로 통합하여 optimization함

(3) Attention: 데이터에서 중요한 부분을 강조
 - Show, Attend and Tell: Neural Image Caption Generation with Visual Attention: CNN과 LSTM에 Attention 함수를 이용하여 조합.
 Attention 함수로부터 주시하는 이미지 영역이 도출됨
 - Spatial Transformer Networks: Affine transformation을 이용한 layer를 넣어 transformation을 학습하고, 이를 이용해 network에서 중요한 부분을 도출]]></content>
		<date><![CDATA[20170226002611]]></date>
		<update><![CDATA[20170307115901]]></update>
		<view><![CDATA[27]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[301]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[18]]></member_uid>
		<member_display><![CDATA[모경현]]></member_display>
		<title><![CDATA[[2017/02/28][CS224d seminar] Intro, word vector]]></title>
		<content><![CDATA[이번 세미나 시간에는 새로 시작하는 NLP강좌 CS224d에 대한 내용을 다루었습니다.

[1] Intro
- NLP의 정의, 목적에 대해서 알아보았습니다.
- NLP Level을 5단계에 나누어 정의하였습니다.
- NLP와 Deep learning이 접목된 이유를 살펴보고 응용 애플리케이션 분야를 확인하였습니다.
- NLP가 왜 어려운 지 예시를 통해 알아보았습니다.

[2] word vector
- texonomy부터 distributional similarity까지 컴퓨터가 단어를 인식하는 방법을 알아보았습니다.
- 차원 축소 방법론으로 SVD를 살펴보았습니다.
- word2vec의 개념과 단어벡터를 학습하는 방식을 살펴보았습니다.]]></content>
		<date><![CDATA[20170302000437]]></date>
		<update><![CDATA[20170313103839]]></update>
		<view><![CDATA[34]]></view>
		<comment><![CDATA[11]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[302]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 03. 03 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170303172245]]></date>
		<update><![CDATA[20170303172245]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[303]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 03. 03. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170303172808]]></date>
		<update><![CDATA[20170303172808]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[304]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[[2017/03/03][CS231n seminar] video]]></title>
		<content><![CDATA[Video
Video 분석은 이미지분석에 시간축(temporal)을 고려한 분석이라고 할 수 있다.
우리가 일반적으로 이미지분석을 할때 필터단위의 패치단위의 관계를 ConvNet으로 표현을 한다. 이러한 합성곱의 방식은 높은 분류성능을 가지지만 이러한 Spatio 관점과 temporal 한 관점을 추가적으로 고려해야한다면 이 연속적인 프레임단위 분석의 복잡성은 굉장히 증가하는데 어떻게 이 시간과 공간적인 관점을 ConvNet으로 표현할지가 연구의 주요 이슈라고 할 수 있다.
- 비디오의 분류는 연속적인 한 프레임단위씩 예측을 하게 되고 SVM 같은 분류기로 Bag-of-word 와 함께 예측이 되어진다.
- [width x height x time 을 고려한 3D matrix] &gt; blocks &gt; bins &gt; pixel 단위로 세분화 시킬 수 있고 밑단 부터 가지는 magnitude 와 oriatation 을 산출해 히스토그램을 작성하고 그중에 영향력이 있는 부분을 강조시키면서 그 3D matrix을 표현하여 descriptors(features)을 뽑아낸다. 
- the curse of dimensionality 을 없애기 위해 차원축소법을 통해 불필요한 부분을 제거하여 bag-of-word 와 매칭시켜 프레임단위로 이미지를 예측한다.
-일반적으로 계산시간과 모델의 정확도는 trade-off를 가진다.]]></content>
		<date><![CDATA[20170306235301]]></date>
		<update><![CDATA[20170313103110]]></update>
		<view><![CDATA[25]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[305]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 03. 10 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170310165409]]></date>
		<update><![CDATA[20170310165409]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[306]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2017/03/10][CS224d seminar]  Advanced word vector representations]]></title>
		<content><![CDATA[이번 발표에서는 저번 cs224d 발표에 이어 word representing 방법에 대해 소개를 하였습니다.

1. Word2Vec 모델의 단어 차원에 대해서 줄이는 방법에 대해 발표하였습니다.

(1) Hierarchical Softmax : 계산량이 많은 Softmax fuction 대신에 빠르게 계산 가능한 multinomial distribution function을 사용한 테크닉입니다. 각 단어들을 leaves로 가지는 binary tree를 만든 후 해당 단어가 나올 확률을 구하게 됩니다. 기존 V차원에서 log2(V)차원으로 단어의 차원수가 감소하여 계산량을 줄일 수 있습니다.

(2) negative sampling
소프트 맥스 함수의 분모에서 모든 분모의 차원에 대해서 계산을 하면 계산량이 많으니 5~20개의 negative sample만 추출해서 학습을 하는 방법입니다. 기존의 word2vec과 다른 error function을 사용하고 계산량이 감소하므로 기존 word2vec의 문제를 해결할 수 있고, Hierarchical Softmax에 비해 성능이 좋다고 하여서 많이 사용되고 있습니다.

(3) Subsampling frequent words
'the', 'a', 'in' 등과 같은 자주 등장하는 단어들을 확률적으로 제외하여 학습속도와 성능을 향상 시키는 방법입니다.

2. Glove
matrix factorization 기반으로 Word Embedding을 하는 방법입니다. 기본적으로 단어들 사이에 co-occurrence matrix X를 만든 후, 각 단어의 weight w_i, w_j의 내적이 log X_ij 와 최대한 유사해지게 만드는 weight 들을 구하는 방식입니다.

3. Fast text
2016년에 Facebook 연구그룹에서 개발한 word embedding, text classifier 방법입니다. NNLM, Word2Vec, Glove 등에서는 개별 단어 벡터들이 기본 분석 단위여서 희귀한 단어나, 형태소가 다양한 언어의 어휘에 대해서 word embedding을 할 경우 좋은 embedding 결과를 얻기 어렵습니다. 이러한 단점을 해결하기 위해서 목적함수는 skip-gram 방식과 유사하지만 어휘를 n-gram 단위로 분해하여 워드 임베딩한 결과들을 출력하고 이를 통해서 기존 word embedding의 한계점이 있는 어휘(희귀한 단어, 형태소가 다양한 언어의 어휘)에 대해서 좋은 embedding 결과를 얻을 수 있습니다.]]></content>
		<date><![CDATA[20170310200252]]></date>
		<update><![CDATA[20170313130059]]></update>
		<view><![CDATA[44]]></view>
		<comment><![CDATA[15]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[307]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 03. 17 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170317164512]]></date>
		<update><![CDATA[20170317164512]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[308]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[[2017/03/03][CS231n seminar] video - second]]></title>
		<content><![CDATA[비디오 객체 행위 탐지의 목적은 크게 두개로 분류 할 수 있습니다.
1. Global motion
: 전역적인 event를 어떻게 Convnet 으로 시간, 공간적의미를 담아 낼수 있을까라는 문제점에서
LSTM이라는 Infinite context 정보룰 담아 낼 수 있는 방법론을 사용해서 그 관계들을 학습시켰습니다.

2. local motion
: 또한 지역적 탐색은 크게 4가지로 분류 됩니다
    (1) single frame
     - 단일 이미지마다 분석하는 방식으로 이 방식 또한 간단한 방식이 것 치고 좋은 성능을 보입니다
    (2) late fusion
    - 꽤 먼거리에서 발생되는 이미지 프레임들을  독립적으로 학습시켜 마지막 fcLayer에서 합쳐지는 구조입니다. 
    (3) early fusion
    - 지역적으로 몇개의 프레임을 첫번째 레이어에서 합치는 구조로 그 이후에는 싱글 프레임 과 같이 학습되는 구조를        가지고 있습니다.
    (4) slow fusion
     - early fusion과 late fusion 두가지 방식을 고려한 구조로 중간의 clips 들이 단계적으로 학습되는 방식입니다.

저 개인적으로 느끼는 비디오 분야와 이미지분야에 가장 큰 핵심 포인트는 이미지는 단순 픽셀값을 고려했다면 비디오분석에 있어서는 그 이미지 픽셀의 값과 방향성을 고려한다는 점이 특별한것 같습니다.]]></content>
		<date><![CDATA[20170319001447]]></date>
		<update><![CDATA[20170403093210]]></update>
		<view><![CDATA[26]]></view>
		<comment><![CDATA[13]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[1]]></unlike>
		<vote><![CDATA[-1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[309]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 03. 24 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170324163417]]></date>
		<update><![CDATA[20170324163417]]></update>
		<view><![CDATA[6]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[310]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[2017/03/24][CS224d seminar] Word window classification and neural networks]]></title>
		<content><![CDATA[이번 세미나에서는 임베딩된 word vector를 이용하여 classification 문제를 푸는 extrinsic task에 대해 알아보았습니다.

[1] extrinsic task and softmax classification
 optimize된 word vector를 실제 문제에 적용하는 것으로 일반적인 머신러닝 문제와는 달리 NLP 문제에서는 training data를 다시 retrain시키기도 합니다.  classification에 사용되는 cost function은 softmax를 사용한 cross entropy function인데 이 function을 목적함수로 쓸때, back propagation에 유리합니다.

[2] window classification
단어를 하나하나 classify하는 것보다 더 빠르고 간단하게 하는 방식으로 여러개의 word vector로 구성된 window를 한 단위로 classify 하는 것입니다. window classification을 이용할 때 주위 단어까지 고려하므로 단어의 문맥적 의미를 더 반영할 수 있습니다.

[3] neural network
linear classifier는 데이터를 정교하게 구분하지 못하고 구분한다고 해도 그 성능이 제한되어 있기 때문에 non linear classifier로 neural network를 사용합니다. neural network에서는  window를 input으로 받고 activation function까지 거쳐 하나의 score값이 나오는데 이 값이 높을 수록 특정 단어의 위치를 잘 구분했다고 할 수 있습니다.]]></content>
		<date><![CDATA[20170325000707]]></date>
		<update><![CDATA[20170403094456]]></update>
		<view><![CDATA[28]]></view>
		<comment><![CDATA[14]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[311]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 03. 29 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170329184323]]></date>
		<update><![CDATA[20170329184323]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[312]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 03. 31 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170331170756]]></date>
		<update><![CDATA[20170331170756]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[313]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[[2017/03/31][CS224d seminar] Project tip &amp; backpropagation-김동화]]></title>
		<content><![CDATA[오늘 세미나에서는 역전파 알고리즘 방법론에 대해서 살펴보았습니다.
학습되는 미분계수는 가중치(W,U), 상수항 (b), 워드벡터(x)에 대해서  고려되어 목적함수 max margin function에 에러를 최소화하는 접근방법론을 살펴보았습니다.
목적함수는 true window 스코어를 최대화시키고 corrupt window 스코어를 최소화시키므로써 목적함수 J를 최소화시키는 방향으로 학습이 되어집니다.]]></content>
		<date><![CDATA[20170331182117]]></date>
		<update><![CDATA[20170411204452]]></update>
		<view><![CDATA[30]]></view>
		<comment><![CDATA[11]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[314]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[17. 04. 06. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170406175706]]></date>
		<update><![CDATA[20170406175752]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[315]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[17. 04. 10 회의록(최종보고)]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170410154216]]></date>
		<update><![CDATA[20170410154216]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[316]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 04. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170412182607]]></date>
		<update><![CDATA[20170412182607]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[317]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[17. 04. 12. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170413002341]]></date>
		<update><![CDATA[20170413002341]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[318]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[2017/04/14] 개인 연구 세미나 - 박민식]]></title>
		<content><![CDATA[이번 세미나에서는 음악 데이터 분석을 위한 푸리에 변환 방법 소개, spectrogram을 이용한  오디오 데이터 CNN 분류, Generative adversarial networks 이용한 음성 인식 성능 향상 논문 소개, 향후 진행 계획 등에 대해서 설명하였습니다.
 
Q. SeqGAN

discrete 한 sequence 데이터의 경우 토큰들의 sequence를 생성할 때 현재 얼마나 generator가 잘 학습하고 있는지 평가할 방법이 마땅치 않아  sequence generation task에서는 잘 활용되지 않았었습니다. 

이 논문에서는 discriminator에서 받은 policy reward를 generator에서 학습을 할때 사용하고 action을 취할때 MonteCarlo Search를 사용하였다고 소개하고 있습니다.

자세한 내용은 https://arxiv.org/pdf/1609.05743.pdf
에서 확인해 볼 수 있습니다.]]></content>
		<date><![CDATA[20170415143129]]></date>
		<update><![CDATA[20170416232131]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[2]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[319]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[2017/04/14][CS224d seminar] Neural Tips &amp; Tricks  and RNN - 김형석]]></title>
		<content><![CDATA[본 세미나 시간에는 본 강좌에서 주가되는 Deep leanrning기반의 NLP에 대해서 다양한 팁과 trick들을 다루었습니다.
 우선적으로 지난 시간에 다룬 Backpropagation에 대해서 다시 한번 재정리하는 시간과 예제를 통해서 이에 대한 이해를 돕도록 하였습니다. 그리고 딥러닝을 통한 NLP의 일반적인 구성 전략으로서 다음과 같은 모델 구성 요령을 알아보았습니다.
  1. Select appropriate Network Structure 
   - Structure: Single words, fixed windows vs Recursive, Sentence Based vs Bag of words
   - Nonlinearity
 2. Check for implementation bugs with gradient check
 3. Parameter initialization
 4. Optimization tricks
 5. Check if the model is powerful enough to overfit
    - If not, change model structure or make model “larger”
    - If you can overfit: Regularize
 
 상기 모델 구성 요령에 대한 다양한 선택 옵션들에 대해서 배워보았으며, 추가적으로 RNN에 대한 기본적인 소개와 RNN에서 문제시 되는 gradient vanishing과 exploding에 대해서 어떠한 해법들이 존재하는 지도 알아보았습니다.
 lecture이외의 다양한 논문에 대해서 다루지 못한점이 다소 부족하다고 생각됩니다. 다음 세미나에서는 좀더 많은 논문 탐색을 통해서 랩실 구성원들에게 좋은 지식을 전달하도록 노력하겠습니다.
 이상입니다. 감사합니다.]]></content>
		<date><![CDATA[20170417021120]]></date>
		<update><![CDATA[20170419165805]]></update>
		<view><![CDATA[32]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[320]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 05. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170512175747]]></date>
		<update><![CDATA[20170512175747]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[321]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[2017/05/12][CS224d seminar] Recurrent neural network]]></title>
		<content><![CDATA[이번 세미나에서는 Language model에서부터 시작하여 RNN, RNN에서 발생하는 gradient vanishing, explosion 문제에 대해 알아보았습니다. 또한 이를 해결하기 위한 경험적인 tip들과 RNN을 활용한 연구까지 소개하였습니다.

[1] language model은 sequence가 있는 단어의 발생을 확률로 나타내는 모델입니다. Machine translation에 흔히 사용되는 모델이지만 몇 개의 window size로는 문서의 문맥적 의미를 파악하는데 한계가 있어 recurrent neural network를 사용합니다. RNN은 이론상으로는 문서 내 모든 단어를 사용하여 다음 단어를 예측하지만 backpropagation을 할 때 앞 단으로 갈수록 gradient가 사라지는 경향이 있어 실제 문제를 푸는데 한계가 있습니다. Gradient가 사라지는 vanishing gradient에 대해 수식적으로 증명하였습니다.

[2] 이런 vanishing gradient를 해결하기 위한 경험적인 tip으로 gradient clipping, class를 먼저 예측한 후 다음 단어 예측, identity matrix로 초기화 한 후 ReLu 사용하기 등을 소개하였습니다. 

[3] 마지막으로 RNN의 application으로는 language modeling과 opinion mining 등을 소개하면서 bidirectional RNN을 설명하였습니다. 다음시간에 bidirectional RNN의 학습과정을 준비해오겠습니다.]]></content>
		<date><![CDATA[20170513100816]]></date>
		<update><![CDATA[20170519115012]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[13]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[322]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2017/05/12][CS224d seminar] Fancy recurrent neural networks for machine translation]]></title>
		<content><![CDATA[제가 이번 세미나 시간에 맡은 부분은 기계번역을 위한 딥러닝 방법입니다. 발표 내용을 순서대로 요약하면 아래와 같습니다.

[1] 기계번역의 목적을 명시하고 이 때 필요한 데이터인 Parallel Corpus를 소개하였다.

[2] 딥러닝이 기계번역에 적용되기 이전에 사용되던 (아직 사용되고 있는) Statistical machine translation 방식의 대략적인 작동원리를 소개하였다. 이 방법론에 쓰이는 세부적인 내용 중 Alighnment와 Reordering 작업을 조금 더 구체적으로 설명하였다. 이를 통해 기존의 방법론이 가지고 있는 문제점을 부각시켜 설명하였고, 딥러닝에 기반한 방법론은 이러한 문제점을 어느정도 해소할 수 있음을 소개하였다.

[3] 기계번역에 쓰일 수 있는 딥러닝 기반의 방법론 5가지를 소개하였다. 

[4] RNN의 성능을 대폭 향상시켜 준 게이트 유닛인 LSTM과 GRU중 후자를 소개하였다. GRU의 Forward-propagation과 Backward-propagation 과정을 자세히 살펴보았다.]]></content>
		<date><![CDATA[20170514180313]]></date>
		<update><![CDATA[20170526124208]]></update>
		<view><![CDATA[21]]></view>
		<comment><![CDATA[12]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[323]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[17. 05. 12 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170516131810]]></date>
		<update><![CDATA[20170516131810]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[324]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[[2017/05/19][CS224d seminar]Recursive Neural Networks for parsing]]></title>
		<content><![CDATA[[1] 이번 시간에는 문장을 계층 구조로 분석하는 파싱 기법으로 자주 쓰이는 Recursive Neural Networks에 대해 살펴 보았습니다. Recursive Neural Networks의 특수 케이스는 Recurrent Neural Networks입니다.
[2] Recursive Neural Networks는 응집성이 높은 입력값을 트리 구조의 형태로 만들어 학습을 진행합니다. 트리 구조를 만들 때는 Beam Search 알고리즘을 씁니다. Beam Search 알고리즘은 최고우선탐색(Best-First Search) 기법을 기본으로 하되 기억해야 하는 노드 수를 제한해 효율성을 높인 방법론입니다.
[3] Simple Recursive Neural Networks의 순전파와 역전파는 기존 Recurrent Neural Networks와 유사합니다. 다만 모든 parent node가 정답 레이블로부터 그래디언트를 전파받는다는 점이 다릅니다.
[4] 한국어 파싱 방법에 대해 살펴보았습니다. 명사, 동사, 관형사, 부사, 어미, 조사 등 통사원자 두 개가 결합해 구(phrase)를 형성합니다. 이때 중심 역할을 하는 요소를 핵(head)이라고 합니다. 
[5] 한국어의 일반적인 문장 형성 규칙은 다음과 같습니다.
1) 관형사 + 명사 = 명사구
2) 부사 + 동사 = 동사구
3) 명사구 + 격조사 = 격조사구
4) 명사구 + 동사구 = 동사구
5) 격조사구 + 동사구 = 동사구
6) 동사구 + 선문말어미 = 동사구
7) 동사구 + 문말어미 = 절(節)]]></content>
		<date><![CDATA[20170519170727]]></date>
		<update><![CDATA[20170526124653]]></update>
		<view><![CDATA[20]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[325]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 05. 19 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170519175151]]></date>
		<update><![CDATA[20170519175151]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[326]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 05. 26 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170526170355]]></date>
		<update><![CDATA[20170526170355]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[327]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[[2017/05/26][CS224d seminar] Advanced Recursive Neural Networks]]></title>
		<content><![CDATA[이번시간에는 Recursive Neural Network의 여러 Composition Function을 사용해서 다양한 NLP Task를 수행할 수 있음을 알아봤습니다.

[1] Paraphrase Detection - Standard Recursive Neural Network
 - 두 문장이 주어졌을 때 같은 의미를 갖는지 판단하는 작업을 수행할 수 있고, 이를 응용해서 중복해서 나오는 내용을 제거하여 요약 기능으로도 사용할 수 있음을 알아봤습니다.
 - Recursive Auto Encoder를 이용해서 단어를 나타내는 Vector를 구하고, 이 Vector를 이용하여 Similarity Matrix를 구할 수 있음을 알아 봤습니다.
 - Similarity Matrix를 처리하는 Dynamic Pooling Layer(Variable)에 대해 알아봤습니다.

 [2] Relation Classification - MV-RNN
  - Simple RNN이 하나의 W로 모든 단어의 복잡한 관계를 설명하는 데 어려움이 있기 때문에, 새로운 Matrix Vector RNN을 제안하였습니다.
  - Matrix Vector RNN 을 이용해서 단어 간 관계를 분류할 수 있음을 살펴 봤고, Relational Classification에도 사용할 수 있음을 살펴봤습니다.

 [3] Sentimental Analysis - Recursive Neural Tensor Network
  - Negation 문제를 해결하기 위해 새로운 Recursive Neural Tensor Network를 제안
  - 당시 Negation 을 고려할 수 있는 유일한 모델

 [4] Phrase Similarlity 
  - LSTM 과 Recursive NN을 조합하여 Semantic Analysis와 Phrase Similarlity 를 추정할 수 있음]]></content>
		<date><![CDATA[20170526220326]]></date>
		<update><![CDATA[20170602194614]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[328]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[[2017/06/02][CS224d seminar] Convolutional Neural Networks]]></title>
		<content><![CDATA[이번 시간에는 지난 시간에 제대로 다루지 못했던 Beam Search 기법과 이번 강의 주제인 Convolutional Neural Networks에 대해 살펴보았습니다.
[1] Recursive Neural Networks는 일반적으로 트리 구조를 탐색할 때 Greedy search를 합니다. 다만 Socher(2011)은
 탐색 범위를 주변 이웃으로만 제한해 계산 효율성을 높였습니다.
[2] Beam Search는 이웃이 2개뿐인 텍스트 파싱 데이터 처리에 적합한 기법입니다. 기억해야 하는 노드수를 줄여 효율성을 높였습니다.
[3] Socher(2011)에서는 현재 주어진 파라메터로 트리 구조를 만들고, 이 트리 구조로 산출된 스코어를 기준으로 파라메터를 업데이트합니다. 이를 반복해 가면서 학습을 해 나가는 구조입니다. 트리 구조는 관측치마다, Iteration마다 다릅니다.
[4] CNN은 다양한 n-gram 모델을 커버할 수 있고, 지역 구조를 전체적으로 파악하는 데 특화된 아키텍처입니다. 특히 Yoon Kim(2014)는 기존 이미지 처리 기법으로 제안된 CNN을 문서 분류에도 도입해 주목을 받았습니다.
[5] CNN의 순전파와 역전파 과정을 자세히 살펴보면 기존 뉴럴네트워크의 방식과 크게 다르지 않습니다. conv, pooling-layer의 그래디언트는 선형연산이거나 max/average 연산 같은 간단한 형태이기 때문에 얼마든지 전체 구조를 파악할 수 있습니다.
[6] 마지막으로 21세기 세종계획의 일환으로 만들어진 '한국어 명사 온톨로지'에 대해 살펴보았습니다. 국립국어원에 있는 자료를 엑셀 형태로 가공했는데, 연구실 전체에 공유하겠습니다.]]></content>
		<date><![CDATA[20170602172749]]></date>
		<update><![CDATA[20170702190628]]></update>
		<view><![CDATA[32]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[329]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 06. 01 회의록 (NC 미팅)]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170602183002]]></date>
		<update><![CDATA[20170602183002]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[330]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 06. 02 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170602183114]]></date>
		<update><![CDATA[20170602183114]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[331]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 06. 23 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170629124616]]></date>
		<update><![CDATA[20170629124616]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[332]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 06. 30 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170703091653]]></date>
		<update><![CDATA[20170703091653]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[334]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[[2017/06/30][paper seminar] AlexNet]]></title>
		<content><![CDATA[12년 ImageNet 대회에서 1등을 기록한 소위 AlexNet이라 불리는 network를 소개한 
[ImageNet Classification With Deep Convolutional Neural Networks]에 대하여 발표하였습니다.

기본적인 네트워크 구조와 input image가 어떤식으로 handling되는지를 보았습니다.
추가적으로 정확도를 높이기 위하여 저자들이 사용한 방식(PCA, ReLU, 
Local Response Normalization, Overlapping Pooling, Dropout)들도 확인하였습니다.

R-CNN  series와 GAN은 추후 추가적인 발표 후 후기를 작성하도록 하겠습니다.]]></content>
		<date><![CDATA[20170705100254]]></date>
		<update><![CDATA[20170710100204]]></update>
		<view><![CDATA[14]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[335]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[[2017/07/03][Paper seminar] ZF-net. Google-Lenet]]></title>
		<content><![CDATA[이번 세미나에서는 ZF-net과 Google Lenet을 살펴보면서 ZF-net의 핵심인 un-pooling과 deconvolution을, Lenet의 핵심인 Inception module에 대해 다뤄보았습니다.

[1] ZF-net
 ZF-net은 Alex net의 각 Layer가 어떤 역할을 수행하는지를 Un-pooling과 Deconvolution을 사용해 Visualization을 하는 것이 주 목적입니다. Unpooling은 Pooling의 반대 개념으로 Pooling을 진행할 때의 Maximum value의 위치를 Switch를 통해 기억해 놓고 이를 통해 up-sampling을 진행하는 방법입니다. Deconvolution은Convolution의 Backward 방법으로 Convolution을 Y=HX 형태로 표현했을 때 Matrix H의 Column이 서로 Orthonormal 하다는 가정을 전제하고 있습니다.  따라서 t(H)Y=X 형태로 Filter의 Weight를 별도의 학습 없이 그대로 사용하여 Feature map을 Reconstruction할 수 있습니다.

[2] Google Lenet
 Lenet은 모델을 깊게 쌓을 때 parameter의 수가 급속도로 증가하는 문제점을 해결하기 위해 inception module을 사용해 parameter의 수를 효과적으로 감소시킨 모델입니다. Inception module은 Feature map의 Feature는 Input image의 특정 지역에 해당하고, 각 Feature는 Filiter에 의해 하나의 Group으로 묶인다는 가정을 상정하고 있습니다. 
따라서 1X1, 3X3, 5X5 convolution, Max-pooling의 결과를 Concatenation하여 다음 Layer의 Input으로 전달합니다. 
하지만 이 경우도 모델이 깊어지면 parameter의 수가 부담되기 때문에 3X3. 5X5 convolution 직전에 1X1 convolution을 통해 Dimension을 축소시킨 뒤 3X3, 5X5 convolution을 진행합니다.]]></content>
		<date><![CDATA[20170705102108]]></date>
		<update><![CDATA[20170710100524]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[336]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[[2017/06/30][Deep RL seminar] Imitation Learning]]></title>
		<content><![CDATA[이번 시간에는 Deep RL 강좌의 인트로와 Imitation Learning에 대해 살펴보았습니다. 주요 내용은 다음과 같습니다.

[1] 강화학습은 Agent가 Environment를 관측한 결과인 Observation을 토대로 Reward가 최대가 되는 최적의 Action을 하면 Environment로부터 그에 해당하는 Reward를 받게 됩니다. Agent의 행동은 Reward를 최대화하는 과정에서 강화, 학습됩니다. Policy는 Observation과 Action을 연결해주는 함수입니다.
[2] 강화학습은 비용(보상)이 unknown probability distribution을 따른다는 점에서 Supervised Learning과 차이를 보이고, 현재 상태가 직전 상태에만 영향을 받는다는 점에서 Contextual bandits 기법과 다릅니다.
[3] 다만 강화학습 모델을 구축함에 있어 보상이 명확한 Task가 많지 않습니다. 이 때문에 제시된 기법이 Agent가 사람의 행동을 그대로 따라하도록 설계한 Imitation Learning입니다.
[4] Imitation Learning은 Agent의 예측(행동)이 Environment를 변화시키기 때문에 의도치 않은 방향으로 학습될 가능성이 큽니다. 이에 중간에 사람이 다시 개입해서 Agent의 행동을 수정하는 DAgger 기법이 제안됐습니다.
[5] Imitation Learning의 Application 또한 설명했습니다. 드론 자율 비행, 로봇 제어, 자율주행차 등 많은 응용 사례가 있습니다.]]></content>
		<date><![CDATA[20170705110857]]></date>
		<update><![CDATA[20170710100727]]></update>
		<view><![CDATA[17]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[337]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[17. 07. 07 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170707164107]]></date>
		<update><![CDATA[20170707164107]]></update>
		<view><![CDATA[5]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[338]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[25]]></member_uid>
		<member_display><![CDATA[정 재윤]]></member_display>
		<title><![CDATA[[2017/07/10][Paper seminar] VGGNet, ResNet]]></title>
		<content><![CDATA[이번 세미나에서는 VGGNet과 ResNet의 네트워크 구조를 살펴보았습니다.

[1] VGGNet
VGGNet은 망의 깊이가 깊어짐에 따라 어떤 변화가 있는지 비교하다가 완성된 네트워크입니다. 레이어를 다르게 설정하여 실험하는 대신 conv 크기를 3x3, 풀링을 2x2 stride 2로 통일하여 차이점을 관찰하였습니다. 마지막 단에는 4096, 4096, 1000의 FC layer를 공통적으로 설정하였습니다. 결론적으로 망이 깊어질수록 정확도가 증가한다는 결론을 내릴 수 있었습니다.


[2] ResNet
본격적으로 100개 이상의 층을 쌓으면서 인간보다 높은 이미지 분류 성능을 보여주는 네트워크입니다. Residual block이라는 새로운 개념을 도입하여 레이어를 통과 전 후의 잔차를 학습시켜 네트워크 최적화에 엄청난 성능 향상을 이뤄낼 수 있었습니다. Google Lenet과 마찬가지로 1x1 conv를 활용한 bottleneck 구조를 사용하여 파라미터 계산량을 줄였습니다. 
ResNet을 여러 layer들의 앙상블로 보는 관점의 논문과 채널 수를 늘리고 Dropout을 추가한 wide ResNet도 살펴보았습니다.]]></content>
		<date><![CDATA[20170714165152]]></date>
		<update><![CDATA[20170721135708]]></update>
		<view><![CDATA[23]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[339]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[[2017/07/17][Deep NLP] Introduction &amp; Word Representations]]></title>
		<content><![CDATA[이번 발표에서는 Deep NLP 강좌의 첫 번째와 두 번째 강의를 한 번에 묶어서 다루었습니다. 
첫째, 일 강에서는 기계학습의 기본 개념을 가상의 예제 상황을 통해 전달하였고, 두 번째로 딥러닝 모델을 NLP 과제에 적용하기 위한 시작점으로 흔히 분석의 기본 단위가 되는 단어를 임베딩하는 방법을 다루었습니다.

[1] 기계학습의 기본 개념
가상의 캐릭터들이 과일을 주고 받는 상황을 통해 기계학습의 기본 개념에 대해서 살펴봤습니다.
우리가 관심있는 대상을 숫자로 개념화하고, 함수를 이용해서 기능화하는 과정이 직관적으로 설명되었습니다.
이 함수를 어떻게 찾는 과정이 기계학습이고, 그 과정이 어떻게 되는지 또한 본 강의와 발표에서 다루었습니다.

[2] 워드 임베딩
본격적으로 NLP과제를 위한 딥러닝 모델을 학습하기에 앞서, 이 모델의 인풋이 되는 워드 임베딩에 대해서 살펴봤습니다. Count-based, Predivtive, 그리고 Task-based 세 가지 유형의 방법론에 대해서 다루었습니다. 이를 통해 NLP 모델의 인풋이 어떻게 구성되는지 파악할 수 있었습니다.

이 강까지는 본격적인 학습에 앞선 사전 준비 과정이었습니다. 기계 학습의 기초와 워드 임베딩을 다루는 방법을 복습함으로써 후속 강의들을 이해하는데 큰 도움이 되었으면 합니다.]]></content>
		<date><![CDATA[20170720115951]]></date>
		<update><![CDATA[20170729171408]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[340]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[17. 07. 18 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170721135114]]></date>
		<update><![CDATA[20170721135154]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[341]]></uid>
		<board_id><![CDATA[8]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[17. 07. 18 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170725165144]]></date>
		<update><![CDATA[20170725165144]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[342]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[[Deep NLP]Lecture 3 - Language Modelling and RNNs Part 1]]></title>
		<content><![CDATA[이번 세미나를 통해서 Language models을 알아보았습니다.
기본적으로 language model이란 text를 확률적으로 나타내는 모델이며 대부분 chain rule을 적용합니다. 하지만 chain rule을 사용하게 되면 문장의 마지막 단어를 유추하는 확률이 0으로 수렴하는 문제가 발생합니다.
이를 막기위해 markov chain을 사용하게 되며 이는 자연스레 bigram language model로 넘어가게 됩니다.
강좌에서 소개하고 있는 language model을 보면,
[1] Count based N-Gram Language Models
- 이는 unigram, bigram, trigram등을 사용하여 단어를 확률적으로 예측하는 모델입니다.
단일 n-gram만 사용하는 Back-Off방식의 단점을 보완하기 위하여
여러 n-gram을 사용하는 Interpolated Back-Off방식을 사용합니다.

[2] Neural N-Gram Language Models
-흔히 알고있는 neural network를 사용합니다. 
비선형성을 띄기 때문에 상기 방법론으로는 파악할 수 없었던 단어들 사이의 상관관계를 파악할 수 있지만 여전히 input에 n-gram방식을 사용하기때문에 모든 정보를 유지할 수 없는 단점을 가집니다.

[3] Recurrent Neural Network Language Models
-Neural N-Gram의 단점을 보완하는 방법론으로, 더이상 n-gram방식을 취하지 않습니다.
학습 방식으로는 흔히 생각할 수 있는 back-propagation을 하는 
Back Propagation Through Time(BPTT)방식과
forward와 backward의 네트워크를 잘라서 학습하는 
Truncated Back Propagation Through Time(TBPTT)방식이 있습니다.

NLP task의 가장 기본적인 개념과 architecture를 복습해보는 시간이었습니다.]]></content>
		<date><![CDATA[20170725170539]]></date>
		<update><![CDATA[20170804101644]]></update>
		<view><![CDATA[23]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[343]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[[Paper seminar] Generating Image Descriptions]]></title>
		<content><![CDATA[이번 세미나에서는 machine translation의 구조를 이미지의 caption을 generation하는데 적용한 모델에 대해 다뤄보았습니다.

[1] 본 논문에서는 기존의 hard coded 형태의 caption generation에서 탈피하고자 하였으며 이를 위해 모델이 데이터를 통해 스스로 학습해야 한다는 점에서 착안하였으며 caption의 각 문장들은 이미지의 특정 location에 해당될 것 이라는 가정에서 시작하였습니다. 

[2] 모델은 2개의 모델로 구성되며 caption의 문장을 이미지의 각 부분에 할당하는 alignment model과 이미지의 caption을 발생시키는 generation 모델로 구성되어 있습니다.

[3] Alignment model은 이미지의 caption을 weakly label로 활용하여 문장의 segment를 이미지의 location에 할당합니다.R-CNN을 통해 발견한 20개의 location과 문장의 단어를 h차원의 vector로 embedding 시킨 뒤 이들의 내적을 단어와 이미지 location의 score로 정의하여 파라미터를 학습합니다. 또한 Markov Random Field 방법을 사용하여 문장의 segment가 깔끔하게 이미지의 location에 할당될 수 있게 하였습니다.

[4] Generation model은 Rnn 계열의 machine translation의 구조를 그대로 사용하여 caption을 generate합니다. 이미지를 CNN network에 통과시켜 vector로 표현한 뒤 이를 RNN의 첫번째 hidden state의 input으로 사용합니다. Test 단계에서는 beam search 방법을 사용하여 문장을 generate 합니다.

[5] 본 논문의 한계점은 우선 각 location의 outside context를 반영하지 못했습니다. 또한 20개의 location이 각각 CNN 모델을 통과하여 vector로 표현된다는 점에서 비효율적인 면이 있습니다. 마지막으로 2개의 model을 각각 training 시키는 구조로 end-to-end learning이라고 볼 수 없습니다.]]></content>
		<date><![CDATA[20170725175016]]></date>
		<update><![CDATA[20170811154719]]></update>
		<view><![CDATA[30]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[344]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[[Deep NLP]Lecture 4 - Language Modelling and RNNs Part 2]]></title>
		<content><![CDATA[이번 세미나에서는 지난 시간에 이어 RNN Language Modelling에 대하여 살펴보았습니다.

1. Vanishing/ Exploding gradients
RNN model에서 Vanishing/ Exploding gradients problem이 발생하게 되는 원인을 살펴보았고 이에 대한 해결방안으로써 model structure을 바꾼 LSTM model을 살펴보았습니다.

2. LSTM/ Deep RNN
Basic RNN model의 vanishing gradient 문제를 해결하기 위해 만들어진 LSTM model을 살펴보았습니다. 곱하기로 update가 진행되어 vanishing gradient 문제가 발생하는 RNN의 문제점을 해결하기 위해 LSTM 에서는 cell state를 도입하여 더하기로 update를 진행합니다. 따라서 gradient가 잘 전파될 수 있습니다. 또한 계속 더하기가 진행되어 cell state가 커지는 것을 막기 위해 gate의 개념을 도입하여 적절한 balance를 유지하게 합니다. Deep RNN은 RNN model을 여러 층으로 쌓는 것으로 더 좋은 표현력을 얻을 수 있지만 recurrent flow가 증가하여 memory 사용량이 증가합니다. Depth를 time dimension으로 증가를 시키면 역시 더 좋은 표현력을 얻을 수 있지만 recurrent flow가 여전히 1개 이기 때문에 memory 사용량은 증가하지 않습니다. 처음에는 time dimension으로 증가시켰을 때 하나의 recurrent weight를 공유하면 표현력이 증가하지 않을 것이라고 생각했지만 세미나 도중 준홍형과 재선형의 피드백으로 하나의 recurrent weight를 공유해도 표현력의 증가 효과를 얻을 수 있다고 생각합니다.

3. Scaling large vocabularies
RNN language modelling에서 대부분의 computation이 발생하는 softmax 부분을 효율적으로 계산하기 위한 방법들을 살펴보았습니다. 소규모의 단어 sample을 통해 softmax probability를 추정하는 batch list 방법, 목적함수를 아예 바꾸어 단어가 noise인지 아닌지를 분류하는 binary classification문제로 전환한 Noise-Contrastive Estimation(NCE) 방법과 단어의 output을 factorize하여 computation cost를 줄인 방법을 살펴보았고 마지막으로 sub-word 수준의 language modeling에 대해 다루어보았습니다.

4. Regularization : Drop out
RNN에서 Recurrent connection 부분에 drop out이 적용될 수 없는 이유에 대해 살펴보았고 이에 대한 해결책으로서 하나의 random mask를 설정하고 이를 모든 recurrent connection에 적용하는 방법에 대해 살펴보았습니다.]]></content>
		<date><![CDATA[20170807103108]]></date>
		<update><![CDATA[20170811155019]]></update>
		<view><![CDATA[25]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[345]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[25]]></member_uid>
		<member_display><![CDATA[정 재윤]]></member_display>
		<title><![CDATA[[2017/08/04][Paper seminar] Spatial Transformer Network]]></title>
		<content><![CDATA[이번 논문발표 세미나에서는 Spatial Transformer Network와 Deformable Convolutional Network에 대하여 살펴보았습니다.

[1] Spatial Transformer Network
Spatial Transformer Network는 Spatially invariant하지 못하다는 CNN의 단점을 보완하기 위하여 나온 서브 네트워크입니다. 이미지 원본 또는 피쳐맵을  인풋으로 집어놓고 6개의 쎄타값을 통해 Grid generator로 가장 중요한 부분을 회전, 확대 등등을 시켜줍니다. 이렇게 변환된 이미지나 피쳐맵을 인터폴레이션 이후 다시 CNN에 넣어 효율적인 classification이 가능합니다.

[2] Deformable Convolutional Network
Spatial Transformer Network에서 영감을 받아 만들어진 새로운 구조입니다. Deformable ConvNet의 경우 convolution kernel이 항상 fix되어있다는 점을 바꾸기 위해 커널의 좌표도 같이 학습을 시킵니다. 학습이 완료된 네트워크는 이미지가 들어왔을 때 최대한 좌표들이 많이 이동하여 관련있는 부분을 더 많이 포착할 수 있게 됩니다. Deformable RoI Pooling 또한 기존 RoI Pooling에 좌표값만 추가하여 유동적인 크기를 pooling해올 수 있게 만들 수 있습니다.
 기존의 네트워크와는 다르게 커널이 보는 위치까지 학습시켜 더 좋은 성능을 보여주었습니다.]]></content>
		<date><![CDATA[20170809145220]]></date>
		<update><![CDATA[20170811231611]]></update>
		<view><![CDATA[17]]></view>
		<comment><![CDATA[7]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[346]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[18]]></member_uid>
		<member_display><![CDATA[모경현]]></member_display>
		<title><![CDATA[[Deep NLP]Lecture 5 - Deep Learning for Natural Language Processing Text Classification]]></title>
		<content><![CDATA[이번 세미나 시간에는 Text Classification을 위한 여러 approach를 살펴보았습니다.

1. Text Classification Method
전문가가 직접 문서를 보고 분류하는 방식, 규칙에 기반하여 문서를 분류하는 방식, 통계적 방식에 기반하여 문서를 분류하는 방식으로 나눌 수 있습니다. 각 방식의 장단점에 대해서도 알아보았습니다.

2. Document Representation과 classification stage
문서를 표현하는 방식으로 Bag of Words와 Hand-crafted feature, Learned Feature representation을 살펴보았고, classification을 위한 generative방식과 discriminative방식을 살펴보았습니다.

3. Naive Bayes Classifier
나이브 베이지안의 핵심 규칙인 베이즈 규칙을 살펴보고 조건부 확률이 구해지는 과정, 최적해를 구해가는 과정을 살펴보았습니다. 또한 장단점에 대해서도 알아보았습니다.

4. Logistic Regression Classifier
로지스틱 회귀분석의 구성요소를 살펴보고 베타값을 찾아가는 과정과 장단점에 대해서 알아보았습니다. 
또한 로지스틱 회귀가 최적해를 찾아가는지에 대한 것과 이진 분류식이 정확하게 작성되었는지에 관해 토론시간도 가졌습니다.

5. Deep Learning Classifier
Recurrent Neural Network, Recursive Neural Network, Bidirectional approach, Convolutional Neural Network를 사용한 Text Classification을 살펴보았습니다.]]></content>
		<date><![CDATA[20170811162155]]></date>
		<update><![CDATA[20171008023641]]></update>
		<view><![CDATA[16]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[347]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[[2017/08/11][Paper seminar] Vanilla GAN]]></title>
		<content><![CDATA[2014년에 나왔던 vanilla GAN에 대하여 발표하였습니다.

GAN은 기본적으로 discriminator와 generator가 반복적으로 학습이 되는데
optimal D(x)와 p_data와 p_g의 분포에 대한 논문의 식들을 증명해보는 시간을 가졌습니다.

최종적으로 optimal D(x)는 p_data(x) / (p_data(x) + P_g(x)) 가 됨을 알 수 있었으며
해당 D(x)를 사용할 때, cost function의 값은 p_data와 p_g가 일치할 때에 -log4로 나타남을 보았습니다.

또한 JSD를 사용함으로써 기존의 loss function들을 사용하여서는 만들 수 없는 sharp한 이미지를 생성한다고 
설명하였으나, 이와같은 이유로 mode collapse가 발생하지 않나 라는 생각도 하였습니다.]]></content>
		<date><![CDATA[20170811171126]]></date>
		<update><![CDATA[20171008022946]]></update>
		<view><![CDATA[12]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[348]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[[2017/08/18][Paper seminar] Deep learning and Data]]></title>
		<content><![CDATA[이번 세미나 시간에는 Deep Learning(CNN)과 Data(Noisy label or quantity of data)를 주제로 다음 4개의 논문을 다루었습니다.

1. Understanding Deep Learning Requires Rethinking Generalization
ImageNet, CIFAR10의 데이터를 이용하여 큰 CNN이 random label과 random input까지도 학습을 잘 한다는 것을 실험적으로 보이고 이를 기반으로 DNN은 전통적인 기계학습으로 설명할 수 없는 일반화 능력을 지닌다는 것을 다루었습니다.

2. Deep Nets Don’t Learn via Memorization
1번 논문에 반박을 하였으며, DNN은 단순히 외우기 때문에 좋은 성능을 내는 것은 아니라고 주장하였지만, 비약이 있는 것은 아닌가 싶습니다.

3. Deep Learning is Robust to Massive Label Noise
DNN은 Noisy data가 있어도 데이터의 양이 충분하다면 어느정도 일반화 성능을 보일 수 있다는 것을 실험적으로 보였습니다. 하지만 실험 데이터 생성의 과정에 따라 모든 데이터에 대하여 같은 결론에 도달할 지 의심이 됩니다.

4. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era
최근에 연구된 큰 CNN 모델들은 ImageNet은 충분히 표현할 능력이 있으며, 더 큰 데이터가 있을 때 더 좋은 성능을 보일 수 있다는 것을 보였습니다.]]></content>
		<date><![CDATA[20170818154230]]></date>
		<update><![CDATA[20171008024049]]></update>
		<view><![CDATA[18]]></view>
		<comment><![CDATA[5]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[349]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 08. 18. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170818162752]]></date>
		<update><![CDATA[20170818162752]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[352]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[[Deep NLP] Lecture 7 - Conditional Language Models]]></title>
		<content><![CDATA[Unconditional LMs
-단어들의 순서에 확률을 할당
-단어의 정보가 주어지면, 다음에 나올 단어의 확률을 모델링

Conditional LMs
-단어들의 순서에 확률을 할당 + 조건부 문맥(X)

Encoder- decoder model
x에서 특정 벡터로 맵핑시키는 함수를 구함 (source vector)
그 함수를 단어 순서를 표현하는 데 사용 (w)
-CNN: source vector(영어) -&gt; RNN(many to many) (프랑스어) (w): word translation
-LSTM: source vector(영어) -&gt; LSTM(many to many) (프랑스어) (w): word translation

beam search
각 분기 시점에서 beam size 만큼 가장 보상이 큰 대안을 선택]]></content>
		<date><![CDATA[20170823204241]]></date>
		<update><![CDATA[20171008024353]]></update>
		<view><![CDATA[23]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[353]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[[Deep NLP] Lecture 8 - Conditional Language modeling with attention]]></title>
		<content><![CDATA[이번 세미나에서는 attention model을 활용하여 language modeling을 하는 방법, 그 중에서도 machine translation에 대해 알아보았습니다.

[1] Matrix구성
machine translation 분야에서 기존 RNN 모델의 문제점은 문장의 많은 정보를 한정된 길이의 벡터에 넣게되면 정보를 보존하기 어렵다는 것입니다. 또한, 문장이 길어질 때, gradient를 학습시키는데 한계가 있습니다. 따라서 문장을 임베딩한 후 이를 matrix로 표현하면 위의 문제들을 해결할 수 있습니다. matrix를 구성하는 방식으로는 단순 concatenation, CNN 필터를 사용한 방식, bidirectional RNN을 사용한 방식 3가지를 소개하였습니다.

[2] Attention 사용
기존 RNN 모델에 각 시간대별 input에 weight를 부여하는 방법인 attention의 개념과 이 attention을 계산하는 방법에 대해 설명하였습니다. 또한, linear attention 뿐만아니라 non linear 방식을 소개하였습니다. attention을 사용하면 모델이 과거에 어떤부분에 집중하였는지 가시적으로 알 수 있고 메모리 문제를 해결한다는 장점이 있습니다.

[3] Hard attention &amp; Soft attention
이미지에 적용되는 attention을 간단히 소개하고 hard attention과 soft attention을 설명하였습니다. soft attention은 명확한 답을 알고 학습시켜 deterministic하고 gradient가 존재하지만  hard attention은  그렇지 않아 sampling하는 방식으로 진행하여 강화학습과 같은 맥락을 갖습니다.

마지막으로 attention model의 장점을 정리하면 사용할 경우 해석이 용이하고 gradient flow가 수월하며 capacity측면에서 더 효율적입니다.]]></content>
		<date><![CDATA[20170828154623]]></date>
		<update><![CDATA[20171008234754]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[354]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[[Paper seminar] Densely connected convolutional networks]]></title>
		<content><![CDATA[오늘은 CVPR 2017에서 Deep conv net의 break through를 이룬 Densely connected convolutional networks를 다루었습니다. 

1. concat이 어떤식으로 이루어지는지
2. growth rate가 어떠한 역할을 하며 k가 작아도 SOTA급을 찍는다는것을 보았습니다.
3. Bottleneck구조와 Compression 을 합한 DenseNet-BC가 어떠한 결과를 나오는지 보았습니다.
4. 논문에서 small dataset에서 overfitting이 나지 않는다는것을 critic해보았습니다.
5. Layer output에 대하여 feature reuse가 어떻게 일어나고 denseNet이 어떠한 장점을 가지는지 토론하였습니다.

앞으로 해당 모델로 몇가지 실험을 해 볼 생각이며 앞으로의 구조가 궁금해 집니다.

감사합니다.]]></content>
		<date><![CDATA[20170901163410]]></date>
		<update><![CDATA[20171008235032]]></update>
		<view><![CDATA[47]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[355]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 09. 01. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170901164053]]></date>
		<update><![CDATA[20170901164053]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[356]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[Deep NLP] Lecture 9 - Speech recognition]]></title>
		<content><![CDATA[이번 세미나 시간에는 deep oxford nlp의 speech recognition에 대한 부분을 발표했습니다.
음성인식 분야는 연구실에서 다루어본적이 없는 주제여서 강의에서 소개하는 기초적인 부분에 대한 설명을 많이 하였습니다. 

[1] 음성 인식에 대한 소개
음성인식은 사람의 음성을 음성인식 시스템에서 입력을 받은 후 음성 특징들(Raw waveform, MFCCs, LPC)로 변환을 하고 Gaussian Mixture Model와 Hidden Makov Model을 이용하여 음소가 나올 확률을 추출하고 순서를 배열하게 됩니다. 그 후 자연어처리 과정을 통해서 음성을 사람이 읽을 수 있는 자연어로 출력을 하게 됩니다. 이번 발표에서는 음성인식 시스템을 만드는 과정에서의 issue, waveform, 주파수, 특징 추출 방법, 음성인식의 역사에 대해서 이야기 하였습니다.

[2] 확률 기반의 음성인식
HMM과 GMM을 통해서 음성인식을 하는 방법, 딥러닝 방법을 이용하여 음성인식을 한 사례들에 대해서 알아보았습니다.]]></content>
		<date><![CDATA[20170902211906]]></date>
		<update><![CDATA[20171008235547]]></update>
		<view><![CDATA[30]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[357]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[17. 09. 07. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20170907145802]]></date>
		<update><![CDATA[20170907145802]]></update>
		<view><![CDATA[3]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[358]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[Paper seminar] Wasserstein GAN]]></title>
		<content><![CDATA[2017년 3월, FAIR의 논문인 WGAN을 발표했습니다.
vanilla GAN과 비교했을 때 어떤 부분이 좋은지 설명하기 위해 본 발표에서는 거리를 비중있게 다루었습니다.

[1] vanilla GAN 복습
WGAN을 알기 이전에, GAN을 복습하였습니다. 구조와 목적식에 대해서 간단히만 다루었습니다.

[2] distance
vanilla GAN에서는 KL, JS divergence를 이용하고, WGAN에서는 wasserstein을 이용하는데, 이 차이를 보기 위해 극단적인 예시를 들어서 확인했습니다. KL, JS, TV의 경우에는 분포가 같다 아니다 정도만을 확인할 수 있는 반면에, W의 경우에는 얼마나 떨어졌는지까지 확인할 수 있었습니다. 

[3] wasserstein GAN
이를 통해 GAN의 loss로써 기존 divergence보다 더 적합하다고 판단하여 적용하였으며, 이는 뒤에서 설명했듯이 디버깅에도 사용 가능함을 확인했습니다. 또한 loss와 사진의 질이 직접적으로 관련이 있어 보이는 것을 확인할 수 있었습니다. 반면에 JS의 경우에는 그렇지 못함도 추가로 확인했습니다. 여기에서 여러 개념들이 나오는데, 작은 부분까지 최대한 다루고자 했습니다. 학습에서는 RMSProb과 적은 learning rate가 좋음을 밝히고 있었으며, 그 이유를 간단히 설명했습니다.

[4] result
결과로는 역시 WGAN이 기존 vanilla GAN보다 더 우수함을 보이는데 그쳤습니다. 대부분의 GAN 논문이 그렇듯이, 정량적 지표가 없기에 어떤것이 좀 더 선명하다, 창의적이다 등을 정성적으로 평가하는데, 그런 부분은 조금 아쉬움이 남습니다.]]></content>
		<date><![CDATA[20170919144702]]></date>
		<update><![CDATA[20171008235812]]></update>
		<view><![CDATA[19]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[359]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[[Deep NLP] Lecture 10 - Text to Speech]]></title>
		<content><![CDATA[주어진 텍스트로부터 음성을 학습하는 Text to Speech(TTS)에 대하여 소개하였습니다.

[1] TTS 소개
TTS라는 분야의 문제를 정의하고 성능지표 산출 방식과 전통적인 방법을 소개하였습니다.

Rule-baed로 모음의 주요 성분에 중점을 두어 합성하는 formant synthesis와 
Sample-based로 텍스트와 음성을 매칭하고 스무딩 과정을 거치는 concatenative synthesis,
Model-based로 모델 기반의 합성을 다루었습니다.

[2] Probabilistic formulation
TTS를 확률로서 문제를 정의했습니다. 과거에는 Step-by-Step으로 진행했던 optimization 방식이 점점더 end-to-end로 발전하는 방향을 논의했습니다.

[3] Model-based
HMM 기반으로 음성을 생성했던 과거로부터 신경망 기반으로 넘어가는 트렌드에 대해서 다루었습니다.]]></content>
		<date><![CDATA[20170919171048]]></date>
		<update><![CDATA[20171009000708]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[8]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[360]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[[Deep NLP] Lecture 11 - Question Answering]]></title>
		<content><![CDATA[이번 세미나에서는 Question Answering의 각 분야인 Reading comprehension, Answer sentence selection, Visual Question Answering에 대해 다루어 보았습니다.

[1] Reading comprehension
Reading comprehension은 주어진 context를 읽고 Question에 대한 정답을 제공하는 방법입니다. 따라서 context, question, answer의 tuple이 하나의 데이터 셋으로 구성됩니다. 이러한 데이터를 만드는것은 매우 힘든 일이기 때문에 논문의 저자들은 대용어 시스템을 이용하여 context와 context의 summary를 사용하여 보다 쉽게 학습 데이터를 만들었습니다. 또한 연구의 목적이 common knowledge를 학습하는 것이 아니기 때문에 임의의 entity를 사용하였습니다. 모델은 attention 기반의 RNN 계열 모델이 주를 이루었으며 실험 결과 attention이 reading comprehension에 있어서 상당히 중요한 역할을 한다는 것을 보였습니다.

[2] Answer sentence selection
Answer sentence selection은 reading comprehension과 비슷하지만 정답을 제공하는 것이 아니라 정답을 이끌어낼 수 있는 문장을 제공하는 방법입니다. 따라서 실제로는 multi classification 문제이지만 논문의 저자들은 데이터를 변형하여 이를 binary classification으로 바꾸어 보다 쉽게 문제를 해결하였습니다.

[3] Visual question answering
Visual question answering은 이미지와 질문이 주어졌을 때 이에 대한 답을 해주는 방법입니다. Open source로 제공되는 데이터가 없기에 저자들은 이를 직접 수집하였고, 수집한 데이터의 당위성을 증명하기 위해 여러가지 설문 조사와 실험을 진행하였습니다. 모델은 정형화된 방법으로 이미지는 VGG net, 질문은 RNN 계열의 network를 사용하였고, 정답을 맞추는 것은 classification 문제로 해결하였습니다.]]></content>
		<date><![CDATA[20170921131616]]></date>
		<update><![CDATA[20171009002239]]></update>
		<view><![CDATA[45]]></view>
		<comment><![CDATA[10]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[361]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[[Deep NLP] Lecture 12 - Memory]]></title>
		<content><![CDATA[금일 세미나에서는 Attention을 좀 더 유동적인 관점으로 바라본 Memory에 대해 다루었습니다.

simple RNN 기반으로 기계번역 문제등을 풀려고 접근할 때 발생하는 문제로 bottleneck과 long seq.에 대해 의미를 완전히 담지 못하는 한계점을 먼저 언급한 뒤, 이를 풀기 위해 Memory를 제안하는 흐름으로 강의가 진행되었습니다.
총 3단계로 발전하는 모형을 살펴보았으며, deepmind에서 2016년에 나온 논문이고, 아직 발전하는 단계이므로 세미나시간에 준홍이형이 얘기한 것처럼 많은 변형을 시도할 수 있지 않을까 생각이 듭니다.

[1] Attention: ROM
Memory라는 개념을 cell안에 넣어서 controller와 분리시켜 attention을 해보려는 시도를 한 방법론입니다. Memory를 먼저 참조하는 early fusion과 나중에 참조하는 late fusion이 있으며, 이에 관련한 수식을 살펴봤습니다. 개념과 수식, 도식화를 통해 해당 방법론이 가진 기저가되는 아이디어를 살펴봤습니다. 강의에서 예로 들고있는 recognizing textual entailment 논문을 살펴보았는데, 본 논문은 premise와 hypothesis간의 관계를 contradiction, neutral, entailment로 나누는 문제였습니다. encoding단의 정보와 제일 마지막 h_N만을 엮어서 접근하는 기존 attention 방식과, 점진적으로 M, alpha, r을 만들어가며 정보를 엮어가는 word-by-word를 설명했습니다. 추가로 bi-directional LSTM을 섞은 것까지 총 4가지 모형을 보았는데, 가장 성능이 좋은 경우는 one way로 LSTM을 적용한 word-by-word 모형이었습니다.

[2] Register machine : RAM
ROM모형의 경우 Memory는 오로지 encoding시에 저장되며, 이후 decoding에서 풀어나갈때는 읽는 연산만을 합니다. Decoding 단계에서 controller가 Memory에 쌓인 정보에 관여하는 모형이 RAM 모형입니다. k_read, k_wirte, v등의 정보를 ROM에 추가로 산출하여, 다음번 step에서 고려해주는 방법론이었습니다.

[3] Stack: Neural PDAs
마지막으로 설명한 모형은 컴퓨터과학 자료구조에서 사용되고 있는 stack, queue, deque를 neural structure에 접목시킨 모형이었습니다. 이 모형의 관건은 미분가능성에 있었으며, 이는 continuous하게 operation(pop, push, read)를 정의함으로써 가능하게 만들었습니다. 추가로, stack과 함께 strength라는 벡터를 만들어서 접근 했습니다. 본 방법론은 수식이 너무 복잡하게 되어있어 개념 위주로 설명하기 위해 노력했으며, 각각의 operation이 작동하는 과정을 도식화 하여 설명하였습니다. 학습은 gradient를 기반으로 별다른 trick이 없어 생략하였습니다.]]></content>
		<date><![CDATA[20171010234248]]></date>
		<update><![CDATA[20171021102722]]></update>
		<view><![CDATA[38]]></view>
		<comment><![CDATA[11]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[362]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[[Paper Seminar]  A simple neural network module for relational reasoning]]></title>
		<content><![CDATA[[Paper Seminar]  A simple neural network module for relational reasoning
  본 세미나 시간에는 관계형 추론(relational reasoning) 문제를 relational network를 통해서 해결한 Deep Mind 에서 2017년 6월에 발표한 A simple neural network module for relational reasoning 이라는 논문을 다루어 보았습니다.
 기존 CNNs나 MLPs 등은 Object를 인식하는 데는 뛰어나지만, Object 간의 관계에 대해서는 추론하지 못하는 맹점이 있었습니다. 본 논문에서는 Relational Network 을 이용하여 관계에 대한 추론 능력을 model design framework을 통해서 구현하였으며, 이는 간단한 object set의 simple function(MLPs)과 element-wise sum의 결합으로 CNN, LSTM, both 모두 간단한 Plug In만으로 working 합니다.  이러한 구조를 통해서 RNs는 관계형 추론이 가능해지며 데이터 를 효율적으로 학습에 반영할수 있으며, 순서에 invariant한 학습구조를 가질 수있습니다. 무엇보다도 사람 그이상의 관계형 추론이 가능해 진것이 매우 인상적이었습니다.]]></content>
		<date><![CDATA[20171017165012]]></date>
		<update><![CDATA[20171116201358]]></update>
		<view><![CDATA[25]]></view>
		<comment><![CDATA[9]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/1/201710/59e5b634c2edb1228515.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[Relatioanl Network.png]]></thumbnail_name>
		<category1><![CDATA[세미나]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[363]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[[Paper Seminar] : Get To The Point: Summarization with Pointer-Generator Networks]]></title>
		<content><![CDATA[본 논문은 Text로부터 주요 핵심 내용을 1~2문장길이로 요약해주는 논문입니다. Google Research 팀에서 ACL2017에 발표한 연구로서, 기존 Abstractive Text summarization에서 문제시 되던 부정확한 재현성과 반복적인 생성 문제를 각각 ‘Point-Generator Network’와 ’Coverage Mechanism’을 통해 해결하였습니다
딥러닝이전의 문서요약의 경우, 규칙 위주의(Rule Based) 혹은 통계적기반(Statistical based) 방법론이 주를 이루었지만 상용화할 수 있는 부분이 제한적이고 그 성능 또한 사람에 의한 문서요약보다 월등히 떨어졌습니다. 하지만, 현재 RNN(Recurrent Neural Network), CNN(Convolution Neural Network)과 같은 다양한 딥러닝기법을 활용하게 되면서 Text summarization(문서요약)은 비약적으로 그 성능을 향상 시킬 수 있었습니다.
우선 Text summarization(문서요약) 이란, 주어진 문서로부터 특정 사용자나 작업에 적합한 축약된 형태의 문서로 재생성하는 작업을 말합니다. 이를 통해서 복잡도를 줄이면서 필요한 정보 유지하는 것이 주목적이 되겠습니다. 일반적으로 Text summarization(문서요약)의 접근법은 크게 2가지 방법으로 나누어 볼 수가 있겠습니다. 1)첫번째는 ‘Extractive approach’로 말그대로 본문에서 의미있는 부분을 선택하고 추출하여 이를 재정렬하여 요약하는 접근법이 되겠습니다. 일종의 highlighter라고 생각하면 좋을 것 같습니다. 2)두번째는 ‘Abstractive approach’로 RNN과 같은 자연어생성기술을 활용하여 기존 Text문장을 토대로 창의적인 새로운 문장을 작성하는 방법이 되겠습니다.
지금 제가 소개해드리는 ‘Get To The Point: Summarization with Pointer-Generator Networks’ 은 기본적으로는 두번째 abstractive한 접근법을 취하면서, 필요 시에는 앞선 Extractive한 접근을 hybrid로 동시에 사용하여 좀더 자연스럽고 함축적인 문서 요약을 가능하게 합니다. 
본 논문methods는 크게 세가지 Parts로 구성됩니다.

	Sequence-to-sequence attentional model
Nallapati et al. (2016)에 발표한 ‘Abstractive text summarization using sequence-to-sequence RNNs and beyond.’ 논문의 attention 메커니즘을 통한 seq2seq RNN model을 baseline 으로 잡고 있습니다. 하지만 기존 부정확한 재현성과 반본적 단어의 재활용 등이 문제점으로 지적됩니다.
자세하게는 아래 Figure 2를 활용하여 특징을 설명하도록 하겠습니다.
 
attentional seq2seq RNN model(See, Liu, &amp; Manning, 2017) 은 크게 4가지 모듈로 구성되어 있습니다. 
	Encoder RNN: 원문 Text로부터 word2Vec으로 표현된 단어들을 word-by-word단위로 읽어드리는 module입니다. 일반적으로 encoder는 양방향의 순서를 고려한 bidirectional RNN을 활용합니다.
	Decoder RNN: 요약을 구성하는 단어들의 Sequence 형태로 output을 반환하게 됩니다. Decoder RNN은 Encoder와 다르게 한 방향의 directional RNN을 활용하여, 이전 요약 단계의 단어들을 input으로서 받게 됩니다. 원문 text로부터 부분적인 문서 요약이 반환된다고 생각하시면 될 것 같습니다.
	Attention Distribution 과 Context Vector: 원문 text 로부터 다음 단어를 생성할 때, 원문 text의 단어에 대한 확률인 attention distribution은 Encoder RNN과 Decoder RNN의 hidden state를 input으로 하여 아래와 같은 함수를 통해서 표현됩니다. 이는 직관적으로 해당 네트워크가 다음 단어를 생성시 원문 text 중에 어떠한 단어를 주의 깊게 봐야 할지를 표현해주는 것이라 생각하면 되겠습니다. 
식 (1), (2)에서의 V^T,W_h, W_s,b_attn 등은 네트워크의 학습과정을 통해서 학습하는 파라미터 입니다. 이러한 attention distribution을 활용하여 encoder 의 hidden state와 가중합을 통해서 Context vector를 생성합니다. 이는 decoder가 해당 원문으로부터 어떠한 단어를 읽어오는지를 표현해준다고 생각하면 되겠습니다.
	Vocabulary Distribution: 최종적으로 context vector와 decoder hidden state의 output을 결합하여 2개의 선형 결합 layer를 통해서 Vocabulary distribution을 표현하게 됩니다. 이는 원문 text가 아닌 전체 단어(일반적으로 그 사이즈를 제한)에 대한 확률을 표현하게 됩니다. 이를 통해서 최종적으로 문서 요약을 통해 생성될 단어들을 순서대로 표현하게 됩니다. 
 
	Pointer-generator network
본 논문에서 제안하는 ‘Pointer-generator network’는 baseline model로 언급한 상기 attentional seq2seq RNN model(See, Liu, &amp; Manning, 2017)과 Vinyals et al.(2015)에서 제안한 pointer network(Vinyals, Fortunato, &amp; Jaitly, 2015)의 hybrid network라고 볼 수가 있습니다. 이러한 hybrid network로서의 ‘Pointer-generator network’를 통해서 summarization단계에서 기존 Text 본문에서의 단어들을 (1-P_gen)의 확률로 복사하여 재활용하거나 새로운 novel 단어들을 P_gen확률로 생성할 수가 있습니다. 아래 그림을 통해서 ‘Pointer-generator network’의 diagram을 확인 하 실수 있습니다.

 

직관적으로 ‘Pointer-generator network’를 역할을 생각해보면 1.기존 baseline 모델의 출력값 (Vocabulary Distribution)과 2. 원문 text로부터의 attention distribution사이에서 ‘Pointer-generator network’기반의 학습된 생성 확률(P_gen)을 반영하여 새로운 단어를 생성하여 문서를 요약하거나 기존 원문 text를 재현하여 요약할지를 학습하여 결정하게 되는 것입니다. 예를 들면, 생성 확률이 0에 가까우면 기존 원문단어로부터 재현된 단어들을 통해 문서 요약이 되고, 생성 확률이 1에 가까우면 encoder-decoder기반의 생성 단어들을 통해서 문서 요약이 수행되게 되는 것입니다.

	Coverage mechanism
기존 attentional seq2seq RNN model(See, Liu, &amp; Manning, 2017)에서 문제시 되는 text summarization단계에서의 반복적 단어 재활용을 해결하기 위해서 본 연구에서는 coverage vector를 통해서 일종의 penalty term을 Loss에 반영하여 이를 방지하고자 합니다. 이 아이디어는 현재까지 사용되었던 단어의 attention distrubution의 누적 값을 coverage vector라 정의하고, 이를 ‘covLos’에 반영하여 summarization단계에서 같은 위치에 반복적으로 등장하는 것을 방지하는 역할을 하게 됩니다. 이러한 ‘covLos’를 반영하여 기존 attention distribution와 학습을 위한 Loss는 기존 방법에서 아래와 같이 변화 됩니다.
 
아래는 기존 baseline-model(Nallapati et al. ,2016) 과 본 논문의 ‘Summarization with Pointer-Generator Networks’의 문서 요약 예시입니다.
   
두번째로 제시된 baseline(seq2seq+attention)의 경우에는 ’UNK’으로 표시된 비 이상적인 단어들로 인해 부정확한 요약 결과가 생성되는 반면에, 해당 방법론을 사용한 경우 필요시 원문 text의 어휘들을 재현하면서도, 함축적인 새로운 어휘들을 통해서 문서 요약이 잘 수행된 것을 확인 할 수 있습니다. 이는 본 접근법이 ‘abstractive’와  ‘extractive’의 장점을 잘 반영한 문서 요약이 가능함을 알 수 있습니다.

&lt;한계점 및 시사점&gt;
본 논문의 경우 원문 text와 문서 요약된 text가 쌍으로 존재할 경우 수행이 가능한 접근법입니다. 하지만 우리 한글의 경우, 위와 같은 pair한 학습데이터를 구성하기가 매우 어려워 정확한 학습이 어려울 것이라 판단됩니다. 따라서 뉴스의 첫문단을 문서의 요약 문단으로 정의하고 학습하는 방법도 하나의 대안이 될 수 있을 것이라 생각됩니다.
또한 영문과 달리 조사 및 어미 등의 결합이 존재하는 있는 한글의 특성상 자연스러운 문장 요약이 매우 어렵다고 판단됩니다. 이러한 문제점을 어떻게 잘 반영하여 함축적이고 가독성있는 문서 요약을 가능토록 하는 시도는 앞으로 재미있는 연구 주제가 될 수 있을 것 같습니다.]]></content>
		<date><![CDATA[20171017165147]]></date>
		<update><![CDATA[20171116201650]]></update>
		<view><![CDATA[25]]></view>
		<comment><![CDATA[4]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[364]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 김준홍]]></title>
		<content><![CDATA[&lt;발표후기&gt;

 이번 산업공학회에서는 2가지 연구에 대하여 제출하였다.

====== 연구 발표 후기 ====== 
--------------------------------------------------------------------------------------
[1] 김준홍, 김형석, 박재선, 모경현. 강필성. (2017) 웨이퍼 Bin Map 데이터 기반의 불량 탐지 및 시각화를 위한 Bin2Vec 기반의 합성곱 신경망 분류기 ,대한산업공학회 추계학술대회, KAIST, 대전, 11월 4일.

    [간단 내용]
[1]번의 경우는 삼성전자에서 현재 2차산학과제까지 완료 되었는데 1차에서 행했던 방식들을 발표하였다. 발표는 본인 대신 같은 연구 진행하였던 재선이(석사과정 박재선)가 잘 해주었다. 해당 연구는 Wafer Fail BIN을  명목에서 연속형으로 바꾼뒤 RGB mapping을 하여 BIN의 정량적인 similarity와 color mapping에 대한것과 Wafer 제품 별로 양/불량 단순 binary classification을 한 것이었다. 성능적으로는 반복 30회 test data AUROC기준 거의 0.99까지 나오는 상황이여서 해당 문제가 단순 WBM으로도 양 불량 pattern을 할 수 있다는것을 입증하였다.

    [질문]
질문으로는 "불량 Pattern을 보고 싶을때는 어떻게 해야하는가?" 였는데 일단 class를 불량으로 넣었지만 conv자체가 spatial정보를 가지고 있기 때문에 가장 간단하게는 flatten하기전에 conv activation map을 t-SNE로 내린다음 clustering등으로 grouping하여 BIN map을 시각화 하면 패턴을 볼 수 있을것이라 생각한다.


--------------------------------------------------------------------------------------
[2] 김준홍, 강필성. (2017) 자유로운 문자열 기반의 사용자 인증을 위한 LSTM 기반 이상치 탐지 기법 ,대한산업공학회 추계학술대회, KAIST, 대전, 11월 4일.

    [간단 내용]
개인적으로 자유로운 문자열 기반의 키스트로크 다이나믹스 사용자 인증 문제는 짧지 않게 진행해온 문제이다. 이전 연구(Junhong Kim, Haedong Kim, and Pilsung Kang*. (2017+). Keystroke dynamics-based user authentication using freely typed text based on user-adaptive feature extraction and novelty detection, Applied Soft Computing, Accepted for Publication.) 에서는 One-class classification의 feature variant를 주는 방식이었다.
기존연구들을 보면서 time series와 di-graph의 계속적인 sub-distribution이 필요하다는 생각을 RNN으로 formulation하여 문제를 풀었다. 해당 연구의 performance는 One-class classification처럼 imposter user dataset없이 기존 연구 되었던 알고리즘 대비 최대 10배 이상 성능이 우수하였으며, 실 상황에서 적용하기 위해 아주 짧게도 실험하여 실제 우리가 채팅창이나 메신져에서도 활용 될 수 있다는 것을 확인하였다.

    [질문]
첫번째 질문으로는 "현재 test dataset을 짧게 하고 train을 길게 하였는데 반대로는 실험을 해보지 않았는가?"였다. 해당 질문은 감사하였다. 답변으로는 "train dataset을 짧게 500에서 길게 10,000까지 한 이유는 자기 데이터는 계속적으로 모을수 있기 때문이며 실제 제품 적용상에서는 test dataset이 10 50 100처럼 아주 짧게 해야지 해당 사항이 실상황
 적용으로써 의미가 있다. 그렇게 때문에 반대 상황은 실험하지 않았다."라고 답변하였다.

두 번째 질문으로는 "해당 방법론이 time axis에 따른 key sequence가 거꾸로 오게 되면 distribution이 변화하는 알고리즘인가?"였다. "keystroke 자체가 이미 di-graph에서 반대 key set에 대하여 표현하고 있지만 제안한 방식은 di-graph를 넘어서 unigraph에서 사용하는 train의 RNN sequence 길이에 따라서 n-graph까지 쓰기 때문에 진행방향이 반대이면 key에 대하여 전혀 다른 time distribution이 나온다"고 답하였다.

&lt;청취후기&gt;

개인적으로 같은 연구실 이외에 청취한 연구는 아래와 같다.
[1] 딥러닝 기반 강화학습을 이용한 토목 공정 계획	지민기, 우성철, 정요한, 박진규, 문일철(KAIST 산업및시스템공학과)
[2] Deep Q-network를 통한 미래 전장 무인전투기 임무 수행 알고리즘	이경택, 김상진, 김창욱(연세대학교 산업공학과)
[3] Multi-task ConvLSTM 네트워크를 통한 풍력 터빈 예측	우성철, 박진규(KAIST 산업및시스템공학과)
강화 학습을 통한 토목 공정 최적화	우성철, 정요한, 지민기, 문일철, 박진규(KAIST 산업및시스템공학과)
[4] AR 기반 개방형 개인화 제조서비스 모니터링 및 협업지원	배희철, 손지연, 강현철, 이은서, 한효녕, 김민기(한국전자통신연구원)
[5] 머신러닝 기반 박막 블리스크 가공 공정 이상탐지	권순찬, 김동일, 박경희, 윤주성(한국생산기술연구원), 최재윤(한화테크윈)
[6] 제조 공정에서의 데이터 기반 품질문제 원인인자 분석	김동일, 구정인, 김보현(한국생산기술연구원), 이상현, 최성수, 이준용, 강정태((주)유라)
[7] Word2vec 기반의 의미적 유사도를 고려한 웹사이트 키워드 선택기법	이동훈, 김관호(인천대학교 산업경영공학과)

전체적인 관점에서 보면 산업공학회에서 딥러닝 사용 연구가 늘고 있다. 올해는 몇가지 간단한 Environment와 Action을 설정하여 reinforcement learning을 실제 필요한 분야에서 시작하는 연구가 몇개가 있어서 재미있게 보았고, 풍력 등과 같은 conv input data만 있으면 task를 만들어서 몇가지 적용해본 사례가 있어서 재미있었다. 또한, 현업에 계신분들은 아직도 black box 모델에 대한 거부감이 심한부분이 있다며 단변량 분석을 하는 상황도 있었다. 

개인적으로는 산업공학과에서만 아니라 전기과와도 같이 협업을 하고도 있고, 개인적으로 OCR을 하기 위하여 비전 분야의 공모전도 참가하여 운이 좋게 수상하였다. 산업공학학도로써, 어떤 분야에서 분석들이 이루어 지고 있는지 여러 연구실의 결과물들을 간단하게나마 보면서 본인이 앞으로 어떤식으로 연구 방향을 좀 더 확장 시킬것인가에 대한 도움이 되었다고 생각한다.

읽어주셔서 감사합니다.
- 김준홍 올림 -]]></content>
		<date><![CDATA[20171108143539]]></date>
		<update><![CDATA[20171108160347]]></update>
		<view><![CDATA[667]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201711/5a0297c585fe21939331.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[DSBA_Logo.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[365]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[2017 INFORMS Annual Meeting - 김동화]]></title>
		<content><![CDATA[안녕하세요.
DSBA 연구실 박사과정 김동화입니다.
이번 년도 처음으로 INFORMS라는 OR 중심의 학회를 다녀오게 되었습니다. 

[발표후기]
제가 이번에 발표한 주제는 Multi-co-training for document classification 이며 이 방법론은 문서분류에 있어서 라벨이 부족할 경우가 다 반사인데 이러한 문제점을 효율적으로 풀기 위해 문서 재 표현기법들을 앙상블하여 라벨이 적더라도 충분히 안정적으로 학습되는 방법을 발표하고 왔습니다. 다소 학회의 주된 흐름에서 벗어난 것 같았지만 적지 않은 사람들 앞에서 발표를 하여서 뜻깊은 시간이었습니다.

[참여 후기]
저희 연구실은 Deep Learning과 Machine Learning을 기반으로 연구를 계속 해왔기 때문에 이번 학회에서 최대한 관련 있는 필드들을 찾아서 참석하려고 하였습니다.
가장 관심 있었던 세션 내용들은 다음과 같습니다.

["Learning Combinatorial Optimization Algorithms over Graphs"]
전반적인 흐름으로 해석하자면 네트워크 최적화문제를 많이 푸는 것 같습니다. 예를 들어, Minimum Vertex Cover 문제 같은 경우 어떤 노드를 선택했을 때 모든 엣지들을 cover할 수 있는지에 대한 문제인데 여러해 중에서 최소의 노드로 그 목적을 달성할 수 있는 최적화 문제라고 할 수 있습니다. 그래서 이러한 문제를 풀기 위해 두가지 개념을 적용했는데 강화학습의 Q-learning과 CNN 개념에 모티브가 된 같은  Relu함수와 pooling(단순 합)을 적용하여 그래프 임베딩을 구현 하였습니다. 이러한 접근방법의 목적은 데이터가 조금 바꿔더라도 같은 최적화문제를 반복적으로 풀게 되는데 이러한 비효율적인 문제를 해결하기 위해 이러한 방법을 사용하였습니다. 이러한 방법은 기존의 pointer-network 보다 좋은 성능을 가지고 있다고 밝히고 있습니다. 이러한 분야도 조만간 여유가 생긴다면 연구해보고 싶은 분야입니다. 같은 목적이 아니라 텍스트마이닝이나 이미지 분류에서도 충분히 적용 될수 있는 접근법인것 같습니다. 혹시 이 방법에 대해서 관심있는 사람들을 위해 아래의 Github의 주소를 첨부합니다.
 https://github.com/Hanjun-Dai/graph_comb_opt

[Autoregressive Forests for Multivariate Time-Series Modeling]
이 연구는 시계열 기반의 연구로 자기상관있는 데이터를 어떻게 풀어나가는 방법을 제시하고 있습니다. 핵심 내용은 멀티 클래스 문제일 경우 각 클래스별로 데이터를 분리하고 자기상관의 데이터 값(y)를 반응변수로 생각하고, time order나 lagged features를 설명변수로 트리구조로 예측하는 randomforest 기법을 사용였습니다. 다시 말해서 각 클래스별로 트리를 구성해 자기자신의 값을 예측하는 모델을 만들고 그곳에서 나온 error들을 다시 테이블형태로 변환하여 KNN이나 randomforest 같은 머신러닝기법을 적용한 접근법이었습니다.

연구실 내에만 있다보니 시야가 좁았는데 시야를 넓히고 온 것 같아 의미있는 시간이었습니다.]]></content>
		<date><![CDATA[20171109161924]]></date>
		<update><![CDATA[20171110160139]]></update>
		<view><![CDATA[589]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201711/5a054509633103629028.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[cropped-Houston_Logo.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[366]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[25]]></member_uid>
		<member_display><![CDATA[정 재윤]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 정재윤]]></title>
		<content><![CDATA[&lt;연구 발표 후기&gt;

이른 아침 7시 기차를 타고 대전을 가는 길은 새벽 기상으로 인한 쏟아지는 졸음에도 불구하고 기대되는 마음을 품고 내려가는 길이었습니다. 공식적인 자리에서 첫 발표를 한다는 것과 다른 학교에서 어떤 흥미로운 주제로 발표를 할지 기대를 품고 카이스트에 도착했습니다.
마지막 세션으로 발표 시간이 정해져서 그 전까지 다른 세션에 참여하며 여러 주제에 관련된 발표들을 많이 들었습니다. 제가 발표한 내용은 다음과 같습니다.

[1] 이기창, 정재윤, 서승완, 김창엽, 강필성. (2017). 합성곱 신경망을 사용한 약지도학습 기반의 감성분류 및 단어 어텐션, 대한산업공학회 추계학술대회, KAIST, 대전, 11월 4일. 

- 발표 내용 -

1학기 비정형 데이터 분석 수업 프로젝트로 진행했던 CNN을 이용한 약지도 학습 기반의 문장 분류 및 단어 어텐션에 대해 발표하였습니다. 이 연구는 이미지에서 쓰이는 클래스 액티베이션 매핑(CAM : Class Activation Mapping)방법론과 텍스트 분류에서 CNN이 좋은 성능을 내고 있다는 것에 기반하여 아이디어를 냈습니다. 네트워크 아키텍쳐는 각 문장을 단어 단위로 잘라서 CNN layer를 통과시킨 뒤 feature map에다 global average pooling을 시행하고 문장 범주에 FC layer를 걸어주는 구조입니다. 여기서 FC layer의 가중치들과 feature map을 가중합 시키면 CNN이 문장의 어떤 부분을 분류에 가장 중요한 핵심이라고 판단하는지 볼 수 있습니다. 왓챠 한글 영화리뷰 데이터와 IMDB 영문 영화리뷰 데이터를 충분히 수집한 뒤 실험한 결과 의미있는 결과가 나옴을 확인할 수 있었습니다.

질문 내용

1. CNN 2-ch, CNN 4-ch 모델은 어떤 방식으로 입력 벡터를 쌓았는가?

CNN모델을 구축하는 과정에서 여러 모델이 있는데 다중 채널을 사용하는 모델에 관련된 질문이 나왔습니다. 입력 벡터를 2채널로 사용한 모델 중에 한 개의 채널에는 word2vec으로 미리 선행학습 시키고 학습 과정에서 업데이트 하지 않는 벡터를 넣고, 나머지 한 채널은 처음에 랜덤 벡터를 사용하면서 학습 과정에서 이를 업데이트하는 모델이 있습니다.  쌓을 때는 그대로 채널을 2배로 하여 쌓는 구조입니다. 

2. 워드 어텐션 과정에서 채널이 여러개일 경우 학습은 어떻게 하고 가중합은 어떻게 구했는가?

학습은 그대로 그래디언트를 전파하는 채널과 전파하지 않는 채널을 따로 설정하였습니다. 채널이 여러개인 모델은 CNN filter 또한 채널 갯수를 맞춰주었습니다. 가중합도 채널이 한 개인 모델과 똑같이 CNN layer를 거친 feature map과 FC layer의 가중치를 가중합했습니다.


&lt;세션 참가 후기&gt;

[1] 다범주 감성분석을 위한 딥러닝 모형 개발 Muhammad Alex Syaekhoni(동국대학교 산업시스템공학과) 서상현(동국대학교 컴퓨터공학과), 권영식(동국대학교 산업시스템공학과)
[2] Adam에서 사용하는 Learning rate에 대한 고찰 이원준(KAIST 산업및시스템공학과)
[3] 반도체 FAB 자동반송시스템에서 온라인 Q(λ) 학습을 이용한 최적 경로 탐색 황일회, 장영재(KAIST 산업및시스템공학과), 김선일, 문인호(신성이엔지) 
[4] 머신러닝 기반 박막 블리스크 가공 공정 이상탐지 권순찬, 김동일, 박경희, 윤주성(한국생산기술연구원), 최재윤(한화테크윈) 
[5] 제조 공정에서의 데이터 기반 품질문제 원인인자 분석 김동일, 구정인, 김보현(한국생산기술연구원), 이상현, 최성수, 이준용, 강정태((주)유라) 
[6] Word2vec 기반의 의미적 유사도를 고려한 웹사이트 키워드 선택 기법 이동훈, 김관호(인천대학교 산업경영공학과)

공정, 생산 관리 분야 뿐만 아니라 우리 연구실처럼 일반 비정형 데이터에도 딥러닝 모델들을 활용하는 발표들이 생각보다 많았습니다. 아직은 딥러닝이 학술대회에서 크게 주류가 아니라서 발표 관심도 다른 분야들에 비해 적긴 했지만, 전통적인 산업공학 분야인 생산 관리 등에서도 많이 활용되는 움직임을 느낄 수 있었습니다. 아주 독특하고 기발한 아이디어를 발표한 세션은 없었지만 여러 방면에서 머신러닝 기법들을 사용하는 방법을 확인할 수 있는 좋은 기회였습니다. 아쉬운 점이라면 아침 8시 30분 첫 세션에 시작한 "Faster R-CNN의 중심 Convolution Neural Network 모델이 훈련 및 추론에 미치는 영향에 대한 분석, 신수진, 박준건, 김윤영, 장준호, 문일철(KAIST 산업및시스템공학과)" 발표를 듣지 못했다는 것입니다. 아무래도 서울에서 대전 카이스트까지 가는 거리가 굉장히 멀어서 너무 이른 발표는 놓칠 수 밖에 없는 어쩔 수 없는 상황이었습니다. 다음 추계발표대회에서도 이런 오브젝트 디텍션 같은 산업공학에서 마이너한 부류 발표도 조금 더 많아졌으면 하는 바람입니다.

춘계 학술대회 때는 이제 갓 공부를 시작한 신입생 입장이라 발표를 완전히 다 이해하지는 못했지만 이번 추계 학술대회는 예전과는 다르게 더 잘 이해할 수  있었습니다. 또한 공식적인 자리에서 하는 첫 발표를 별 문제 없이 무사히 마칠 수 있었고 다음에도 기회가 된다면 한번 더 발표할 수 있는 기회가 있었으면 하는 생각이 들었습니다.]]></content>
		<date><![CDATA[20171109224235]]></date>
		<update><![CDATA[20171109224235]]></update>
		<view><![CDATA[611]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[367]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[18]]></member_uid>
		<member_display><![CDATA[모경현]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 모경현]]></title>
		<content><![CDATA[대학원에 입학한 후 세 번째 산업공학회를 참석하게 되었습니다. 저에게 있어 이번 추계학술대회가 이전에 참석했던 학회와 다른 점이 있다면, 제가 진행했던 연구를 발표하는 학회였다는 점입니다. 발표 전날까지 PT자료를 수정하고 어색한 부분과 매끄럽지 않은 발표를 계속 연습했으며 ‘내가 이 연구를 왜 했는지’, ‘어떤 방식으로 진행했는지’, ‘그리고 그 결과가 어떠했는지’를 청중들에게 잘 전달할 수 있는 방법을 고민했습니다. 그렇게 준비를 했음에도 발표엔 아쉬움이 남았습니다. 연구 발표를 준비하면서 가장 많이했던 생각은 연구를 진행할 때 선택한 방법 하나하나에 그 이유를 담고 진행해야 한다는 점이었습니다. 그것이 곧 논리적인 근거가 되며 현재 진행 중인 연구의 의의가 되었습니다. 발표의 논리적 근거가 약하다고 느껴지거나 논리적 전개가 이상하다고 느껴지면 그것이 바로 발표에서도 드러났을 것이라고 생각합니다. 그럼에도 불구하고 연구원들이 자리를 지키고 응원해줘서 이번 발표를 무사히 마칠 수 있었습니다. 이 자리를 빌려 감사하다고 말씀드리고 싶습니다.

그리고 이번 산업공학회 세션들을 보며 느낀 점은 딥러닝과 강화학습의 비중이 늘어났다는 점이었습니다. 발표 분야의 이름도 4차 산업혁명/산업인공지능 분야라고 명명될 정도로 딥러닝에 대한 관심도가 높아지고 있음을 느낄 수 있었습니다. 때문에 여러 관련된 세션을 보면서 느낀 것은 강화학습이 제조분야의 산업현장에서 최적경로를 찾는 용도로서 매우 용이할 수 있겠다는 생각을 하였습니다. 제가 알고있는 많은 강화학습의 사례들은 게임에서 최적의 행동을 찾는 모습들이지만 산업공학도로서 가장 적용, 접근하기 쉬운 영역은 최적의 물류 이동경로를 찾는 방식인 것 같습니다. 강화학습을 공부하여 응용사례를 직접 구현, 실행해보고 싶은 마음이 들었습니다.

마지막으로 카이스트가 꽤 먼 곳에 있는 학교라고 생각했지만 생각보다 그렇게 먼 곳이 아니었습니다. 직접 본 카이스트는 연구에 몰두할 수 있는 환경이 잘 갖춰진 곳이었습니다. 이번 학회를 겸해 카이스트를 방문하여 짧게나마 그곳의 모습을 본 것이 굉장히 신선하고 좋은 경험이었습니다.]]></content>
		<date><![CDATA[20171110014747]]></date>
		<update><![CDATA[20171110014747]]></update>
		<view><![CDATA[528]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201711/5a0486b3126b72400756.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[로고.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[368]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 김동화]]></title>
		<content><![CDATA[안녕하세요.
DSBA 연구실 박사과정 김동화입니다.
이번 대한산업공학회를 다녀오게 되었습니다.

이번 해외학회 informs랑 비교해서 말하자면 역시  산업공학회는 머신러닝이 주로 이루는 것 같습니다. 그 중에서 점점 딥러닝에 대한 연구가 활발해지고 있는것 같습니다.

과제 연구중에 이상치 탐지 연구를 진행해야하는 상황이라 이런 쪽으로 들으려고 헀는데 막상 프로세스 마이닝에 대해서만 이상치 연구가 많이 이루어져있고, 제가 원하는 사용자로그분석관련해서는 따로 많이 찾아바야할 것 같습니다.

제가 최근 연구하고 있던 semi-supervised learning 에 대해서 그리고 BA시간에 배웠던 SOM에 대한 기법을 적용한 연구 사례가 있었습니다. 역시 수업을 들었던게 개연연구로도 이어지는 교훈에서 앞으로 수업선택과 개인연구 방향과 잘 잡아야 좋은 결실이 있을것 같습니다.

Deep Q-network를 통한 미래 전장 무인전투기 임무 수행 알고리즘 같은 연구는 매우 인상이 깊었습니다.  다양한 상황에서는 Q-learning을 학습하기 어려움이 있는데 이러한 문제를 Deep Q-network를 이용해 쉽게 풀어나가는 문제였습니다. 데이터 타입도 one-hot-encoding인 타입으로 분석했다는 점에서 우리 연구실은 텍스트를 주로 연구하지만 필드가 좀 다르더라도 우리가 배운내용들을 쉽게 적용할 수 있다는 생각이 들었습니다.

심층 강화학습을 통한 토목 공정 최적화 문제 같은 경우에도 Deep Q-learning과 Deep Deterministic policy gradient 를 적용했다는 점이 산업공학회는 강화학습이 더 적합한 연구분야인것 같다는 생각이 들었습니다. 해외학회에서는 healthcare 분야가 주로 이룬 연구들이 활발한 반면 산업공학회는 딥러닝과 공정 최적화라는 두가지 관점을 적용해 연구를 했다는 점이 큰 차이인것 같습니다.

그리고 반도체 분석이 주로 이루어진다는 점이 우리나라는 삼성이라는 기업이 끼고 있어서 그런지 역시 반도체 중심의 연구들이 활발히 진행되다는 점도 느껴졌습니다.]]></content>
		<date><![CDATA[20171110154206]]></date>
		<update><![CDATA[20171110155337]]></update>
		<view><![CDATA[538]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201711/5a054cf1ea5512762481.gif]]></thumbnail_file>
		<thumbnail_name><![CDATA[m_logo.gif]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[369]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 박재선]]></title>
		<content><![CDATA[발표 후기

김준홍, 김형석, 박재선, 모경현. 강필성. (2017) 웨이퍼 Bin Map 데이터 기반의 불량 탐지 및 시각화를 위한 Bin2Vec 기반의 합성곱 신경망 분류기, 대한산업공학회 추계학술대회, KAIST, 대전, 11월 4일.

제게는 첫 학회 발표였습니다. 처음이기도 하고, 실험을 진행한 지도 어느 정도 시간이 지난 것이라 발표를 준비하는 과정에서 여러번 리마인딩을 하였습니다. 중간에 말이 꼬였던 적이 있어서 더 많은 연습이 필요했을 것이라고 생각했습니다. 그럼에도 연구원들이 들어주었기에 긴장을 덜 하고 발표를 마무리 할 수 있었습니다.


참관 후기

한편으로 제가 산업공학회에서 다른 세션의 발표를 듣는 일은 처음이였습니다. 작년 추계학술대회에서는 명찰을 배부하는 일을 맡아 어려웠고, 올해 춘계학술대회는 개인적인 사정으로 참가하지 못했습니다. 그래서 저는 오전에 도착하여 관심있는 될 수 있으면 관심이 있는 모든 발표를 듣고 싶었습니다.

가장 처음 들었던 발표는 'Adam에서 사용하는 Learning rate에 대한 고찰'이였습니다. 이는 Adam이 Learning rate를 Adaptive하게 조정하여 learning rate에 민감하지 않다고 주장하지만 실제로 그러한가에 대한 질문에서 시작합니다. 매 반복마다 gradient의 update 방향을 cosine으로 계산하여 같은 방향일 시 가속, 반대 방향일 시 감속하도록 만듭니다. 이 아이디어를 기반으로 다양한 Variation을 주고, 이에 대한 입력변수 1개 1차 함수에 대하여 학습을 합니다. 물론 이에 대하여 학습을 잘 했고, 빠릅니다. 그리고 조금 더 복잡한 모델로서 강화학습의 TD 학습을 하였고, 성공적이였습니다. 하지만 발표자가 말했듯이, 이론적인 기반이 약하며 보여준 실험들도 간단한 예제로 보였습니다. 다만 이런 접근 방법도 있을 수 있다는 것이 신선했습니다.

그리고 강화학습 관련 연구들에 대하여 발표를 들었습니다. 제가 들었던 발표는 
[1]'반도체 FAB 자동반송시스템에서 온라인 Q(λ) 학습을 이용한 최적 경로 탐색'
[2]'딥러닝 기반 강화학습을 이용한 토목 공정 계획'
[3]'Deep Q-network를 통한 미래 전장 무인전투기 임무 수행 알고리즘'
[4]'강화 학습을 통한 토목 공정 최적화'
입니다.

강화학습에 평상시 흥미를 느끼고 있지만, 한편으로는 강화학습을 실제로 어떤 분야에 적용할 수 있을 지에 대하여 항상 궁금했습니다. 일반적으로 데이터가 엄청나게 많거나, 시뮬레이션이 가능한 환경(예를 들면 게임)이 아니면 쉽지 않아보였기 때문입니다. [1]은 정확하게 이해하지는 못했지만, 반도체 공정 내부의 운반 시스템에서 경로를 최적화하는 것으로 이해했습니다. 이는 시뮬레이션 환경을 충분히 만들 수 있었을 것으로 보이며, 실제 환경에서도 성공적으로 작동한다고 하였습니다. 정적인 DP로 풀지 않은 이유로는 환경이 굉장히 자주 변화하며 분포가 고정되어있지 않다고 생각했다고 합니다. 하지만 이를 제외한 강화학습들은 모두 현실 세계를 지나치게 단순화하였다고 생각합니다. 토목 공정의 경우는 현재는 설계자의 직관에 의존하여 굴착할 곳을 찾는다고 합니다. [2]와 [4]는 이를 강화학습으로 풀어보는 연구입니다. 이에 대한 환경으로 설계한 것이 4x4에 해당하는 grid입니다. 이 환경에서 어느정도 성공적인 결과를 냈다고는 하지만, 환경을 크게 늘렸을 때 구현의 어려움을 이야기했습니다. [3]도 마찬가지로 전투기의 임무 영역을 굉장히 단순하게 작성하였다는 생각을 했습니다.

하지만 외부 연구를 직접 들을 기회가 잦지 않습니다. 이번 학회에서 다양한 연구를 접할 기회가 되어 리프레시 할 수 있었으며, 다양한 연구에 대한 영감을 받을 수 있었습니다. 감사합니다.]]></content>
		<date><![CDATA[20171110192502]]></date>
		<update><![CDATA[20171110192502]]></update>
		<view><![CDATA[594]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[370]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[8]]></member_uid>
		<member_display><![CDATA[김 해동]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 김해동]]></title>
		<content><![CDATA[올해 산업공학회는 좋은 발표도 많이 듣고 카이스트에서 열린 덕에 대전에 거주하는 친구들도 오랜만에 볼 수 있어서 아주 즐거운 시간을 보내고 왔습니다. 학회를 참석하고 든 생각을 세 가지 항목으로 정리하였습니다.

[1] 올해 산업공학회에서 가장 눈에 띄었던 점은 기계 학습 분야의 발표가 아주 많아졌다는 것입니다.  비즈니스 어낼리틱스 세션이 4차 산업혁명/산업인공지능으로 이름이 바뀐 것을 보고 4차 산업혁명과 인공지능이란 사회적 화제거리가 학계에도 큰 영향을 미치고 있음을 실감했습니다. 또한 확률/통계 그리고 정보시스템 세션의 많은 발표도 기계 학습에 관한 내용이었습니다.  특히 주의 깊게 봤던 발표는 강화 학습을 이용해 경로를 최적화하는 발표였습니다. 경로 최적화라는 전통적인 산업공학 문제를 기계 학습의 강화 학습을 적용해서 풀 수 있다는 것이 흥미로웠습니다. 저는 최적화 기법을 기계 학습에 적용하는 방안에 대해서 관심이 많았는데, 그 반대로 기계 학습의 알고리즘을 최적화 문제를 푸는데 응용하는 방법에도 고민을 해봐야겠다고 생각했습니다. 큰 규모를 가지고 있거나 불확실성이 많은 데이터에서는 기존의 최적화 기법들이 작동하지 않는 경우가 종종 있는데, 이런 상황에서 기계 학습 방법을 휴리스틱 해법으로 사용할 수도 있지 않을까요.

[2] 그래도 역시 산업공학의 핵심 응용분야는 물류와 생산이란 생각이 들었습니다. 특히 반도체 공정에 대한 발표들이 많았고, 우리 연구실에서도 하고 있는 분야로 재미있게 발표를 들었습니다. 아무래도 첨단 장비가 많은 반도체가 확보할 수 있는 데이터가 많기 때문에 관련 연구가 많지 않을까 생각합니다. 반도체 사업이 규모가 크다는 것도 한 몫 할 것이구요. 다른 생산 과정에서도 소위 말하는 디지털 트랜스폼이 일어나서 확보되는 데이터가 증가한다면 제조 분야에서 기계 학습을 전공한 산업공학도가 활동할 수 있는 영역이 넓어질 거란 생각이 들었습니다.

[3] 학회에 대해 아쉬운 점도 남습니다. 많은 발표들 중 이론 연구는 거의 찾아볼 수 없었습니다. 발표 수가 적어서 오후에는 최적화 세션은 열리지도 못했습니다. 통계와 최적화 분야에서 지속적으로 이론 연구가 뒷받침이 되어줘야 응용에서도 더 재미있는 결과들이 많이 나올 수 있다고 생각합니다.]]></content>
		<date><![CDATA[20171110200955]]></date>
		<update><![CDATA[20171110200955]]></update>
		<view><![CDATA[603]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[371]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 장명준]]></title>
		<content><![CDATA[이번 산업공학회는 카이스트에서 진행되어 오랜만에 대전에 다녀올 수 있었습니다. 역시 대전은 정말 살기(에는) 좋은 도시인것 같습니다. 이번 학회에 참석하여 제가 느낀점을 아래와 같이 정리해 보았습니다.

[1] 우선 가장 눈에 띈 점은 세션 이름이 '4차 산업혁명/산업 인공지능'으로 바뀐 것이었습니다. 또한 저희 연구실에서 발표한 내용 외에도 'Adam에서 사용하는 learning rate에 대한 고찰', 'Faster R-CNN 중심 Convolution Neural Network 모델이 훈련 및 추론에 미치는 영향에 대한 분석','Deep Q-network를 통한 미래 전장 무인전투기 임무 수행 알고리즘', 등 딥러닝, 강화학습 등에 관련하여 춘계학술대회에 비해 딥러닝, 인공지능에 대한 발표가 훨씬 많아졌습니다. 이를 통해 4차 산업혁명/ 인공지능 등 사회적으로 큰 반향을 일으키고 있는 기술이 학계에도 많은 영향을 미치고 있다고 생각했습니다. 또한 '4차 산업혁명/산업 인공지능' 세션과 '정보시스템' 세션이 각각 5, 6개씩 가장 많은 비중을 차지하였습니다. 이를 통해 산업 공학의 방향성도 전통적인 최적화 중심에서 점차 데이터마이닝, 딥러닝, 인공지능 등의 방향으로 변화하고 있다고 느꼈습니다.

[2] 지난번 학회에 비해 많아지기는 했지만 여전히 딥러닝 관련 연구보다는 아직까지는 이상치 탐지 등의 기계 학습, 그리고 강화학습에 대한 연구가 훨씬 많았습니다. 강화학습의 경우는 아무래도 Dynamic Programming이 중심이 되기 때문에 산업 공학에서 접근하기 쉽기 때문이 아닐까 하고 생각했습니다. 

[3] 가장 관심을 가지고 들었던 발표는 'Word2vec 기반의 의미적 유사도를 고려한 웹사이트 키워드 선택 기법' 입니다. 아무래도 NLP에 관심이 있고, 제가 참여한 연구 발표 바로 직전에 진행되었기 때문이지 않을까 싶습니다. 발표 내용중 한가지 의문점이 들었던 것은, 발표자가 연구를 진행할 때 문서에서 명사를 먼저 추출하고, 이들을 word2vec 학습에 사용하였다고 한 점이었습니다. 한 단어의 vector 값을 학습함에 있어 같은 window 안에 위치하는 다른 단어들과함께 등장할 확률(내적값)을 최대화 하게 학습하는 skip gram의 구조상, 명사만 따로 추출하여 명사의 나열을 한 corpus로 보고 이를 통해 vector 값을 학습하게 되면 단어의 의미가 제대로 학습이 되지 않을까 하는 생각이 들어 질문을 했었습니다. 발표자는 단어의 vector값을 공간에 뿌려보았을때 비슷한 의미를 가지는 단어끼리 근처에 위치했다고 답변을 하긴 했지만 graph를 직접 보여주지는 않았기 때문에 아직까지 의문이 듭니다.

[4] 제가 학부생때 처음으로 참가했던 2015년 이후로 산업공학회 학술대회에 대해 매년 계속해서 같은 아쉬움이 듭니다.  대부분의 연구가 응용적인 측면에 집중되고, 새로운 모델이나 방법론을 개발하는 등의 이론적인 연구가 거의 이루어지지 않는다는 점입니다. 물론 공학자의 입장에서는  방법론을 잘 활용하여 현실의 문제를 해결하는 것도 매우 중요하다고 생각합니다. 하지만  현존하는 방법론들로는 해결할 수 없는 문제가 수두룩하기 때문에 학계의 발전을 위해서는 이론적인 연구가 좀 더 활발히 이루어졌으면 합니다.]]></content>
		<date><![CDATA[20171111162101]]></date>
		<update><![CDATA[20171111194722]]></update>
		<view><![CDATA[562]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[372]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[17]]></member_uid>
		<member_display><![CDATA[이기창]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 이기창]]></title>
		<content><![CDATA[지난 11월 4일 대전 KAIST에서 열린 2017 대한산업공학회 추계학술대회에서는 4차 산업혁명 관련 연구들이 주목을 받았습니다. 그도 그럴 것이 '알파고 쇼크' 이후 딥러닝, 강화학습으로 대표되는 인공지능에 대한 관심이 폭발적으로 증가했기 때문입니다. 실제로 이번 학술대회의 주제 역시 '4차 산업혁명과 산업공학'이었고, 최적화나 생산/물류관리 같은 전통적인 산업공학 관련 세션 외에 4차 산업혁명/인공지능 관련 별도 세션이 5개나 열려 산업공학회가 사회적인 관심과 궤를 같이 하는 모습이었습니다.

이번에 우리 연구실 식구들이 2017 대한산업공학회 추계학술대회에서 발표한 연구는 모두 4차 산업혁명/인공지능 관련 연구로, 세션장에서도 우리 식구들 발표가 청중들의 많은 관심을 받아 고무적이었습니다. 특히 이날 D2 세션에서는 ETRI가 발표한 'AR 기반 개방형 개인화 제조서비스 모니터링 및 협업지원'을 제외하고 우리 연구실 발표로 채워졌는데요. 박재선 학우가 발표한 '웨이퍼 Bin Map 데이터 기반의 불량 탐지 및 시각화를 위한 Bin2Vec 기반의 합성곱 신경망 분류기'의 경우 발표 순간에 청중들이 상당수 몰리고, 질문들도 쏟아져서 열띤 분위기를 보여줬습니다. 

'자유로운 문자열 기반의 사용자 인증을 위한 LSTM 기반 이상치 탐지 기법'을 발표한 김준홍 학우, '합성곱 신경망을 사용한 약지도학습 기반의 감성분류 및 단어 어텐션'을 발표한 정재윤 학우 역시 깔끔한 발표로 어디에 내놔도 손색없을 정도였습니다. 이날 오전 별도 세션에서 '단어와 자소 기반 합성곱 신경망을 이용한 문서 분류'를 주제로 발표한 모경현 학우 역시 수준급의 발표를 했습니다. 연구와 발표 준비에 매진한 연구실 식구들 정말 고생 많으셨습니다.

제 개인적으로는 딥러닝 연구 '붐'이라는 표현이 적절할 정도로 많은 연구실에서 딥러닝을 적용한 논문을 경쟁적으로 작성하고 있다는 생각이 들었습니다. 4차 산업혁명/인공지능 세션은 대부분 Convolutional Neural Network나 Recurrent Neural Network 등을 기본으로 하는 연구였습니다. 딥러닝 연구에 대한 관심이 뜨거운 만큼 앞으로 수준 높은 연구 결과물을 내지 않으면 이러한 경쟁에서 선두가 되기 어렵다는 위기감이 살짝 들기도 했습니다. 개인적으로는 딥러닝을 활용한 Application 연구 못지 않게 'Adam에서 사용하는 Learning rate 고찰(이원준, KAIST)' 같이 research에 가까운 연구도 중요한 주제가 될 것이라는 생각도 듭니다. 

여러모로 자극이 많이 되는 학술대회였습니다. 물심양면으로 모처럼의 기회를 주신 교수님께 깊은 감사를 드립니다.]]></content>
		<date><![CDATA[20171111212921]]></date>
		<update><![CDATA[20171111213053]]></update>
		<view><![CDATA[593]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[373]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 박민식]]></title>
		<content><![CDATA[2017년 11월 4일에 대전 카이스트에서 열린 대한산업공학회 추계학술대회에 다녀왔습니다.
이번 학회에서는 저희 연구실에서 다루고 있는 머신러닝과 관련된 세션인 4차 산업혁명/산업인공지능이라는 세션이 5개 열린것으로 보아 산업공학 분야에서 머신러닝에 대한 관심이 많아졌다는 것을 느낄 수 있었습니다.
이전 학회와 달랐던 점은 딥러닝 외에도 강화학습에 대한 발표가 많아진 것을 확인할 수 있었습니다.
강화학습과 관련된 연구로 알고 있었던 것은 바둑, 스타크래프트, 쿠키런 등의 게임에 적용한 사례들이었는데 산업공학 문제에 적용한 것이 흥미로웠고 이에 대한 청취 후기를 작성하려고 합니다.

[청취 후기]

[1] 딥러닝 기반 강화학습을 이용한 토목 공정 계획
[2] 심층 강화학습을 통한 토목 공정 최적화

위의 두 발표는 같은 연구실에서 토목공정이라는 주제에 대해서 서로 다른 강화학습 발표를 적용한 사례였습니다.
토목 공정은 인위적으로 땅의 높이를 변화시키는 작업인데 공정의 순서에 따라서 공정의 효율이 달라지기 때문에 
이를 최적화 하는 방법이 필요합니다.  첫번째 연구는 Double DQN을 사용하였고, 두번째 연구는 Deep Deterministic Policy Gradient(DDPG)라는 방법을 사용하였습니다 .에이전트로 땅을 파는 굴삭기 에이전트와 흙은 운반하는 트럭 에이전트가 사용되었는데, 2개 이상의 에이전트를 동시에 학습시키는 것이 흥미로웠습니다.

[3] Deep Q-Network를 통한 미래 전장 무인전투기 임무 수행 알고리즘

무인 전투기는 조종사 없이 작전 수행을 하는 전투기이고, 적의 미사일을 피하면서 목표지점에 빠르게 도달하는 것을 목표로 삼을 수 있습니다. 사용한 방법론으로 Q-function 을 추정하기 위해서 Replay memory에 쌓인 state, action, reward, next state를 batch 단위로 추출하여 학습하는 Deep Q-network를 사용하였는데, 200*400 matrix 같은 범위가 넓은 영역에선느 딥러닝 기반의 강화학습 방법이 필요하다고 느꼈습니다. 무인전투기의 environment, state, action, reward 등에 대해서 구체적으로 정의하였는데, 문제상황을 제대로 정의하는 것이 강화학습을 적용하다는데 중요하다고 느끼게 되었습니다.]]></content>
		<date><![CDATA[20171112134618]]></date>
		<update><![CDATA[20171112134645]]></update>
		<view><![CDATA[590]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[374]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[2017 INFORMS Annual Meeting - 김창엽]]></title>
		<content><![CDATA[안녕하세요.
DSBA 연구실 박사과정 김창엽입니다.

저희 연구실 박사과정 김동화 연구원과 같이 2017년 10월 22일 부터 10월 25일까지 텍사스 주 휴스턴에서 개최된 INFORMS 2017에 다녀왔습니다.

INFORMS는 Institute for Operations Research and the Management Sciences로 OR과 경영 과학이 주인 학회 입니다. 저는 상대적으로 INFORMS에서 많이 발표되지 않았던 분야인 Deep Neural Network를 활용한 감성 분류 분야에 대해 발표하였습니다.

발표 제목은 Aspect Extraction and Polarity Classification of Reviews Based on Deep Neural Network입니다. 고객은 제품을 구매하고 나서 의견을 온라인상에 기록합니다. Deep Neural Network를 활용해서 이 리뷰들이 주로 얘기하고자 하는 Aspect를 Unsupervised 방식으로 찾고 극성을 분류하는 주제로 발표하였습니다. 이 분야 연구를 위한 데이터셋은 특정 단어가 어떤 Aspect에 속하고 그에 대한 극성이 레이블되어 있습니다. 하지만 실세계에서는 이런 레이블이 존재하지 않기 때문에, Aspect 레벨로 분류 모델을 만드는 데에 어려움이 있습니다. 

이에 문제를 해결해 보고자 Topic Modeling(LDA)를 이용해 리뷰가 주로 얘기하고자 하는 주제를 찾고, Deep Neural Network가 이 토픽을 분류 하면서 리뷰 평점을 기준으로 한 긍부정을 같이 예측하도록 모델을 구축하여 학습하였습니다. Deep Neural Network는 Dynamic Memory Network를 이용하여 리뷰 평점 정보만 이용한 정확도 보다 리뷰 평점과 토픽 모델링 결과를 결합한 모델이 감성 분류에 대해 성능이 더 좋아지는 것을 확인할 수 있었습니다.

회사 생활을 하면서 Threat Intelligence 시스템 구축 업무를 일부 진행하고 있었는 데, 제 발표 다음 발표자가 이 주제에 대해 발표하여 관심 있게 들었습니다. 발표 제목은 Semantics in Hacker Forums로 해커 포럼에 해커들이 DB 패스워드 해쉬 키를 서로 거래하거나 취약점을 공격하는 코드 들을 서로 공유하는 것을 크롤링하여 해커들과 그들이 사용하는 언어, DB 등의 관계를 Heterogeneous Information Networks(HIN)을 이용하여 분석하고 시각화한 결과를 소개하였습니다. 무엇보다 자신의 연구의 자신감을 갖고 즐겁게 발표하는 모습이 인상적이었습니다.

개인적으로는 국내외를 통틀어 첫 학회 발표여서 무척 긴장되었지만 지식을 확장하고 긴장감을 조금 더 다스릴 수 있는 좋은 기회였던 것 같습니다. 부족하지만 좋은 기회를 주신 강필성 교수님께 감사 드리고 싶습니다.]]></content>
		<date><![CDATA[20171112144238]]></date>
		<update><![CDATA[20171112144339]]></update>
		<view><![CDATA[592]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201711/5a07df8b1f2888668182.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[5a054509633103629028-130x87.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[375]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 김창엽]]></title>
		<content><![CDATA[안녕하세요.
DSBA 연구실 박사과정 김창엽입니다.

11월 4일 대전 KAIST에서 열린 산업 공학회에 다녀왔습니다. 예전 직장에서 같이 일하다 대전으로 내려온 여러 연구원들을 만났고, 딥러닝에 관심이 많았던 연구원과 같이 세션에 참석하여 즐겁게 발표를 들을 수 있었습니다. 

같은 연구실에서 발표한 연구 외 참석한 세션은 다음과 같습니다. 

[1] 딥러닝 기반 강화학습을 이용한 토목 공정 계획 - 지민기, 우성철, 정요한, 박진규, 문일철(KAIST 산업및시스템공학과)
[2] Deep Q-network를 통한 미래 전장 무인전투기 임무 수행 알고리즘 - 이경택, 김상진, 김창욱(연세대학교 산업공학과)
[3] Multi-task ConvLSTM 네트워크를 통한 풍력 터빈 예측	 - 우성철, 박진규(KAIST 산업및시스템공학과)
[4]강화 학습을 통한 토목 공정 최적화 - 우성철, 정요한, 지민기, 문일철, 박진규(KAIST 산업및시스템공학과)
[5] 제조 공정에서의 데이터 기반 품질문제 원인인자 분석 - 김동일, 구정인, 김보현(한국생산기술연구원), 이상현, 최성수, 이준용, 강정태((주)유라)
[6] Word2vec 기반의 의미적 유사도를 고려한 웹사이트 키워드 선택기법 - 이동훈, 김관호(인천대학교 산업경영공학과)

우리 연구실에서 하고 있는 연구와 유사하게 타 연구실에서도 딥러닝을 활용한 방법론 들이 지난 해 학회에 비해 훨씬 더 늘어난 것을 알 수 있었습니다. 

비정형 프로젝트로 진행한 결과물을 정재윤 연구원이 발표를 진행하였고, 어텐션을 활용하여 다시 예측하는 모델을 구축하는 것도 도움이 될 것 같다 의견을 들었고, 네트워크 구조를 변경해서 이를 적용할 수 있는 아이디어도 얻었다는 데에서 좋은 수확이 있었습니다.  

산업 공학이라는 학문이 여러 도메인에 적용되어 활용되기 때문에 무엇 보다 열린 마음을 다른 사람의 의견을 듣고, 또 그들의 도메인 지식을 적극적으로 배워야 한다는 점을 다시한 번 느낄 수 있는 좋은 기회였던 것 같습니다. 해외 학회와 국내 학회 모두 참석할 수 있도록 지원해 주신 강필성 교수님께 다시 한번 감사 말씀 드립니다.]]></content>
		<date><![CDATA[20171112150015]]></date>
		<update><![CDATA[20171112150108]]></update>
		<view><![CDATA[607]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201711/5a07e3a4334d97155639.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[5a0486b3126b72400756-130x87.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[376]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 서승완]]></title>
		<content><![CDATA[2017.11.4(토)에 대전 카이스트에서 열린 대한산업공학회 추계학술대회에 
참석하였습니다. 이번 학술대회의 주제는 "4차 산업혁명과 산업공학"이었으며 이에 
따라 본래 산업공학에서 많이 연구되던 경영 공학과 생산, 물류 시스템과 같은 분야 
만 아니라 데이터 마이닝과 인공 지능에 대한 연구도 확실히 늘어 났음을 확인 할 수 
있었습니다(물론 아직도 부족하다고 생각할 수도 있지만요). 
앞으로도 지속적으로 본 분야에 대한 연구가 확장 되면 좋겠습니다.

 개인적으로 관심이 있어 참석했던 세션은 다음과 같습니다.
[1] 딥러닝 기반 강화학습을 이용한 토목 공정 계획
[2] Deep Q-network를 통한 미래 정장 무인전투기 임무 수행 알고리즘
[3] 심층 강화학습을 통한 토목 공정 최적화

[1], [3]번 발표의 경우 이번 대한산업공학회 추계학술대회의 개최지였던 KAIST 산업 
및 시스템공학과의 발표였으며,  [2]번의 경우 덕성이와 함께 GAN 스터디를 할때 함께 
공부했던 연세대학교 산업공학과 친구들의 발표였습니다. 두 연구 모두 강화학습을 
특정한 도메인에 적용시키는 연구였습니다. 토목에 관한 내용이나 무인전투기에 대한 
기본 지식이 없어 해당 내용을 온전히 전달하는 것이 힘드나, 제가 위의 발표들에서 
전달 받은 내용은 간략히 '간단한 Deep Q-network를 통해서도 좋은 성능을 뽑을 수 
있다' 입니다.

현재 연구실에서 많이들 공부하고 구현하는 CNN과 RNN을 통하여 많은 작업을 할 수 
있으며 좋은 성능을 발휘하고 있음이 사실이지만, 일련의 시퀀스를 가진 프로세스를 
처리함에 있어서는 적용시키기 어렵습니다. 위의 발표들을  들으면서 어렵게만 
생각했던 강화학습을 간단한 네트워크로 구현하여 적용시켜볼 분야가 많음을 느꼈고
 생각 만큼은 강화학습이 어렵고 먼 분야가 아닐 수 있음을 알게되었습니다.

끝으로 이번 학술대회에서 발표를 하신 모든 발표자 분들 수고 많으셨습니다.]]></content>
		<date><![CDATA[20171112153642]]></date>
		<update><![CDATA[20171112153938]]></update>
		<view><![CDATA[574]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[377]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 조수현]]></title>
		<content><![CDATA[11월 4일 대전 KAIST에서 열린 대한 산업 공학회에 다녀왔습니다. 대학원생으로서 참석하는 마지막 학회이고 처음 가는 대전인지라 부푼 기대를 안고 떠났지만 KAIST는 정말 연구와 공부하기에 좋은 환경에 위치했단 점을 깨닫고 돌아오는 기회였습니다. 이번 학회의 세션들에 참가하며 느낀 점들은 다음과 같습니다. 

[1] 인상 깊었던 발표들 중 하나는 word2vec기반의 의미적 유사도를 고려한 웹사이트 키워드 선택 기법입니다. 웹사이트 문서의 키워드를 추출할 시 빈도수에만 근거하여 추출할 경우, 문서의 의미적 키워드가 추출되지 않기 때문에 이를 고려하기 위해 word embedding 모델을 사용한 연구였습니다. 발표자가 word embedding 기반으로 실험 한 후 의미가 잘 반영되었는지 정성적으로 확인했다고 하며 평가의 어려움을 호소했는데 이 부분은 저도 topic naming에 대한 연구를 찾아보며 느꼈던 부분이라 많이 공감할 수 있었습니다.

[2] 기억에 남는 또 다른 발표는 포항공대에서 발표한 TF-IDF와 association rule을 결합하여 대용량의 transaction data에서 규칙을 찾는 연구였습니다. 단순히 빈도수에 근거하여 규칙을 찾을 땐 상대적으로 중요하지 않은 규칙이 포함될 수도 있기 때문에 단어의 중요도를 TF-IDF로 나타내어 이를 반영한 연구였습니다. 평상시 TF-IDF와 association rule이 서로 결합될 수 있는 생각조차 해보지 못했기 때문에 들었을 때 신선한 충격이었습니다. 정말 간단한 아이디어지만 결합하면 의미 있는 연구 결과가 나올 수 있다는 점을 다시 한번 깨달은 기회였습니다. 

[3] 마지막으로 학회 전반적으로 아직까지 통계, 머신러닝 기반의 제조업 관련 연구가 많다는 점을 느낄 수 있었습니다. 제조업보다는 비즈니스 쪽의 인공지능이나 딥러닝 연구를 기대했지만 여전히 반도체, 제조공정과 관련된 연구가 주를 이뤄 약간의 아쉬움이 남는 학회였습니다. 앞으로 대한산업공학회에서도 다양한 연구주제와 신선한 이론들을 많이 봤으면 좋겠다는 생각이 들었습니다.]]></content>
		<date><![CDATA[20171112171952]]></date>
		<update><![CDATA[20171112171952]]></update>
		<view><![CDATA[589]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[378]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[7]]></member_uid>
		<member_display><![CDATA[서 덕성]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 서덕성]]></title>
		<content><![CDATA[지난 주말, 2017.11.4에 카이스트에서 열린 대한산업공학회 추계학술대회에 참석하였습니다. 석사과정 4학기동안 매번 참석하고 이로써 석사과정 마지막 산업공학회라는 점에서 느낌이 남달랐습니다. 먼저, 카이스트에 처음 가보았는데, 정말 공부를 열심히 할 수 있는 분위기라는 것을 실감했습니다. 탁트인 공간과 국내 최고의 기술력을 가진 학교라는 점에서 부러움과 존경의 마음이 더욱 깊게 생겨났습니다.

경현이의 발표가 아침에 배정되어 있어 일찍 집에서 나와 출발했습니다. 매 세션마다 "4차 산업혁명/산업인공지능" 프로그램이 있었는데, 지난 학회와는 또다르게 점점 더 많은 연구자들이 관련 분야에 대해 공부하고 있다는 생각을 했습니다. 그 중 재미있게 들었던 세션을 적어보고자 합니다.

[1] Deep Q-network를 통한 미래 전장 무인전투기 임무 수행 알고리즘
이 발표는 제가 공모전에 같이 나가는 두 친구인 연세대학교 이경택, 김상진 연구자의 발표였습니다. 강화학습 알고리즘을 딥러닝에 적용하여 발전시킨 Deep Q-learning을 이용하여 학습을 진행하였습니다. 연구실 내부 세미나를 통해 Silver 교수님의 강화학습을 공부하기는 했지만, 여전히 제게는 어렵게 느껴지는 학문이라 친구들이 어떻게 공부를 했는지 궁금했습니다. 딥러닝 분야 중에서 강화학습이라는 멋있는 주제를 공부한다는 친구들이 멋있게 느껴지면서, 아직 2차원 상에서의 실험인데, 발전시켜 3차원에서도 멋진 성능을 보여주기를 기대해보고 있습니다. 저도 언젠가는 강화학습을 공부해야겠다는 생각을 했습니다.

[2] TF-IDF based Association Rule Mining for a Large Transaction Data
이 발표는 현재 진행중인 프로젝트에서 텍스트에 연관규칙 분석을 적용해보고자 하기 때문에 눈에 들어오는 주제였습니다. 본 연구에서는 단순히 단어의 빈도수만을 이용해서 Transaction을 만드는 것이 아니라, 그 중요도를 정의하기 위해 TF-IDF를 이용해 접근했습니다. 정수 단위로 구성될 Transaction을 실수단위로 접근했다는 점에서 그 틀을 깼다고 생각합니다. 현재 제가 진행하는 프로젝트에서는 sequence를 고려한 연관규칙 분석을 적용하려 하는데, 당연히 빈도를 기반으로 seq를 정의하려 했습니다. 새로운 접근도 생각해 볼 수 있는 기회가 되었습니다.

이번 학회를 통해 같은 분야에 대해 공부하고 있는 오랫동안 만나지 못한 친구들도 만나고, 새로운 접근들도 알게되는 귀한 시간을 가질 수 있었습니다. 또한, 딥러닝 기반의 연구가 속속 등장하는 것을 보며 조금 더 박차를 가해 공부해야겠다는 생각을 했습니다.]]></content>
		<date><![CDATA[20171112201039]]></date>
		<update><![CDATA[20171112201039]]></update>
		<view><![CDATA[610]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[379]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[20]]></member_uid>
		<member_display><![CDATA[김 창엽]]></member_display>
		<title><![CDATA[[Deep NLP] Lecture 13 - Linguistics]]></title>
		<content><![CDATA[이번 발표에서는 RNN(Recursive, Recurrent Neural Network)로 모델을 학습할 때 언어학적으로 고려해야 할 사항들에 대해서 알아 봤습니다.

발표에서는 크게 네 가지 주제에 대해 다뤘습니다.

[1] 문장 표현
감성 분류예시에서 "The film is hardly a treat"이라는 예시를 통해 부정+긍정 단어가 연속으로 나왔을 때 벡터에 감정을 어떻게 담을 수 있을 지에 대해 알아봤습니다. Bag-of-words와 RNN을 통해 학습할 수 있음을 알아 봤습니다. 문장으로 부터 파싱 트리를 만든 다음, 예전 224d 강의에서 다뤘던 Recursive Neural Network로 감성 분류를 할 수 있음을 알아 봤습니다. Composition Function을 어떻게 구성하느냐에 따라 감성을 얼마나 잘 표현할 수 있는 지에 대해서도 확인할 수 있었습니다. Stanford Sentiment Treebank의 결과로 부터, Recursive Neural Network 계열인 RNTN에서 파싱 트리의 중간 중간에서 Internal Supervision을 얻을 수 있음을 살펴보고, 마지막으로 Recurrent NN과 Recursive NN의 장단점을 비교하였습니다.

[2] 문법과 파싱
다음은 Recurrent NN과 Recursive NN, Stack-RNN 를 활용하여 문장을 생성하는 방법에 대해서 알아보고, 그 내부 알고리즘에 대해 살펴봤습니다. RNNG에 대한 Inductive Bias에 대해서 알아봤습니다.

[3] Word 표현
이전부터 사용해왔던 Word2Vec말고 3가지 모드를 통해 Word를 표현하는 방법을 알아봤습니다. 먼저 세 가지 모드 (Word, Morphological, Character)를 선택하고, 각 모드를 선택했을 때 생성한 벡터를 결합하여, 일종의 Mixture 모델로 단어를 나타내는 벡터를 구성할 수 있음을 확인했습니다.

[4] 인공 신겸망으로 언어학 개념 학습
주어/동사 수일치 예제와 그외 문법에 맞는지/맞지 않는 지 등을 예측하는 예시를 통해 RNN이 언어 문법 특징을 정확히 학습할 수 있는 지 확인해봤습니다.]]></content>
		<date><![CDATA[20171114155032]]></date>
		<update><![CDATA[20171121123502]]></update>
		<view><![CDATA[24]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[380]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[2017 대한산업공학회 추계학술대회 - 김형석]]></title>
		<content><![CDATA[낙엽이 지고, 공기가 차가워지는 적당한 어느날... 올해 대한산업공학회 추계학술대회가 대전 KAIST에서 개최되었습니다.  금번 산공학회는 대학원생 처음으로 참여한 첫 학기를 이후로 처음으로 발표자가 아닌 참석자의 신분으로 참석한 산공학회이었습니다. 발표자 신분과 달리 좀더 학회 프로그램에 집중 할 좋은기회라고 생각되었습니다. 또한 생전처음으로 KAIST라는 곳을 방문하게 되어 설레는 마음을 가지고 참석하였습니다.

 제가 참석한 세션은 주로 4차 산업혁명/산업인공지능이라는 프로그램을 위주로 참관하였으며, 이와 더불어 신임교원 특별세션을 참관하였습니다.
 
 4차 산업혁명/산업인공지능 프로그램에서는 주로 저희가 연구실에서 진행중인 다양한 산학프로젝트와 같이 실제 현업에서 당면한 task들을 기계학습을 기반으로 해결한 사례들을 접할 수 있었습니다. 그중에서도 최근 계속되는 딥러닝에 대한 뜨거운 관심을 대변하듯, 딥러닝기반의 CNN, Faster-RCNN, Adam Optimizer, Reinforcement Learning 등을 활용하여 감성분석, 문서분류, 공정관리, 반도체 등의 다양한 분야의 문제들을 해결하고자 하는 연구들을 확인 할 수있었습니다. 
 
 신임교원 세션에서는 올해 새로이 부임한 각 학교들의 신임 교수님들의 개인의 연구분야에 대해서 발표하는 시간을 가졌습니다. 이 세션을 통해서 각자의 연구분야에 대한 깊이와 열정 뿐만아니라, 그 분둘의 대학원 생활동안의 고민 및 고충들을 같이 나눌수 있는 시간을 가질수 있어 매우 뜻깊은 시간이 되었습니다.
  
 이 후기를 작성하면서, 제가 처음으로 참석한 2015 춘계 학술 대회가 많이 상기됩니다. 그때는 모든것이 새롭고, 제가 알아갈 부분이 엄청 많구나 생각되었습니다. 3년이 지난 지금의 저는 각 발표에 대해서 이해의 깊이는 그때와 달리 나아졌지만, 이제 겨우 학을 떼는 수준이라 생각됩니다. 그때의 마음가짐을 상기하여 초지불망의 자세로 앞으로의 연구활동에 좋은 계기가 될수 있는 학회였다고 생각합니다. 마지막으로, 내년 춘계학술대회에서는 제스스로의 연구를 가지고 다시금 발표자의 신분으로 꼭 참석할 수 있도록 하겠습니다. 읽어주셔서 감사합니다!

여러분 행복하세요~]]></content>
		<date><![CDATA[20171114171318]]></date>
		<update><![CDATA[20171114202611]]></update>
		<view><![CDATA[623]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201711/5a0ad2d36b29f4997375.jpg]]></thumbnail_file>
		<thumbnail_name><![CDATA[아름다운 가을이에요.jpg]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[2]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[381]]></uid>
		<board_id><![CDATA[1]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[[Paper Seminar] : A3C &amp; Reinforcement Learning Research Trend]]></title>
		<content><![CDATA[이번 세미나 시간에는 A3C(Asychronous Advantage Actor-Critic)이라는 강화학습 알고리즘에 대해 중점적으로 다루어 보고 2016~2017년도에 어떤 강화학습 연구가 진행되고 있는지 간략하게 알아보았습니다.

[1] Reinforcement Learning Review
강화학습을 공부 안한지 오래 되었을 것이라 생각해서
기본적인 개념들이 어떤 것이 있고 어떤 흐름으로 연결 되는지 알아보았습니다.
강화학습의 정의, MDP, Dynamic Programming, Value Function, Bellman Equation, Policy Iteration, Value Iteration, MonteCarlo Prediction, Temporal-Difference, SARSA, Q-Learning, Deep SARSA, Deep Q-Network 에 대해서 간략하게 소개하였습니다.

[2] A3C
A3C는 여러에이전트+환경을 생성에 샘플을 생성하고 비동기적인 과정으로 Global Network와 상호작용을 하면서 학습을 하는 강화학습 알고리즘입니다. A3C를 이해하는데 필요한 Policy Gradient, REINFORCE, Actor-Critic 방법과 A3C에 대해서 알아보았습니다.

[3] Reinforcement Learning Research Trend
최근에 강화학습 연구들은 A3C 알고리즘을 기반으로 하고 있습니다. 다중 에이전트 상황에서의 RL, 계층적 구조의 RL, Meta Learning을 활용한 RL 방법등을 알아보았습니다.

※ 세미나 시간에 질문 받았던 Multi-agent 학습 방법과 관련된 자료들이 있어서 첨부합니다.
관심있는 분들 읽어보시면 좋을 것 같습니다.

1. 스타크래프트1(Multiagent Bidirectional- Coordinated Nets for Learning to Play StarCraft Combat Games) : https://www.slideshare.net/KihoSuh/multiagent-bidirectional-coordinated-nets-for-learning-to-play-starcraft-combat-games

2. multi-agent actor-critic for mixed cooperative-competitive environments : http://www.modulabs.co.kr/DeepLAB_Paper_library/15445]]></content>
		<date><![CDATA[20171114193123]]></date>
		<update><![CDATA[20171121123845]]></update>
		<view><![CDATA[20]]></view>
		<comment><![CDATA[6]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[382]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[27]]></member_uid>
		<member_display><![CDATA[민성 정]]></member_display>
		<title><![CDATA[17. 11. 16. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20171116181129]]></date>
		<update><![CDATA[20171116181129]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[3]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[383]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[9]]></member_uid>
		<member_display><![CDATA[조 수현]]></member_display>
		<title><![CDATA[17. 11. 29 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20171201161346]]></date>
		<update><![CDATA[20171201161346]]></update>
		<view><![CDATA[2]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[384]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[27]]></member_uid>
		<member_display><![CDATA[정 민성]]></member_display>
		<title><![CDATA[17. 12. 07. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20171207145428]]></date>
		<update><![CDATA[20171207145438]]></update>
		<view><![CDATA[4]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[385]]></uid>
		<board_id><![CDATA[5]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[27]]></member_uid>
		<member_display><![CDATA[정 민성]]></member_display>
		<title><![CDATA[17. 12. 28. 회의록]]></title>
		<content><![CDATA[]]></content>
		<date><![CDATA[20171229112838]]></date>
		<update><![CDATA[20171229112838]]></update>
		<view><![CDATA[1]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/5/201712/5a45a856556db5276702.docx]]></thumbnail_file>
		<thumbnail_name><![CDATA[회의록1228.docx]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[386]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[10]]></member_uid>
		<member_display><![CDATA[김 준홍]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 - 김준홍]]></title>
		<content><![CDATA[2018 산업공학회 춘계학술대회가 경주 현대호텔에서 개최되었다. 
다양한 주제의 연구들이 발표되었으며, 공정 관련 발표가 상당히 많아져서 재미있었다.

[발표 후기]

이번 학회에서는 Development of defect wafer bin map classification cause region visualization methodology based on CAM and Grad-CAM 주제로 발표하였다. 
해당 연구는 실제 반도체 공정에서 완성품이 생산된 후 EDS test를 통하여 Wafer BIN Map(WBM)을 만들게 된다. 그리하여, 각 Chip에 할당한 BIN이 생성되게 되며 이를 통하여 회사에서 사전에 정한 Fail BIN을 통하여 수율을 측정하게 된다.
생성된 WBM을 통하여 반도체 전문가들은 하나씩 Bad인지 Good인지 즉 불량인지 양품인지에 대하여 패턴을 보고 해석한다. 이는 결국 수동적으로 사람이 하고 있는 상황이다.
이에 대하여 본 연구에서는 WBM단위로 Bad인지 Good인지에 대한 정보가 있기 때문에 이를 일반화 분류할 수 있는 CNN기반의 분류 모델과 class별 어디를 근사적으로 보고 모델이 판단하였는지 알 수 있는 GAP기반의 Class Activation Map과 FC기반의 local gradient를 사용하는 Grad-CAM을 사용하여 문제를 풀어 보았다. 결과는 분류 모델로써는 상당히 높은 수치인 AUROC기준 0.99이상이 나왔으며 Bad BIN단위로 나타낸 Ground truth와 IoU를 비교하였을 때 약 0.4정도의 값이 산출되었다. 그리고 아무래도 데이터가 없기 때문에 CNN에서 마지막단에 activation map이 spatial한 information을 가지고 있다고 하더라고 flatten하고 FC를 쓰는 Grad-CAM의 경우는 standard deviation이 좀 더 크게 나왔다고 해석하였다. 이를 통한 애니메이션과 BIN2Vec을 통한 시각화를 같이 사용하였을 때 Bad pattern에 대하여 어느정도 잡고 있는 것을 확인하였다. 더 많은 제품군과 생상된 대량의 WBM을 이용한다면 실제적으로 세부적인 부분도 잡을것이라고 생각한다. 본 연구는 실제 분류가 잘 되었다고 하더라도 Grad-CAM 논문에서 언급하듯이 data bias에 대한 activation결과가 나올 확률이 있다. 예를 들어 의사에 대한 class를 activation하였을 때 training data에 여자 의사만 있을 경우 청진기같은 영역 대신에 여성의 긴 머리를 activation을 하는 것을 볼 수 있다. 반도체 내에서도 이런 bias가 생길 수 있으며 이는 실제로 현업에서 모델을 심을때, 전문가와의 협업을 통하여 개선 할 수 있을 것이라고 생각한다.
본 발표는 반도체 제품군에 대하여 어느 정도 알고 있지 않으면 한번에 완벽히 이해하기 쉽지 않은 발표였다고 생각하여 처음부터 이 연구가 왜 필요하고 무엇을 하는 것인지에 대하여 한번 다시 언급하고 진행하였다. 발표 후에 따로 질문이나 첨언을 주신분이 없어서 아쉬운 부분이 있었다.
발표한 연구는 현재 진행하고 있는 연구의 base model에 해당된다. 다른 제품군과 새로운 방식으로 연구를 시도하고 있으며 이에 대한 성능이 더 우수하게 산출되고 있기 때문에 좀 더 고도화 해볼 생각이다.

[청취 후기] 
[1] Deep learning을 활용한 반도체 CMP 공정의 가상계측 시스템 제안 - 연세대학교
CMP공정에서의 예측을 위한 연구였다. CMP 공정에 대하여 처음으로 들을 수 있는 자리여서 좋았다.  실 계측값에 대한 성능 비교 연구여서 재미있게 청취하였다. Sensor signal기반의 예측 발표였는데 CNN, RNN, CNN+RNN을 사용하셨다. 결국 같이 사용한게 가장 좋은 결과가 나왔다. CNN이 RNN보다 더 좋고 robust한 결과가 나오는것에 대하여 hyper-parameter와 RNN variant, data augmentation, regularization에 대하여 질문드렸는데 이에 대한 자료가 없어서 아쉬웠다. 실제로 반도체 공정에서는 signal 데이터가 많이 나온다. 하지만 실제 엔지니어들은 현재 통계치의 데이터를 많이 사용하게 된다. 그렇기 때문에 정보 손실이 일어나고 이러한 연구들이 많이 된 것을 확인 할 수 있었다. 실제 공정에서는 완성품까지 수백개 공정이 이어진다. 따라서 전체 signal을 학습할 때 하드웨어적인 문제가 생기게 된다. 이에 대하여 좀 더 고민해봐야 되겠다는 생각을 하였다.

[2] Deep Reinforcement Learning기반의 스마트 팩토리 스케쥴링 - 연세대학교
제목을 보고 상당히 관심이 많이 갔던 연구였다. 왜냐하면 실제 RL을 기준으로 팩토리를 스케쥴링 하는 것은 상당히 많은 제약조건이 필요하며 해당 제약조건들을 알아내기도 힘든 문제중에 하나라고 생각하기 때문이다. 각 공정에 대한 capacity를 측정해야 하며 모든 챔버에 대한 capacity를 제약조건에 넣어야 한다. 그리고 reward를 정의 할때도 생산 제품의 개수도 중요하지만 수율도 중요하기 때문이다. 그리고 job schedule에 대한 부분도 최적화를 하여야 한다. 또한 중간에 몇 개의 챔버가 멈췄을 경우에도 해당 챔버를 제외하고 최적화를 하여야 한다. 본 연구에서는 공정의 sequence과 capacity를 시뮬레이션 하고 각 챔버가 능력이 동일하다고 문제를 가정한 뒤에 최적화를 하였다. 실제 데이터를 통하여 실제 공정에 적합 할때는 어떻게 해야 할까에 대한 정답에 대한 아이디어를 얻을 수 있어서 좋았다.

[3] 실시간 열차 시스템 운영을 위한 분산 최적화 기법 - 서울대학교
이번에 경주를 KTX를 타고 다녀왔다. 서울에 올라올 때 예상치 못한 지연이 일어났으며 이에 대하여 철도청에서 미안하다는 방송을 연신 발표하였다. 해당 연구는 이러한 지연에 대하여 지연을 최소화 하는 방법론을 제안하였다. 결국 지연에 대한 uncertainty를 최적화 식에 적용하여 기존의 알고리즘보다 더 평균적으로 지연사항이 일어나는가에 대한 발표를 해주셨다. 개인적으로 최적화 전공이 아니여서 한번에 다 알아듣지는 못하였지만 사용한 제약 조건에 대한 아이디어를 전해들을 수 있어서 좋았다.

[4] 반도체 공정 제어 현황 및 발전 방향, 반도체 설비 Sensor Data의 거리 기반 Pattern 분석 방법론 – SK 하이닉스
실제 엔지니어분이 실제 회사에서 하고 있는 분석 형태를 발표해 주셨다. One-class classification에 모티브를 얻으셔서 정상에 대한 Signal data의 standard deviation의 cut off를 주어서 그래프의 영역을 만들고 이에 대하여 모니터링하여 이를 초과하는 영역에 대한 것을 penalty로 주어서 novelty score를 만드셨다. 이는 상당히 간단한 방식이지만 실제 영역에서 잘 맞는다고 말씀하셨고 각 공정에 대한 time sequence가 정확하거나 좀 다르더라도 시간 보정을 해주는 DTW같은 알고리즘을 통하여 보정한뒤에 base model로 다른 알고리듬과 비교하면 좋겠다는 생각을 하였다. 또한, 실제 엔지니어의 입장에서 domain-knowledge가 있는 상태에서 분석을 할때, 개인적으로 실제 분석 결과가 잘 나왔더라도 그에 대한 해석이 너무나도 엔지니어분들의 해석과 다르다는 것을 많이 봐왔기 때문에 앞으로 domain-knowledge를 키워야 되겠다는 생각을 하였다. 이에 대한 말씀으로 Yield(수율)을 나타낼때의 실제 반도체 Wafer에서 중앙에서 Bad chip이 발생할 확률과 edge에서 bad chip의 발생할 확률은 다르다. 이에 대한 average yield를 쓰는 것은 실제 발생 확률에 대한 모순점이 있다고 말씀하셔서 그에 대하여 연구하면서 느꼈던 점과 일맥상통하는 점이 있었다. 따라서 지금 연구하고 있는 방식이 이에 대한 하나의 해결책이 될 수도 있을 것 이라고 생각이 들었다. 실제 엔지니어분께서 분석 결과를 해석해 주셔서 좋은 자리였다.]]></content>
		<date><![CDATA[20180409140415]]></date>
		<update><![CDATA[20180414102400]]></update>
		<view><![CDATA[256]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[391]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[18]]></member_uid>
		<member_display><![CDATA[모경현]]></member_display>
		<title><![CDATA[2018 대한산업공학회 추계학술대회 – 모경현]]></title>
		<content><![CDATA[이번 봄에 개최된 경주 현대호텔의 산업공학회를 다녀왔다. 개인적인 사정으로 연구실원들과 같은 시간에 출발하지 못하고 학회 당일 새벽에 출발하여 상당히 긴 하루를 보내게 되었다. 지난 학회와 달리 연구원들의 발표장소가 다양하고 넓은 공간이어서 정신없이 돌아다니며 발표를 들었던 것 같다. 미리 듣고자 하는 발표를 잘 정해놓고 맞춰 움직여야 다른 발표를 놓치지 않을 수 있을 것 같다(못들은 발표들이 있어 아쉬움이 남는다).
 
&lt;청취후기&gt;

- CAM과 Grad-CAM 기반의 불량 웨이퍼 bin map 분류 및 원인 지역 시각화 방법론

본 연구실에서 수행한 연구로써 웨이퍼의 정상과 불량을 탐지하는 것에 이어 “왜” 그것을 불량으로 탐지했는지에 대해 원인을 시각적으로 보여주는 후속 연구가 되겠다. 잘 알려진 CAM 기반 방법론은 CNN 모델의 Fully-Connected 영역을 변경하는 방식이 있고 최근에는 모델의 구조 변경없이 gradient를 이용하는 방식이 있는데 해당 연구는 두 방법론을 모두 사용한 연구였다. 결과적으로 주어진 데이터에 대해 정상 웨이퍼를 정상으로 판단한 이유를 나타낸 영역과 불량 웨이퍼를 불량으로 판단한 이유를 나타낸 영역이 사람의 직관적으로 웨이퍼를 보고 정상과 불량을 판단하는 영역과 유사하다라는 생각이 들었다. 다만 BIN test가 단순히 pass/fail의 문제가 아니며 제품별로 테스트 결과 부여되는 수치가 때로는 정상으로 판단되고 때로는 불량으로 판단되는 점에서 문제를 접근하는 방식에 유연함이 필요해 보였고, 주어진 문제를 데이터 기반으로 풀어내는 능력도 중요하지만 얼마만큼 풀어내고자 하는 분야에 대한 지식을 가지고 있는가가 정확한 분석을 수행하는데 있어 필요한 것인지 다시 한 번 느낄 수 있었다.


-딥러닝 기반의 감성 분석을 위한 비교 연구

해당 연구는 총 14개의 데이터 종류에 대해 두 개의 인코딩과 8가지 딥러닝 구조를 사용하여 총 224개 실험을 진행한 연구가 되겠다. 일반적으로 CNN이 알려지기로는 해당 모델의 깊이가 깊어질수록 성능향상이 있었으며 일정 수준 이상에서는 degradation 문제가 발생하기 때문에 top layer와 bottom layer간에 skip connection을 통해 이를 해결해왔다. 또한 RNN은 long sequence를 가질수록 성능저하가 일어나기 때문에 LSTM, GRU 구조가 제안되었고 문서의 처음과 끝을 비슷한 가중치로 고려해주기 위해 Bi-directional 방법론이 제안되었다. 이런 발전 방향에 맞춰 실제 모델의 성능이 나타나는지를 연구하였고 결론을 먼저 이야기하자면 방향과 맞는 결과가 나타나지는 않았다. 단순히 깊은 CNN모델이 더 높은 성능을 보장하는 것은 아니었고 때로 RNN은 Sequence가 있는 데이터임에도 불구하고 제대로 학습되지 못하는 경우도 보였다. 개인적으로 생각하기에 딥러닝이라 할지라도 일관성 있게 모든 데이터에서 높은 성능을 보이는 모델은 존재하지 않으며, 데이터를 통해 분석하고자 하는 문제에 알맞는 모델의 구조가 필요하다고 생각한다


- 강화학습 알고리즘을 활용한 Overlay Mark 최적 배치 자동화 시스템

반도체를 검사하기 위한 센서는 공정과정에서 다양한 위치에 배치될 수 있는데 어디에 어떤 센서를 배치해야 효율적인가를 연구한 내용이었다. 활용한 방법론은 강화학습 알고리즘은 Deep Q-learning이었으며 개인적으로 강화학습 알고리즘에도 관심이 있어 이해하려 노력한 발표였다. 내가 이해한 해당 발표는 센서가 설치되는 위치를 변수로 두어 이를 선택하는가 하지 않는가를 action으로 설정하고 선택된 변수들을 사용한 회귀식의 결과가 reward가 되는 모델을 구축한 것으로 보인다. 문제에 대한 접근 방법론은 직관적이었고 주어진 문제상황을 풀어내기 위해 모델을 어떻게 적용할 것인지 많은 고민을 한 느낌을 받았다.]]></content>
		<date><![CDATA[20180414075510]]></date>
		<update><![CDATA[20180414075557]]></update>
		<view><![CDATA[229]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[387]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[6]]></member_uid>
		<member_display><![CDATA[Donghwa KIM]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 - 김동화]]></title>
		<content><![CDATA[이번 산업공학회에서 다양한 세션들을 접할 수 있었습니다. 텍스트 마이닝 분야도 항상 일정한 비율로 발표도 있었고, 무엇보다도 예전에는 딥러닝 방법론에 집중했다면 이번 발표는 딥러닝을 활용한 어플리케이션이 눈에 띄는 것 같았습니다. 

[고장진단 &amp; 다중 y 예측 task]

기업 SK하이닉스와 산학을 이루어서 발표에서도 강화학습 딥러닝기반의 고장진단이 인상이 깊었습니다. 웨이퍼의 불량을 공정처리과정에서 misaligned된 위치를 공정 파라미터 컨트롤을 통해서 일치시키는 작업이 수행되는데, 이를 잔차로 정의하여 최소화시키는 강화학습문제로 풀어나가는 내용이 저렇게도 쓰일 수 있는거구나 하는 생각도 들었습니다. 두개의 agent을 정의하여 데이터 레이블의 불균형문제도 해결하는 팁도 얻을 수 있었고, 생각보다 RNN계열의 방법론이 이번 고장진단에 없었다는 점이 의외였습니다. 기회가 되면 제가 직접 연구해보고 싶다는 생각이 들었습니다. 공정분야에 관련해서 더 말하자면, 공정시그널 데이터를 DTW를 더 개선시키는 내용으로써 방향과 압력 다양한 변수를 고려해서 성능을 더 올린 연구가 있었는데, 내용을 듣고 역시 변수는 많으면 많을수록 좋아 질 수 밖에 없는 거구나 하는 생각 또한 들었습니다. 설비공정데이터 분석에서 skip connection을 이용한 오토인코더 CNN기법 같은 경우는 저희 프로젝트에도 쓸수 있는 방법 같았습니다. 1D-conv를 사용해 multi-target regression을 하나의 문제로 푸는 접근법도 알 수 있었고, 다른 방법으로 각 여러 개의 y를 row bind시켜 데이터를 증진 시키는 방법, 최적화 관점에서 해석했을 때 변수들간의 상관관계를 고려해 y가 여러 개인 문제를 해결하는 방법등 기존에 생각하지 안은 분야에 대해서 좀더 넓은 시야를 가지게 되었습니다. 

[텍스트 마이닝]

텍스트마이닝관점에서는 TF-IDF, LDA, Doc2Vec계열이 가장 많이 이루어졌고 3가지 방법의 방법론 그자체 보단 이를 활용한 어플리케이션연구들이 활발 했었습니다. 또한 TF-IDF을 변형으로 IDF를 문서가 등장하는 확률의 비율로 지니 계수와 문서의 거리를 같이 고려 measure에 대해서도 찾아 볼 수 있었고 역시 어플리케이션 관점에서 특허관련 임베딩이 주로 이루었던 것 같습니다. 저희 연구실과 다른 발표들을 비교해보면 우리 연구실에서 하는 분야들이 다른 연구실보다 앞서 나가는 것 같았습니다. 이번에는 단순 참여자로 참가했다면 다음에 발표자로써 한번 서 보고 싶습니다.]]></content>
		<date><![CDATA[20180409154420]]></date>
		<update><![CDATA[20180410131724]]></update>
		<view><![CDATA[240]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[388]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[12]]></member_uid>
		<member_display><![CDATA[김형석]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 - 김형석]]></title>
		<content><![CDATA[매년 봄기운이 완연해 질 무렵, 춘계학술대회는 열리게 된다. 금년도에는 신라 천년의 고도, 많은 문화유산을 가지고 있는 경주에서, 벚꽃의 향연이 펼쳐지는 보문 관강단지의 현대호텔에서 진행 되었다. 항상 춘계학술대회는 수도권이 아닌 지방에서 열려 답답한 연구실에서 벗어나, 다른 대학원 연구생들의 연구 성과물과 나의 연구 성과물을 공유하면서,  환기 시킬수 있는 좋은 자리였다. 

 [발표 후기]

  이번 18년 춘계 산업공학회 학술대회에서는 머신러닝 을 활용한 지능형 트레이딩 시스템에 대한 연구의 일환으로 
뉴스 기사만을 이용하여, 주가시장을 얼마나 이해 가능한지에 대한 연구로써 뉴스기사를 활용한 종목 추천 Knowledge Base 구축 방법론이란 주제로 발표를 수행하였다.  뉴스 기사로부터 주식 시장에서의 영향력을 가지는 이벤트의 추출 및 정의는 이벤트가 가지는 연속성, 모호성, 그리고 동시발생성으로 인해 많은 어려움이 존재한다. 본 연구는 AI기반의 지능형 트레이딩 시스템 구축을 위해 뉴스 기사를 활용하여, 기존 주가 혹은 주가지수의 등락이 아닌 사건(event)에 초점을 맞추어 종목 추천 Knowledge Base 구축 방법론을 제안한다.  뉴스 기사로부터 명시적 이벤트 추출을 위하여 본 연구에서는 기사의 서면 정보 기반의 이벤트 네트워크를 구성, 매개중심성 기반의 분할법을 통하여 중요 이벤트를 정의하였다. 이를 바탕으로 해당시점과 유사한 과거 추정을 위해 1) 명시적 이벤트 기반의 과거 유사 시점 추정법, 2) 일별 뉴스기사기반의 텐서 분해를 통한 과거 유사 시점 추정법, 그리고 3) Doc2Vec 기반의 과거 유사 시점 추정법 등을 제안한다. 마지막으로, 추정된 과거 시점과의 유사도에 근거하여 업종별 주가 등락 및 경제 변수를 요약하여 제시하였다. 실제로 본 연구를 수행한 구성원으로서 많은 애로사항들을 공유하고 싶었지만 한정된 시간으로 인해 이부분을 생략하여 다소 아쉬웠다. 이외에도 많은 피드백을 원했지만, 분위기로 인해서 피드백을 받지 못한점이 다소 아쉬웠다. 
 
 [참관 후기]

&lt; 반도체 웨이퍼의 표면 불량을 자동적으로 분류하는 CycleGAN 기반 방법 &gt;
GAN은 딥러닝 방법론중에서 현재 가장 Hot한 방법론으로, 필자는 논문 및 각종 세미나 발표를 통해서 접해왔으며, 직접 구현해본적이 없어, 본 세미나 발표에 많은 기대를 가지고 발표를 들었다. 본 연구에서는 반도체 웨이퍼 가공상에서 표면상의 불량 탐지를 위해서 웨이퍼 이미지 표면을 촬영 이를 통한 불량여부의 자동화를 목표로 하고 있었다. 이러한 목적을 가지고 본 연구팀은 다중 cycle-consistent adversarial network(CycleGAN)을 활용하여 이미지를 통한 불량 여부를 판단하고자 하였다. 실제로 GAN을 현업에 사용한 case라서 더욱 의미가 있다고 판단하였다. 또한 다소 예민할 수있는 GAN을  실제로 학습하면서 많은 노하우가 쌓였고, 발표에서 이점이 느껴졌었다. 한가지 아쉬운점이 있다면, 기존의 Image classification에서 많이 사용되는 CNN, Spatial Transformer Networks 등과 같은 방법론과의 비교분석이 수행됬다면 어땠을까 하는 점이 아쉬웠다. 이는 추후 paper를 통해서 확인해 볼수 있도록 하겠다.


&lt; Data-driven Understanding of Blockchain through Text Mining &gt;
본 연구의 approach는 필자가 과거에 수행한  'Deep learning' 연구 동향 분석과 유사한 Framework의 연구결과물이라서 더욱 눈여겨보게 되었다. 하지만 LDA수행과 주요 토픽을 추출함에 있어 manually 하게 수행된다는 한계점과 word2vec과 LDA의 수행에 대한 개별 해석등이 다소 아쉬움으로 남는다. 이와 비롯하여 Gibbs Sampling기반의 LDA가 아닌 현재 2018년 2월에 소개한 Deep Unfolding 방식의 학습 과정을 통해서 좀더 Fancy한 approach를 보완하여 수행한다면 더욱 좋은 연구가 되지 않을까 판단된다. 이와 마찬가지로 토픽모델링 수행을 통한 연구동향 분석등의 연구는 자주 수행해왔던 필자로 하여금 많은 공감대를 가질 수 있었다.

&lt; 다양성 지수 기반의 핵심 키워드 탐색 &gt;
본 발표는 필자의 발표 다음으로 수행된 발표였다. 아마도 발표자의 첫 학회 발표가 아닐까 생각된다. 그만큼 많이 긴장한듯이 보였다. 무엇보다도 잘 정리된 발표자료가 인상적이었다. 글자의 폰트도 가독성이 매우 높았음며, 적절한 예시로 사용된 그림 및 equation 및 definition 등 발표자의 슬라이드만으로도 해당 연구내용의 대략적 내용을 바로 파악 할 수 있을 정도였다. 해당 연구에서는 기존의 BOW 방식의 문서 표현 방법인 TF-IDF를 대체 가능한 TF-IGini, TF-IStirling이라는 index를 제안하였다. 이는 각 연구별 저자가 정의한 핵심 keyword를  통해서 그 성능을 평가하였는데, 이를 비교하기 위해서 제안한 두 인덱스별 상위 N개의 키워드와 저자가 정의한 핵심 keyword간의 비교를 수행하였다. 보다 정확한 비교를 위해서는 저자가 정의한 상위 키워드의 갯수와 본 연구에서 제안한 각 인덱스별 상위 N개의 키워드간의 비교 성능을 평가하였다면, 좀더 neat한 연구 결과를 보여 줄수 있을 것 같다. 또한 각 index가 가지는 의미와 해석을 좀더 구체적으로 설명하지 못한 점이 다소 아쉽게 느껴진다. 하지만 첫 발표임에도 불구하고 매우 침착한 발표는 매우 인상깊었다.]]></content>
		<date><![CDATA[20180409230123]]></date>
		<update><![CDATA[20180410131633]]></update>
		<view><![CDATA[247]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[389]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[29]]></member_uid>
		<member_display><![CDATA[HeeJeong Choi]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 - 최희정]]></title>
		<content><![CDATA[2018년 4월 15일 경주에서 개최된 2018 대한산업공학회 추계학술대회에 다녀왔습니다. 처음으로 참석한 학회이고, 벚꽃이 만개한 경주에서 열린 학회였기에 설렘과 함께 학회로 떠나는 기차에 올랐습니다. 비록 경주는 기대와는 다르게 3일내내 흐린 하늘과 떨어진 벚꽃잎들로 저희를 맞아줬지만, 학회의 발표자들은 흐린 날씨 속에서도 밝은 기운으로 흥미로운 발표들을 전해주었습니다. 첫 학회에 대한 설렘과 기대를 저버리지 않은 몇몇 발표들에 대해 후기를 남기고자 합니다.


[ DBSCAN과 ensemble 방법을 이용한 하이브리드 resampling 방법 – 연세대학교 ]

   본 연구는 여러 분야에 걸쳐 발생하고 있는 클래스불균형 문제를 해결하기 위해 사용되는 resampling 방법을 개선해 보다 좋은 decision boundary를 제공하는 하이브리드 resampling 방법에 관한 연구입니다. DBSCAN과 ensemble 방법을 이용해 decision boundary 부근에 있는 데이터만을 조정해 데이터 분포를 해치지 않으면서 더 좋은 decision boundary를 만들고자 했습니다. 
   이를 위해 resampling 과정에서 오분류된 majority 데이터에는 undersampling을 수행하고, 오분류된 minority 데이터에는 DBSCAN을 이용한 clustering을 수행한 후 noise 군집을 제외한 각 군집 안에서 DBSMOTE를 이용해 oversampling을 수행합니다. 이와 같은 두가지 방법으로 decision boundary 부근에 있는 데이터만을 조정해 더 좋은 decision boundary를 만들었습니다. 
   또한, 해당 resampling 방법은 decision boundary를 좋게 만들 뿐만 아니라 다른 방법들보다 대부분의 데이터에서 좋은 성능을 보여 robust한 방법이라는 결과도 보였습니다. 산업공학 분야에서 흔히 마주하게 되는 클래스불균형 데이터에서 중요한 resampling 방법을 간단하고 효과적인 아이디어로 개선한 점이 인상 깊었습니다. 연구의 아이디어를 제시하는데 있어서 자연스럽게 거창한 아이디어에 대한 갈망을 가지게 되는데, 그러한 모습을 되돌아 보게 되는 발표였던 것 같습니다.


[ 다양성 지수 기반의 핵심 키워드 탐색 – 명지대학교 ]

   본 연구는 frequency와 centrality로 중요한 키워드를 추출하는 기존 연구의 한계점을 지적하고, 이를 다양성 지수를 이용해 발전시킨 연구입니다. 기존 키워드 추출 알고리즘은 빈도기반의 접근법으로 자주 등장하는 키워드를 추출하고, 연구 도메인을 파악할 수 있는 중요한 키워드의 발견이 어렵다는 한계점을 가집니다. 이를 해결하기 위해 기존 TF-IDF 키워드 추출 알고리즘에서 IDF 부분을 다양성지수인 I-Gini와 I-Stirling으로 대체해 다양성지수를 기반으로 유의미한 키워드를 추출하는 알고리즘을 고안했습니다. 본 연구에서 제안한 TF-IGini와 TF-IStirling은 기존 TF-IDF보다 향상된 precision과 recall값을 보였으며, 기존 TF-IDF보다 연구 도메인을 잘 파악할 수 있는 키워드를 추출한다는 결과를 도출했습니다.
   본 발표를 들으며 위에서 언급한 발표와 마찬가지로 간단하고 효과적인 아이디어로 기존의 모델을 크게 개선시킨 점이 인상 깊었습니다. 또한, 현재 많은 사람들이 너무나도 당연하게 사용하고, 특히 문제점을 인지했음에도 불구하고 당연하게 사용하던 알고리즘에 간단한 아이디어를 더해 개선시킨 연구라 더 흥미로웠습니다. 본 발표를 들으며 사소한 것과 당연한 것에도 더 많은 관심을 기울이는 것이 연구의 첫걸음이 될 수 있겠다는 생각이 들어 많은 것을 느끼게 된 발표였습니다.


이상으로 2018 대한산업공학회 추계학술대회 후기를 마칩니다.
감사합니다.]]></content>
		<date><![CDATA[20180411194703]]></date>
		<update><![CDATA[20180411194729]]></update>
		<view><![CDATA[239]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[390]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[19]]></member_uid>
		<member_display><![CDATA[박재선]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 - 박재선]]></title>
		<content><![CDATA[이번 춘계 산업공학회는 경주에서 열렸습니다. 학회에 직접 참여하여 세션을 듣는 것은 두번째였으며, 멀리 나오는 만큼 의미있는 학회가 되길 바라며 참여하였습니다.
저는 주로 제조 공정 분야와 컴퓨터 비전 분야에 관심을 두었으며, 이 주제를 가진 연구를 위주로 주의를 기울여 청취하였습니다.

[1] Deep learning을 활용한 반도체CMP 공정의 가상 계측 시스템 제안
[2] 반도체 웨이퍼의 표면 불량을 자동적으로 분류하는CycleGAN 기반 방법

[1]은 반도체 제조 공정의 시계열 데이터을 이용하여 딥러닝에 적용하는 것입니다. 이 연구는 RNN과 CNN을 결합하여 시계열 데이터의 feature를 추출하여 regression 문제를 풀었습니다. 사실 모델에는 특별한 것이 없으며, 데이터를 이해하고 처리하는 데에 큰 고생을 했을 것으로 생각됩니다. 이 연구는 우리 연구실에서도 앞으로 시도해 볼 수 있는 연구와 연관이 있습니다. 우리 연구실에서 수행한 연구는 시계열 데이터 없이 Wafer map에 딥러닝을 적용하여 문제를 풀었지만, 앞으로 시계열 데이터와 접목시킬 가능성은 얼마든지 있을 것입니다.
[2]는 반도체 제조 공정에 비교적 최신 연구에 해당하는 CycleGAN을 응용하였습니다. CycleGAN은 기본적으로 이미지의 domain을 transfer한다고 볼 수 있습니다. 이를 이용해 정상 이미지로부터 각각 패턴별 불량 이미지를 generation하고 이를 real인지 fake인지 구별하는 discriminator가 있습니다. 새로운 이미지가 들어오면 각 불량 패턴별 discriminator가 real로 판별하면 해당 불량 패턴으로 분류합니다. 이로써 CNN에 버금가는 성능을 보였으며, 데이터에 없던 패턴의 이미지를 Unknown으로 분류할 수 있었다고 밝혔습니다.
하지만 GAN은 학습이 불안정하며, 연구의 프레임워크 상 각 불량 패턴별로 Generator와 Discriminator를 학습해야하기에 리소스 측면이나 효율성 측면에서나 더 좋다고 할 수 있을지 의문이 들었습니다.

이번 학회에서 전반적으로 제조 공정에 Deep Learning을 적용한 연구가 늘어났다는 인상을 받았습니다. Deep Learning을 비롯해 여러 개선된 방법론으로 다룰 수 있는 문제가 많아져 이들을 해결하는 것에 긍정적일 것입니다. 하지만 일부 연구들은 현실과 모델의 가정이 일치하는지 깊게 고려하지 않고 최신의 연구를 먼저 적용하여 성능이나 효율성 측면에서 유의미하지 않은 결과를 보인 사례도 있다는 생각을 하였습니다. 저도 처음에는 유사한 생각을 가지고 제조 공정에 최신 방법론을 적용하는 데에 급급했지만, 유능한 동료들 및 교수님과 함께하여 더 힘들고 어려웠지만 의미있는 연구를 했음에 새삼 감사하다는 생각이 들었습니다. 한편, 앞으로도 이런 생각을 경계하고 방법론은 목적보다는 수단이라는 생각으로 연구 및 응용해야겠다는 교훈을 다시금 받았습니다.]]></content>
		<date><![CDATA[20180411215213]]></date>
		<update><![CDATA[20180411215213]]></update>
		<view><![CDATA[257]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[392]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[26]]></member_uid>
		<member_display><![CDATA[서하 송]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 – 송서하]]></title>
		<content><![CDATA[같은 장소에 있던 수백명의 인파들은 마지막엔 제각기 다른 경험을 들고 흩어진다. 석사과정인 내게 봄에 열리는 큰 학술대회는 참석할 기회가 두번뿐이다. 가을학기에 입학한 나는 졸업학기에 마지막으로 학회에 참석하게 될 것이다. 그때는 지금보다 아는것이 많을때라 연구실 선배들처럼 학술대회에서 더 좋은 경험을 얻고 갈 수도 있겠다.

  나는 벚꽃이나 소풍에 대해 그다지 감흥을 못느끼는 부류의 사람인데, 이런걸 보고 즐거워하는 동료들과 함께 있으니 괜히 마음이 편안해 졌다. 연구실사람들과 머나먼 경주까지 함께 가는길은 학술대회겸 즐거운 여행길이며, 여행은 일상의 전환점이 된다.

  학술대회에서 나눠준 책자를 펼쳐보면 수십개가 되는 세션들이 준비되어 있다. 이 목록이 근 1년간 원생들이 만들어낸 결과물들을 대표하는 것일까? 이중에서 내가 감동받을 수 있는 세션은 얼마나 될까? 으레 이런 큰 모임이 생기면 모인 사람들은 또다시 서로 자극을 주고 자극을 받는다.

  나는 AI전문가를 목표로 공부하는데, AI 외에 다른 분야는 제목부터 어떤 말인지 이해할 수 없다. 가서 듣다보면 어떤말인지 알 수 있을까? 듣고도 이해하지 못해 시간만 낭비하는건 아닐까? 산업공학의 특성인지도 모르지만, 분야가 너무 광범위해 내 지식수준에서 자극을 받을 기회와 자극의 강도가 부족한것 같아 아쉽다.

  긴 역사동안 다재다능한 인재가 되고자 하는 사람들은 그만한 고통을 받았다. 여러 분야를 배우는 전략을 취한 사람은 항상 "그러다가 아무것도 전문가가 되지 못하는건 아니냐"는 코너에 몰린다. 이는 다분히 상대적이어서, 타 공대에서 산업공학에게 하는 질문일 수도 있고, 역설적으로 산업공학이 산업공학에게 묻는 질문일 수도 있다.

  이 질문은 순수 궁금증일수도 있지만, 사실은 다재다능하고자 하는 사람에게 던지는 효용에 대한 날카로운 공격이자, 전문가에 대한 정의를 한정하기 위한 헤게모니의 일환일수도 있겠다. 물론 이에 대한 나의 결론은 없다. 그냥 죽도록 열심히 하는게 답이다. 나의 최종 연봉이 산업공학도의 평균치를 말해주겠지. 나는 아이유보단 트와이스가, 가우스보다는 다빈치가, 김범수보단 이승기가 더 좋더라. 다재다능한게 훨씬 멋지다.

  들을 세션이 없는 시간에는 계속 코딩만 했다. 산업공학학회에 와서 나처럼 할일없이 코딩만 하고있는 사람이 또 있나 찾아보았는데, 다들 서로 대화를 하고 있었다. 인간은 사회적 동물임을 여실히 보여주는 풍경이다. 나는 여기서 노트북을 덮고 누군가와 대화하는 것이 바람직하다는걸 본능적으로 느낀다. 하지만 누구와 대화하고 어떤 네트워크를 구축해야 하는건가.

  학술대회의 본래 가치를 위해서는, 나를 포함한 모두가 이곳에서 최대한 교류해야만 한다. 그게 아니라면 그저 동영상을 시청하고 동영상을 방영하며 서로 자신이 환상적인 연구에 다가가고 있음을 자랑하는 일 이상의 의미가 있겠는가. 그런 마인드로는 그 무엇도 만들어 낼 수 없다는걸 학부시절에 어느 창업강연에서 배웠다.

  나는 다시 노트북을 열고 마저 코딩했다. 아까 풀리지 않은 알고리즘 문제가 신경이 쓰여서인지, 아니면 그다지 모르는 사람들과 네트워크를 구축할 효용이나 방법을 찾지 못해서인지는 잘 모르겠다. 내가 나를 기만하고 손해를 보고 있는건지 신경이 쓰인다. 비록 도메인 지식이 부족하더라도, 다음세션의 발표는 더 집중해서 들어보자고 다짐한다.

  나는 산업공학에 속한지 5년밖에 되지 않았으면서, 산업공학의 광범위한 영역으로 인해 자주 혼란스러움을 겪는다. 나는 5년 내내 내가 의지할 사람을 찾아왔다. 산업공학을 사랑하고 산업공학의 특성을 100퍼센트 활용할 줄 아는 다재다능하기 전략의 대가를 찾고싶다. 오버워치에서 일반인이 한조를 플레이하면 효용이 나오지 않지만, 그들은 한조를 100퍼센트 효용으로 사용하는 판타지스타 때문에 한조를 플레이한다.

  학술대회의 누군가와 대화했다면 이에 대한 답에 더 가까워 지지 않았을까? 여러모로 나 자신이 학술대회의 본래 가치를 이용하지 못한게 아닌가 싶어 찜찜한 기분이다. 나는 부족한 사람이지만 아쉬운 마음으로 내년을 기약한다.]]></content>
		<date><![CDATA[20180414120410]]></date>
		<update><![CDATA[20180416141324]]></update>
		<view><![CDATA[240]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201804/5ad177cdbae249109283.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[thumb.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[393]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[23]]></member_uid>
		<member_display><![CDATA[서 승완]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 – 서승완]]></title>
		<content><![CDATA[2018 춘계 학술대회 참가 후기

[참여 후기] 

어느덧 세 번째 참가하는 대한산업공학회 학술대회였다. 최근 가장 핫 한 토픽인 ‘스마트 팩토리’에 대한 연구 동향을 공유하기 위하여 학술대회의 주제는 ‘비즈니스 어낼리틱스와 스마트 경영’ 이었다. 그게 맞게 ‘비즈니스 어낼리틱스’와 ‘스마트 팩토리’에 관한 세션의 비중이 높았으며 많은 연구실의 연구 실적을 공유 할 수 있었다. 우리 연구실에서 공부하고 연구하는 분야에 대한 지식을 약간이나마 갖춘 상태로 학회에 참석 할 수 있었기에, 앞서 말한 여러 연구실에서 진행했던 연구에 대해서 작년과는 다르게 감탄도 하고 아쉬움도 느꼈던 학회였다. 

[발표 후기]

연구실의 연구 성과를 외부에서 공식적으로 발표하는 첫 자리였다. 이번 학술대회에서 발표를 맡은 주제는 다음과 같았다.
 - 감성 분류를 위한 다양한 딥러닝 기반의 문서 분류 알고리즘 비교 연구
 - Distance Decomposition for Variable Importance of Distance-based Novelty Detection
처음 하는 발표라 긴장과 기대를 안고 여러모로 열심히 준비를 하였으나 발표한 세션장에 pdf 리더기가 없어 ppt로 발표를 진행하면서 폰트가 깨지는 등 예상하지 못했던 일들도 있었고, 발표 자료에 기재를 하고 설명을 했음에도 질문을 다시 받는 상황도 있었다. 교수님의 피드백처럼 전공자들에게 설명하는 자리가 아닌 만큼 다음 발표때는 조금 더 기본적인 내용들에 초점을 맞추어 설명을 진행 하는게 좋겠다는 생각을 하였다. 또한 교수님이 외부 발표 심지어는 연구실 세미나 때도 항상 강조하시는 것처럼 폰트 사이즈나 그림, 표와 같은 시각적인 것들에 대한 크기 조정도 중요함을 여실히 느꼈던 발표였다.

발표를 함에 있어서 가장 인상 깊었던 경험은 청중들이었는데, 앉아 있는 태도와 자세만으로 ‘내가 너의 발표를 듣고있다’ 라는 느낌을 주는 분들이 계셨다. 이 자리를 빌어 정말 큰 힘이 됐다고, 고맙다고 전하고 싶다(전해지지 않겠지만..). 나를 포함해서 많은 사람들이 발표자의 입장 보다는 청자의 입장에서 학회를 참석하는 일이 많을텐데 저와 같은 태도를 가질 수 있으면 참 좋겠다는 생각을 학회 내내 했었다.

[청취 후기]

	DBSCAN과 ensemble 방법을 이용한 하이브리드 resampling 방법
Task independent하게 기업들과 과제를 하다보면 항상 겪는 문제 중 하나가 데이터 불균형이다. 보안, 제조 심지어는 감성분석 또한 심각하게 불균형적인 데이터를 가지고 분석을 진행하게 되는데 이를 해결하기 위한 흥미로운 주제였다. 방법론은 DBSCAN과 앙상블을 이용하여 데이터를 최대한 보존하면서 decision boundary를 만들게 된다. 다시 말해 resampling에서 오분류된 majority 데이터는 undersampling을, minority 데이터는 oversampling을 하게 되는데 oversampling을 하기 전에 DBSCAN을 이용하여 군집화를 진행하게 되고, 결과로 나온 noise cluster를 제외한 군집들에 대해서 oversampling을 진행하게 된다. 이와 같은 방법으로 class의 불균형 함을 다소 완화 할 수 있었다.

	다양성 지수 기반의 핵심 키워드 탐색 
이번 학회에서 가장 인상깊었던 발표였다. 여전히 정말 많이 사용하는 TF-IDF방법론에 대한 문제점은 다들 인지하고 있다. 하지만 개선에 대한 노력이나 의심 없이 그저 문제점을 수용하고 사용하고 있었다. 하지만 본 연구에서는 정말 간단하지만 임팩트 있는 방법을 제시했다. Gini index와 stirling을 이용하여 TF-IDF의 문제점을 해결 하였다. 
발표를 들으면서 아이디어 자체에 대한 감탄 뿐만 아니라 연구자로써 갖추어야 할 기본적인 마인드에 대해서도 많이 생각하게 되었다.

	강화학습을 활용한 반도체 포토리소그래피 공정의 Overlay Mark 최적 배치
웨이퍼에서 존재하는 vernier key 이슈에 대한 연구였다. Vernier key를 많이 사용하면 정확도는 올라가지만, 웨이퍼 자체에 손상이 가게 됨으로 트레이드 오프가 발생하게 된다. 연구의 목적은 최소한의 Vernier key를 이용하여 최대의 정확도를 산출하는 것이었고 이를 위하여 강화학습을 도입하였다. [Dual Learning for Machine Translation]이라는 논문에서도 기계번역 모델에서 학습을 위하여 강화학습을 도입하고 있는데, 강화학습을 이런식으로 가볍게 연구에 접목시킬 수 있다는 것이 놀라웠다.]]></content>
		<date><![CDATA[20180414153114]]></date>
		<update><![CDATA[20180414153114]]></update>
		<view><![CDATA[224]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[394]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[25]]></member_uid>
		<member_display><![CDATA[정 재윤]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 – 정재윤]]></title>
		<content><![CDATA[발표 후기

2018 춘계학술대회에서는 현대자동차와 함께 진행한 프로젝트인 머신러닝 및 텍스트마이닝 기반의 VDS 고객불만 분석 기법 개발에 관하여 발표하였다. 
미국 JD Power사에서는 매년 전세계 자동차를 대상으로 VDS(Vehicle Dependability Study)를 시행한다. 이 조사는 출고된지 3년 된 차량을 대상으로 진행하며, 객관식과 주관식 문항으로 이루어져 있다. 주관식 문항에 대해서는 현재 엔지니어가 하나씩 확인하며 소비자 의견을 분석하고 있는 상황이다. 다수의 차량 이용자를 대상으로 하는 설문인 만큼, 효율적인 텍스트마이닝을 통한 고객불만 분석이 필요하다. 
이에 본 연구에서는 전처리한 주관식 문항을 distributed representation 방법론 중 하나인 Doc2Vec을 사용하여 임베딩하였다. 길이가 너무 짧아서 정보가 없는 문장을 제거하고, 벡터로 표현된 문서에 이상치 탐지 기법 중 하나인 LOF(Local Outlier Factors)를 적용하여 이상 문서를 추출하였다. LOF가 일반 이상치 탐지 방법론들과는 다르게 거리에 더해 밀도까지 고려하는 장점이 있기 때문에 정성적으로 평가했을 때 더 나은 결과가 나오는 것을 확인할 수 있었다. 실제로 이상 문서와 무작위 문서에서 TF-IDF 기준 상위 단어들을 추출한 결과 80%가 unique한 단어들이 나왔으며, 이를 특이 키워드로 설정하였다. 
여기에 키워드 및 관계도 분석을 위해 카테고리 별, 특이단어 별 co-occurrence 그래프를 생성하였다. 특이단어와 가장 많이 등장하는 상위 5개 단어를 1-step word라고 지칭하였으며 1-step word들과 가장 많이 등장하는 각각의 상위 5개 단어들을 2-step word로 설정하였다. 이후 특이 단어, 1-step, 2-step word 모두 등장하는 문장도 같이 추출하여 사용자가 핵심 문장을 쉽게 파악할 수 있도록 하였다. 두 번째로는 특이 문서에 N-gram을 사용하여 추출한 구(phrase)를 가지고 거리 기반 그래프를 생성하였다. 구에 대한 임베딩 값과 핵심 단어에 대한 임베딩 값을 가지고 가장 거리가 가까운 핵심 구를 추출하였다.
발표 후 “이상 문서”에 대한 정의에 관한 질문이 매우 날카롭게 들어왔다. 사실 프로젝트를 진행하면서 가장 헷갈리던 부분이 데이터 사이언티스트 입장에서는 결과물들의 정성적 평가가 어려웠다는 점이다. 해당 분야 전문가와의 의사소통이 굉장히 중요하다는 점을 다시 한 번 느낄 수 있었던 부분이기도 했다. 

청취 후기

 Semi-supervised Learning with End-to-end Graph Convolution – 고려대학교
그래프 기반 준지도학습(Graph based Semi-supervised learning)에 graph convolution을 활용한 연구였다. 기존 graph convolution은 graph representation 자체와 데이터의 구조를 prior로 설정한다는 한계점이 있다. 또한 domain knowledge가 없다면 일반적인 준지도학습으로 확장하기가 힘들다. 발표에서는 먼저 edge를 k-NN을 사용하여 생성하고 weight는 가우시안 커널을 사용하여 정해주었다. 이후 graph convolution을 사용하여 레이블이 달려 있는 데이터에 대해서만 inference를 한 후 loss를 최소화하는 방향으로 학습하였다. 구조 자체가 end-to-end에다가 역전파를 통한 edge weight 학습이 가능하다. 준지도 학습에 convolutional layer를 적용하는 방법론은 처음 듣는 분야라서 매우 흥미롭게 들었다.  아쉬운 점이라면 기존 graph convolution 방법보다 조금 더 나은 성능을 보여주기는 했지만 다른 그래프 기반 준지도학습 방법론에 대비하기에는 아직도 살짝 부족한 듯한 실험 결과였다. 

다목적 특징 선택을 통한 반도체 제조 공정에서의 불량 탐지 방안 – 한양대학교
보통 공정에 달려있는 센서 데이터를 가지고 반도체의 양/불량을 탐지한다. 하지만 모든 센서의 특정 값을 사용하면 이를 판단하는데 시간이 너무 오래 걸리므로 효율적인 불량 탐지 기법이 필요하다고 한다. 발표에서는 다양한 유전 알고리즘을 활용하여 필요 센서만 선택하는 방법론을 제시하였다. 일단 목적식을 불량 탐지 성능의 최대화, 특징 집합의 크기 최소화 그리고 프로세스 시작을 최대한 빠르게 시작하는 것으로 설정하였다. 이를 적응형/엘리트 유전 알고리즘을 사용하여 결과값을 비교하였다. 결과적으로는 적응형 유전 알고리즘이 가장 적은 수의 센서를 선택하여 효율적이라고 한다. 유전 알고리즘을 feature selection 방법론으로 사용했지만 다른 방법론과의 비교가 없는 부분이 조금 아쉬웠다. 간단히 step-wise selection정도만 추가하여 비교했으면 더 탄탄한 흐름이 되지 않았을까 싶었다.

반도체 설비 Sensor Data의 거리 기반 Pattern 분석 방법론  - SK하이닉스
삼성 프로젝트를 해본 경험 때문인지 반도체 관련 발표임에도 이해하기 더 쉬웠다. 반도체 생성 공정에서 전체 웨이퍼 데이터를 확보할 수 있는 것 중 하나가 FDC(Fault Detection and Classification)이다. 이는 그 전 발표인 유전알고리즘을 사용한 부분처럼 센서 데이터를 의미한다. 이러한 센서 데이터를 사용하여 정상 웨이퍼의 FDC 데이터로 base pattern을 만들고 불량 웨이퍼가 벗어난 정도를 수치화하는 간단한 아이디어로 이상 parameter를 검출하였다. 지금 연구실에서 보고 있는 웨이퍼 데이터도 엔지니어마다 해석이 매우 다르다. 삼성 프로젝트를 진행했을때도 처음 받았을 때는 대체 레이블이 왜 다른지 이해가 가지 않는 웨이퍼들도 많았는데 이는 엔지니어들의 해석이 개인마다 굉장히 다를 수 있기 때문이었다. 이 때문에 실제 공정 데이터에 여러가지 알고리즘을 적용할 때 주의해야한다. 이에 더해 음성 인식 분야에서 사용하는 DTW 거리 알고리즘으로 FDC 패턴 간 거리에 따라 불량 발생과 상관있는 parameter 검출 방법에 대한 설명도 들었다. 발표 도중 제품에 대한 상세한 분석 결과까지 굉장히 자세히 설명 해주셔서 매우 놀라웠다. 제품군이 낸드플래시인 것 뿐만 아니라 데이터도 상세히 나와있었다.]]></content>
		<date><![CDATA[20180414171536]]></date>
		<update><![CDATA[20180414214313]]></update>
		<view><![CDATA[222]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[396]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[13]]></member_uid>
		<member_display><![CDATA[박민식]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 – 박민식]]></title>
		<content><![CDATA[2018년 봄, 경주 신라호텔에서 열린 대한산업공학회 춘계학술대회에 다녀왔습니다.
매학기 참석하게 되면서 느끼게 되는 점인데, 산업공학의 다양한 연구주제들 중에서도
산업공학의 문제를 데이터 분석을 통해서 해결하는 비즈니스 어낼리틱스에 대한 관심이 높아지는 것을 알 수 있었습니다. 새로 듣게 되는 연구들에 대한 과정을 들으면서 연구를 설명할 때 논리적으로 설명하는 방법에 대해서 생각해 보는 계기가 되었습니다.

- 청취 후기-
[DBSCAN과 ensemble 방법을 이용한 하이브리드 resampling 방법]
최근의 머신러닝 연구는 딥러닝을 이용한 연구들이 많이 등장하고 있는데, 다른 주제의 연구여서 주의 깊게 들었던 것 같습니다. 데이터 분석시에는 majority class의 수가 minority class의 수보다 많은 클래스 불균형 문제가 많이 있습니다. 이러한 경우에 minority class에 대해서 oversampling 을 수행하게 되는데 decision boundary가 지나치게 커지거나 분포를 왜곡시키는 경우가 있습니다. 이 문제를 해결하기 위해 저자들은 Ensemble 방법인 bagging을 이용해 분류를 수행하고 오 분류된 majority 데이터는 undersampling 을 수행하고 오 분류된 minority 데이터에 대해서는 Clustering 방법인 DBSCAN을 수행한 후 군집 안에서 oversampling 하는 방법을 제시하였습니다.
연관이 적어보이는 두 방법론들을 이용하여 문제를 해결하려 노력한 점이 좋다고 생각했습니다.

[다양성 지수 기반의 핵심 키워드 탐색]
텍스트 분석 분야에서 문서 직합에서 어떤 단어가 특정 문서 내에서 얼마나 중요한지를 나타내는 통계적 수치로 TF-IDF 를 사용하고 있습니다. 빈도 기반의 접근법이기에 자주 등장하는 키워드에 가중치를 두게 되고 자주 출현하지 않는 중요한 키워드의 발견이 어려운 한게점이 있습니다. 저자들은 정보과학 분야에서 쓰이는 Gini index, I-Gini index를 사용하여 중요한 키워드 추출하는 방법을 제시하였습니다. 논리적으로 복잡하지 않은 방법론임에도 기존 tf-idf 방법에 비해서 개선 된 결과를 보여줬다는 점에서 배울점이 많다고 느꼈습니다..


[AI 알고리즘을 활용한 Overlay Mark 최적 배치 자동화 시스템 개발]
기존에 수행했었던 반도체 wafer bin map 연구와는 다른 새로운 개념인 Vernier Key와 Overlay라는 개념을 듣게 되었습니다. 반도체 필드 내에 Vernier Key 라는것을 표시할 수 있는데 이를 이용해 반도체의 상하층이 정확하게 정렬됬는지 확인할 수 있는 Overlay라는 것을 추출 할 수 있습니다. Overlay의 계측값과 예측값 사이의 residual에 대해서 강화학습을 적용한 연구였습니다. 기존에 공부하던 강화학습에서는 적용된 사례를 본적이 없는 연구여서 신선했습니다. 그래서 실험결과에 대한 해석은 잘 이해가 되지 않았지만, 보통의 강화학습에서 exploration을 할 때 사용하는 epsilon-greedy를 사용하지 않고 알파고에서 사용한 PUCT(Polynomial Upper Confidence Trees)라는 알고리즘을 exploration에 대해 적용하는 것을 보면서 다양한 시도를 하려고 노력한 것을 느낄 수 있었습니다.]]></content>
		<date><![CDATA[20180414182557]]></date>
		<update><![CDATA[20180414182639]]></update>
		<view><![CDATA[218]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[397]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[27]]></member_uid>
		<member_display><![CDATA[정 민성]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회  - 정민성]]></title>
		<content><![CDATA[2018 대한산업공학회 춘계학술대회 – 정민성
경주에서 많은 세션을 접할 수 있었습니다. 아직은 직접 참가하여 프레젠테이션을 하진 못했지만 그럼에도 많은 것을 느낄 수 있는 자리였습니다. 학문적으로 미숙하고 정해진 바가 많이 없기 때문에 좀 더 새롭고 접해보지 못한 분야의 세션을 참석해보며 견문을 넓히는 것도 좋겠다는 생각을 했으나 제대로 이뤄지지는 않은 것 같습니다. 어쨌든 이번 학회는 여러 대학원생들이 노력하여 만든 결과물을 직접 마주하며 흥미로움, 의문, 자기반성, 감탄 등의 감정을 느끼게 된 또는 생각을 하게 된 좋은 기회였습니다. 

유한요소해석을 활용한 무인비행장치의 충돌 리스크 평가-
제목에 혹하여 참석하였지만 연구실 연구방향이나 제가 공부해야 할 방향과는 다른 영역에 있는 연구였습니다. 이 연구는 유한요소해석이라는 시뮬레이션 기술을 통해서 드론과 같은 무인비행장치가 추락, 충돌했을 때 유리, 시멘트와 같은 오브젝트나 사람에 얼마나의 충격을 미치고 그 결과가 어떻게 될지를 예측하는 것이었습니다. 꾸역꾸역 집중해서 들었으나 모르는 용어가 반 이상이었기 때문에 이해하기 힘들었습니다. 세션 중에 해당 연구와는 상관 없지만 슬쩍 들은 바로는 이런 무인비행장치 운용을 위한 기술 개발을 위해 250억 정도에 달하는 예산이 형성됐다고 합니다. 몇 개의 연구실이 참여하고 있는지는 모르겠으나 행정하는 이들의 관심도가 높은 것은 짐작할 수 있습니다. 저희 연구실에서도 대대적으로 크게 벌이지는 못하더라도 조금씩 눈 여겨볼 주제일 수 있겠다는 생각이 들었습니다.

다양성 지수 기반의 핵심 키워드 탐색-
당 연구는 TF-IDF의 한계를 지적하고 이를 개선한 지수인 I-gini와 I-stirling를 제시했습니다. TF-IDF는 많은 사람들이 만만하게 생각하고 만만하게 사용하는 지수라고 생각합니다. 그렇게 만만하게 생각하니 그 지수가 가지고 있는 한계도 당연하게 생각하게 되고 개선할 여지조차 없어지게 됩니다. 모두가 쉽게 지나치는 것에 문제의식을 느끼고 바꾸고자 노력하고 결과물을 가져온 것에 대해 박수를 치고 싶었습니다. 질문하시는 몇몇 분이 공격적인 의도로 질문하는 것이 보이고 발표자도 약간 당황하게 되는 순간이 있어서 안타까웠습니다.]]></content>
		<date><![CDATA[20180414190609]]></date>
		<update><![CDATA[20180414190654]]></update>
		<view><![CDATA[216]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[398]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[24]]></member_uid>
		<member_display><![CDATA[장 명준]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 - 장명준]]></title>
		<content><![CDATA[지난 4월 4일 ~ 4월 6일까지 경주에서 진행된 대한산업공학회 춘계학술대회에 참가하였습니다. 이번이 세번째 학회 참석이었지만, 발표자로써 참석하는 첫번째 학회였기 때문에 평소보다 조금 떨리는 마음으로 출발하였습니다. 첫 발표이기에 열심히 준비 했음에도 막상 많은 사람들 앞에 서니 매우 떨렸습니다. 하지만 교수님과 연구실 구성원을 비롯하여 여러 분들께서 발표에 집중하여 주셔서 큰 떨림 없이 발표를 마칠 수 있었습니다. 발표에 집중하는 청취자의 태도만으로도 많은 힘이 되었기에, 다른 발표자의 발표에서도 항상 집중하는 태도를 가져야겠다고 생각했습니다. 발표 중 인상적이었던 발표에 대한 느낀 바는 아래와 같습니다.  

[딥러닝 기반의 감성 분석을 위한 비교 연구]
같은 연구실 중견 연구 팀에서 진행한 연구로 사용한 데이터 셋의 개수나 모델의 개수는 정확히 기억나지는 않지만, 200개가 넘는 엄청난 양의 실험을 진행한 연구 결과였습니다. 해당 연구가 인상적이었던 점은 우선 말도 안되는 양의 실험이었습니다. 이사 전 213호에서 생활하면서 실험을 진행하기 위해 승완형이 집에서 개인 컴퓨터를 뜯어오고 창엽형이 개인 컴퓨터까지 사용하시는 등 각자 3~4대의 컴퓨터를 돌려서 실험을 진행하는 모습을 보고 대단하다는 말이 나올 수 밖에 없었습니다. 두번째는 실험을 통해 insight를 도출한 부분입니다. “Word-RNN이 일반적으로 가장 좋은 성능을 보인다”, “RNN에서 character level은 쉽게 망가지는 경향이 있다” 등 해당 연구를 통해 얻은 많은 insight는 앞으로 저 뿐만 아니라 많은 사람들이 연구를 진행함에 있어 guideline을 제공할 수 있을 것으로 보입니다. 

[다양성 지수 기반의 핵심 키워드 탐색]
이번 학회에서 가장 인상깊었고 느끼는 바가 많았던 발표였습니다. 해당 연구에서는 단순히 count에 기반한 TF-IDF의 문제점을 해결하기 위해 IDF term을 Gini 지수와 Stirling 지수를 사용하여 TF-IDF를 개선한 TF-IGini, TF-IStirling 지수를 제시하였습니다. 특히 Stirling 지수를 사용함으로써 단어 사이에 correlation을 반영할 수 있었습니다. 이 발표다 인상적이었던 이유는 문제의 인식과 이를 해결한 방법에 있습니다. TF-IDF의 한계점은 이미 널리 알려져 있었지만, (개인적으로) 오래된 방법론이라는 이유로 이 문제에 대한 해결책을 생각하지 않고있었습니다. 이 발표를 청취한 후 최근 트렌드에만 몰두하는 경향이 있는 저의 연구 태도를 반성하게 되었습니다. 또한 발표자는 매우 간단한 방법을 사용하여 아주 효율적으로 문제를 해결하였습니다. 사람들은 소위 있어 보이는 연구를 하기 위해 문제를 어렵게 꼬아서 생각하는 경향이 있는데, simple한 방법으로 문제를 간단히 해결한 것을 보고 “성능이 비슷하다면 가장 간단한 알고리즘이 가장 좋은 알고리즘이다”라는 것을 다시 한번 느낄 수 있었습니다.]]></content>
		<date><![CDATA[20180415004003]]></date>
		<update><![CDATA[20180415004003]]></update>
		<view><![CDATA[239]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[399]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[30]]></member_uid>
		<member_display><![CDATA[Woosik Yang]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회- 양우식]]></title>
		<content><![CDATA[경주에서 열린 2018 대한산업공학회 추계학술대회에 다녀왔습니다. 대학원 진학 후 처음으로 참석한 학회이며 경주 또한 처음으로 가게 되어서 두배의 설렘을 안고 학회를 참여하였습니다. 화려한 벚꽃길과 함게 다양한 주제의 연구들을 접하며 개인적으로도 많은 영감을 받았던 좋은 경험이 되었습니다. 그 중 인상깊었던 몇 가지 연구들의 후기를 공유하고자 합니다. 

-DBSCAN과 Ensemble 방법을 이용한 하이브리드 Resampling 방법 – 연세대학교 

Task에서 가장 쉽게 나타나며 중요한 문제 중 하나로 데이터 불균형을 뽑을 수 있습니다. 본 연구는 데이터 불균형을 해결하고자 하는 연구였기에 주목해서 듣게 되었습니다. 딥러닝, 머신러닝 분야인 만큼 데이터의 중요성은 아무리 강조해도 부족할 것이며 실제 현장에서의 데이터 불균형 문제 해결은 Task를 훨씬 원활하게 수행하게 만들 수 있기 때문입니다.  DBSCAN과 앙상블 기법을 통해 decision boundary를 만들어 데이터를 최대한 보존하고자 하는 방법론을 채용했습니다. 그리고 이와 같은 방법론이 대부분의 데이터에서 좋은 성능을 보이는 robust한 방법론임을 보이기도 하였습니다. 실제 Task에서 마주치는 현실적 문제에 대하여 적확한 판단을 통해 해결책을 제시했다는 점에서 기억에 남습니다. 

- 다양성 지수 기반의 핵심 키워드 탐색 – 명지대학교

이번 학회에서 한 가지 새롭게 배운 점이 있다면 계량서지학 분야에 대해 머신러닝 기법을 적용하려는 다양한 시도가 많았다는 점입니다. 계량서지학이란 출판물의 다양한 측면을 측정하는 학문을 말하면 이는 결국 논문 출판물 자체에 대한 연구가 활발하게 진행되고 있다는 점에서 의외라고 느꼈습니다. 본 연구는 기존 빈도 기반의 접근법 연구가 가지는 한계점을 지적하며 다양성 지수의 방법을 발전시켜  유의미한 키워드를 추출하는 것을 목표로 하였습니다. 즉, 기존의 frequency와 centrality를 활용한 TF-IDF 수치와 Gini idex와 같은 다양성 지수를 혼합하여 중요한 키워드를 추출하는 방법을 보였습니다. TF-IDF는 저 또한 개인적으로도 많이 써왔던 방법론이며 흔히 알려져 있는 방법론입니다. 단순히 알려져 있는 방법론을 활용하는 것에 그치지 않고 그 한계를 파악하여 더욱 발전시키려 했다는 점에서 연구자가 가져야 할 바람직한 자세에 대해 다시 한 번 더 느낄 수 있던 발표였습니다. 

첫 학회 참석이어서 큰 기대와 함께 했으며 그 기대가 오롯히 충족되어 개인적으로 뜻 깊은 시간이었습니다. 비록 석사과정이라 내년 봄 학술대회가 마지막이겠지만 그때에는 한 명의 발표자로 저의 개인적 연구를 발표하게 되는 것을 목표로 해야겠다는 생각이 들었습니다. 

이상으로 2018 대한산업공학회 추계학술대회 후기를 마칩니다.
감사합니다.]]></content>
		<date><![CDATA[20180415150943]]></date>
		<update><![CDATA[20180415150943]]></update>
		<view><![CDATA[216]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[401]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[31]]></member_uid>
		<member_display><![CDATA[Kyoungchan Park]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회-박경찬]]></title>
		<content><![CDATA[올해 4월 5일에서 6일까지 이틀간 진행되었던 대한산업공학괴 춘계학술대회에 다녀왔습니다. 이번에 처음 이 학회에 참석하였기 때문인지 모든 것이 새롭고 흥미로웠습니다. 이번 학회에서는 “비즈니스 애널리틱스와 스마트경영”이라는 주제로 다양한 학교 혹은 회사에 소속된 연구원 분들의 연구에 대한 발표가 있었습니다. 신입생으로서 제가 가야할 길을 앞서 걸어가고 있는 사람들의 연구에 대한 발표를 들을 수 있다는 점에서 매우 유익한 시간이였습니다. 그 중에서도 기억에 남는 몇가지 발표에 대한 후기를 남겨보도록 하겠습니다.

[다양성 지수 기반의 핵심 키워드 탐색 – 명지대학교]
이 발표에서는 빈도기반의 접근법인 TF-IDF의 문제점을 지적하고 이를 다양성 지수를 사용하여 해결하였음을 보여주었습니다. TF-IDF 방법론은 단순히 특정 문서에만 특히 자주 등장하는 단어에 가중치를 더 주고자 하는 빈도기반의 접근법이기 때문에 연구 도메인의 정말로 중요한 키워드를 잘 뽑아낼 수 없다는 태생적인 한계점이 있었는데 이를 다양성 지수를 활용하여 TF-IGini와 TF-IStirling을 개발함으로써 해결한 것입니다. 사실 이 발표는 제가 학회에서 들었던 모든 발표 중에 가장 쉽게 이해할 수 있었던 발표였습니다. 그럼에도 불구하고 이 발표가 인상 깊었던 이유는 제가 가지고 있던 편견을 벗겨주었기 때문입니다. 이 발표를 듣기전 까지 저는 연구라는 것을 굉장히 어렵고 복잡한 어떤 것을 개발하는 것이라고 생각했습니다. 하지만 이 발표를 듣고 난 후 연구라는 것은 꼭 어렵고 복잡해야 하는 것이 아니고, 특정 상황에서 해결되지 못한 문제를 자신만의 방법으로 풀기만 하면 된다는 것을 깨달았습니다. 따라서 이 발표는 이제 막 대학원에 입학하여 공부를 시작하는 신입생인 저에게 굉장히 뜻깊은 시간이였습니다. 

[딥러닝 기반의 감성 분석을 위한 비교 연구 - 고려대학교]

이 발표는 감성 분석에 사용될 수 있는 다양한 종류의 데이터 셋과 이에 적용되는 딥러닝 모델에 따른 성능을 측정하는 비교실험에 관한 것 이였습니다. 현재까지 CNN 혹은 RNN으로부터 파생된 수 많은 딥러닝 모델과 테크닉들이 연구/개발되었습니다. 하지만 정작 새로운 문제에 당면했을 때 어떤 모델을 쓰는게 적합한지에 대한 지침은 매우 부족한 실정이였습니다. 이 발표는 이러한 문제를 해결하기 위해 각 모델의 성능을 비교하기 위해 수 많은 실험을 했고 각각의 성능을 측정하였습니다. 이를 통해 각 모델이 어떤 task에 적합한가에 대한 잠정적인 결론을 이끌어낼 수 있었고 향후 많은 사람들이 연구를 진행함에 있어서 좋은 참고자료를 제공하였습니다. 이 발표는 자신의 아이디어를 통한 연구 성과물을 발표하는 다른 발표들과는 조금 성격이 달랐지만, 이전까지 아무도 하려고 하지 않았던 지난하고 고된 그렇지만 중요한 일을 시도했다고 생각되어 굉장히 인상 깊었습니다.]]></content>
		<date><![CDATA[20180416114848]]></date>
		<update><![CDATA[20180416114848]]></update>
		<view><![CDATA[217]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[]]></thumbnail_file>
		<thumbnail_name><![CDATA[]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[402]]></uid>
		<board_id><![CDATA[7]]></board_id>
		<parent_uid><![CDATA[0]]></parent_uid>
		<member_uid><![CDATA[28]]></member_uid>
		<member_display><![CDATA[Gyubin Son]]></member_display>
		<title><![CDATA[2018 대한산업공학회 춘계학술대회 - 손규빈]]></title>
		<content><![CDATA[2018 대한산업공학회 춘계학술대회 후기

이번 학회는 제 첫 학회입니다. 발표를 하거나 논문을 제출하진 않았지만 불과 1년 전만 하더라도 학회에 참가한다는 것을 상상도 못한 저에겐 매우 특별한 시간이었습니다. 관심 분야에 대해 최대한 많이 들어보려 노력했고 학계의 트렌드나 공부하면 좋을 키워드들을 많이 알게 되어 좋았습니다. 저희 연구실 소속 연구원들의 발표가 많아서 각각 속한 세션들에 참가해서 여러 발표들을 들었습니다.

먼저 김준홍 박사과정의 "CAM과 Grad-CAM 기반의 불량 웨이퍼 bin map 분류 및 원인 지역 시각화 방법론" 발표를 들었습니다. 딥러닝이 가장 산업과 직접적으로 맞닿아있는 분야라 생각합니다. 기존의 엔지니어가 직접 불량 웨이퍼를 분류하던 방식을 어떻게 자동화하고, 시각화했는지 잘 알 수 있었습니다. 알고리즘이 분류를 잘 하더라도 그것이 "왜 그렇게 분류했는지"를 알아야 현직 엔지니어들에게 설명을 할 수 있는데 그걸 시각화로 잘 해결했습니다. 자연어처리나 음성, 컴퓨터 비전이 활발하게 연구되고 있고, 그런 기반 이론들을 바탕으로 이렇게 산업에 적용되어 실용적으로 좋은 결과를 보이는 것이 의미가 큰 것 같습니다.

이후엔 다른 세션장으로 옮겨 Business analytics 분야의 발표를 들었습니다. 연세대학교 이경택님의 "DBSCAN과 ensemble 방법을 이용한 하이브리드 resampling 방법"에 대한 발표였는데요. 클래스 불균형 데이터에서 resampling할 때 minority 클래스만 고려하는데 그 중에서도 DBSCAN을 활용해 노이즈가 아닌 데이터만 resampling 하는 방법입니다. 들으면서 좋은 접근이구나 생각했고, 실제로도 굉장히 많은 실험을 통해 다른 모델과 비교해서 더 나은 성능을 내는 것을 보였습니다. 질문에 대한 대처도 깔끔했고 매우 훌륭한 발표였습니다.

이후 서승완 박사과정의 "Distance Decomposition for Variable Importance of Distance-based Novelty Detection"과 "딥러닝 기반의 감성 분석을 위한 비교 연구"를 연달아 들었습니다. 첫 번째 발표는 novelty detection에서 왜 그렇게 분류되었는지 distance decomposition을 통해 score 값으로 설명하는 내용이었고, 두 번째 발표는 감성분석에서 CNN과 RNN 기반 아키텍처들이 각각 어떤 성능을 보이는지 동일한 조건에서 다양한 데이터에 대해 학습한 결과와 해석을 보여주었습니다. 첫 번째는 결과를 설명해내는 새로운 metric을 제시했다는 점에서 매우 인상깊었고, 두 번째는 신입생의 입장에서 수 백번에 달하는 학습 결과를 통해 모델을 비교할 수 있었다는 점에서 큰 그림을 그리는데 큰 도움을 받았습니다.

이 외에도 재밌는 발표들을 많이 들었습니다. 장명준 석사과정의 Semantic VAE에선 기존 RNN-AE보다 정성적으로 더 나은 결과를 보이는, 즉 모델이 좀 더 사람같은 해석을 내는 것이 좋았고, 김형석 박사과정의 뉴스기사를 활용한 Knowledge Base 구축은 접근 자체가 매우 흥미로웠고 실제 사회의 하루 하루들을 벡터로 구성해 비슷한 날들을 파악할 수 있다면 할 수 있는 것들이 굉장히 많아질 것 같습니다. 스마트팩토리 분야에선 강화학습을 이용한 공정 개선이나 GAN을 이용한 웨이퍼 분류에 대한 내용들이 많았고 실무진들도 많이 참석해서 열띤 토론을 보였습니다. 고려대학교 강현구 박사과정의 "Semi-supervised learning with end-to-end graph convolution"도 흥미롭게 들었는데요. 모델 설명에 앞서 기존 방식과 이론에 대해 설명해주셔서 신입생인 저에게 매우 좋았고 graph convolution에 대해서 더 자세히 알아봐야겠다는 생각이 들었습니다.

위의 학회 관련 내용 이외에도 연구원 전원이 참석해서 같이 숙식하며 시간을 많이 보내서 좋았습니다. 교수님이 근사한 저녁도 사주셔서 다들 굉장히 맛있게 먹었습니다. 역시 맛있는 음식 앞에서는 다들 표정이 더할 나위없이 밝았습니다. 앞으로도 더 함께 좋은 시간들 많이 보내고 학회에서 좋은 자극 받으며 훌륭한 연구를 했으면 좋겠습니다.

감사합니다.]]></content>
		<date><![CDATA[20180416212712]]></date>
		<update><![CDATA[20180416212712]]></update>
		<view><![CDATA[245]]></view>
		<comment><![CDATA[0]]></comment>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<thumbnail_file><![CDATA[/wp-content/uploads/kboard_thumbnails/7/201804/5ad496a063a187906280.png]]></thumbnail_file>
		<thumbnail_name><![CDATA[gcn_web.png]]></thumbnail_name>
		<category1><![CDATA[]]></category1>
		<category2><![CDATA[]]></category2>
		<secret><![CDATA[]]></secret>
		<notice><![CDATA[]]></notice>
		<search><![CDATA[1]]></search>
		<password><![CDATA[]]></password>
	</data>
</kboard_board_content>
<kboard_board_latestview>
</kboard_board_latestview>
<kboard_board_latestview_link>
</kboard_board_latestview_link>
<kboard_board_meta>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[comment_skin]]></key>
		<value><![CDATA[default]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[permission_comment_write]]></key>
		<value><![CDATA[1]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[permission_read_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[permission_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[permission_comment_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[comments_plugin_row]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[pass_autop]]></key>
		<value><![CDATA[disable]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[auto_page]]></key>
		<value><![CDATA[553]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[auto_page]]></key>
		<value><![CDATA[627]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[permission_comment_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[permission_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[permission_read_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[permission_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[permission_read_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[max_attached_count]]></key>
		<value><![CDATA[50]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[comments_plugin_row]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[pass_autop]]></key>
		<value><![CDATA[disable]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[permission_comment_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[pass_autop]]></key>
		<value><![CDATA[disable]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[comments_plugin_row]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[auto_page]]></key>
		<value><![CDATA[812]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[permission_comment_write]]></key>
		<value><![CDATA[1]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[max_attached_count]]></key>
		<value><![CDATA[5]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[permission_read_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[permission_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[permission_comment_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[pass_autop]]></key>
		<value><![CDATA[disable]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[comments_plugin_row]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[permission_comment_write]]></key>
		<value><![CDATA[1]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[permission_comment_write]]></key>
		<value><![CDATA[1]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[permission_read_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[permission_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[permission_comment_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[pass_autop]]></key>
		<value><![CDATA[disable]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[comments_plugin_row]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[auto_page]]></key>
		<value><![CDATA[617]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[max_attached_count]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[auto_page]]></key>
		<value><![CDATA[870]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[max_attached_count]]></key>
		<value><![CDATA[5]]></value>
	</data>
	<data>
		<board_id><![CDATA[5]]></board_id>
		<key><![CDATA[comment_skin]]></key>
		<value><![CDATA[default]]></value>
	</data>
	<data>
		<board_id><![CDATA[6]]></board_id>
		<key><![CDATA[comment_skin]]></key>
		<value><![CDATA[default]]></value>
	</data>
	<data>
		<board_id><![CDATA[4]]></board_id>
		<key><![CDATA[comment_skin]]></key>
		<value><![CDATA[default]]></value>
	</data>
	<data>
		<board_id><![CDATA[1]]></board_id>
		<key><![CDATA[max_attached_count]]></key>
		<value><![CDATA[3]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[comment_skin]]></key>
		<value><![CDATA[default]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[permission_comment_write]]></key>
		<value><![CDATA[1]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[permission_read_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[permission_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[permission_comment_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[comments_plugin_row]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[pass_autop]]></key>
		<value><![CDATA[disable]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[auto_page]]></key>
		<value><![CDATA[1050]]></value>
	</data>
	<data>
		<board_id><![CDATA[8]]></board_id>
		<key><![CDATA[max_attached_count]]></key>
		<value><![CDATA[5]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[auto_page]]></key>
		<value><![CDATA[1103]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[max_attached_count]]></key>
		<value><![CDATA[5]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[permission_read_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[permission_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[permission_comment_write_roles]]></key>
		<value><![CDATA[a:1:{i:0;s:13:"administrator";}]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[pass_autop]]></key>
		<value><![CDATA[disable]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[comments_plugin_row]]></key>
		<value><![CDATA[10]]></value>
	</data>
	<data>
		<board_id><![CDATA[7]]></board_id>
		<key><![CDATA[comment_skin]]></key>
		<value><![CDATA[default]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[permission_comment_write]]></key>
		<value><![CDATA[1]]></value>
	</data>
	<data>
		<board_id><![CDATA[9]]></board_id>
		<key><![CDATA[comment_skin]]></key>
		<value><![CDATA[default]]></value>
	</data>
</kboard_board_meta>
<kboard_board_option>
</kboard_board_option>
<kboard_board_setting>
	<data>
		<uid><![CDATA[1]]></uid>
		<board_name><![CDATA[Lab Archive]]></board_name>
		<skin><![CDATA[default]]></skin>
		<use_comment><![CDATA[yes]]></use_comment>
		<use_editor><![CDATA[]]></use_editor>
		<permission_read><![CDATA[author]]></permission_read>
		<permission_write><![CDATA[author]]></permission_write>
		<admin_user><![CDATA[dsba_admin]]></admin_user>
		<use_category><![CDATA[yes]]></use_category>
		<category1_list><![CDATA[공지,세미나]]></category1_list>
		<category2_list><![CDATA[]]></category2_list>
		<page_rpp><![CDATA[10]]></page_rpp>
		<created><![CDATA[20160111161153]]></created>
	</data>
	<data>
		<uid><![CDATA[6]]></uid>
		<board_name><![CDATA[Project Board(국가보안기술연구소)]]></board_name>
		<skin><![CDATA[default]]></skin>
		<use_comment><![CDATA[]]></use_comment>
		<use_editor><![CDATA[]]></use_editor>
		<permission_read><![CDATA[author]]></permission_read>
		<permission_write><![CDATA[author]]></permission_write>
		<admin_user><![CDATA[]]></admin_user>
		<use_category><![CDATA[]]></use_category>
		<category1_list><![CDATA[]]></category1_list>
		<category2_list><![CDATA[]]></category2_list>
		<page_rpp><![CDATA[10]]></page_rpp>
		<created><![CDATA[20160324100548]]></created>
	</data>
	<data>
		<uid><![CDATA[4]]></uid>
		<board_name><![CDATA[Lab Photos]]></board_name>
		<skin><![CDATA[venus-webzine]]></skin>
		<use_comment><![CDATA[]]></use_comment>
		<use_editor><![CDATA[]]></use_editor>
		<permission_read><![CDATA[all]]></permission_read>
		<permission_write><![CDATA[administrator]]></permission_write>
		<admin_user><![CDATA[]]></admin_user>
		<use_category><![CDATA[]]></use_category>
		<category1_list><![CDATA[]]></category1_list>
		<category2_list><![CDATA[]]></category2_list>
		<page_rpp><![CDATA[10]]></page_rpp>
		<created><![CDATA[20160206225748]]></created>
	</data>
	<data>
		<uid><![CDATA[5]]></uid>
		<board_name><![CDATA[Project board(NCSoft)]]></board_name>
		<skin><![CDATA[default]]></skin>
		<use_comment><![CDATA[]]></use_comment>
		<use_editor><![CDATA[]]></use_editor>
		<permission_read><![CDATA[author]]></permission_read>
		<permission_write><![CDATA[author]]></permission_write>
		<admin_user><![CDATA[]]></admin_user>
		<use_category><![CDATA[]]></use_category>
		<category1_list><![CDATA[]]></category1_list>
		<category2_list><![CDATA[]]></category2_list>
		<page_rpp><![CDATA[10]]></page_rpp>
		<created><![CDATA[20160324093152]]></created>
	</data>
	<data>
		<uid><![CDATA[7]]></uid>
		<board_name><![CDATA[Lab News]]></board_name>
		<skin><![CDATA[venus-webzine]]></skin>
		<use_comment><![CDATA[]]></use_comment>
		<use_editor><![CDATA[]]></use_editor>
		<permission_read><![CDATA[all]]></permission_read>
		<permission_write><![CDATA[author]]></permission_write>
		<admin_user><![CDATA[]]></admin_user>
		<use_category><![CDATA[]]></use_category>
		<category1_list><![CDATA[]]></category1_list>
		<category2_list><![CDATA[]]></category2_list>
		<page_rpp><![CDATA[10]]></page_rpp>
		<created><![CDATA[20160418135310]]></created>
	</data>
	<data>
		<uid><![CDATA[8]]></uid>
		<board_name><![CDATA[Project Board(중견연구과제)]]></board_name>
		<skin><![CDATA[default]]></skin>
		<use_comment><![CDATA[yes]]></use_comment>
		<use_editor><![CDATA[]]></use_editor>
		<permission_read><![CDATA[author]]></permission_read>
		<permission_write><![CDATA[author]]></permission_write>
		<admin_user><![CDATA[]]></admin_user>
		<use_category><![CDATA[]]></use_category>
		<category1_list><![CDATA[]]></category1_list>
		<category2_list><![CDATA[]]></category2_list>
		<page_rpp><![CDATA[10]]></page_rpp>
		<created><![CDATA[20160817133136]]></created>
	</data>
	<data>
		<uid><![CDATA[9]]></uid>
		<board_name><![CDATA[Project Board(삼성전자)]]></board_name>
		<skin><![CDATA[default]]></skin>
		<use_comment><![CDATA[]]></use_comment>
		<use_editor><![CDATA[]]></use_editor>
		<permission_read><![CDATA[author]]></permission_read>
		<permission_write><![CDATA[author]]></permission_write>
		<admin_user><![CDATA[]]></admin_user>
		<use_category><![CDATA[]]></use_category>
		<category1_list><![CDATA[]]></category1_list>
		<category2_list><![CDATA[]]></category2_list>
		<page_rpp><![CDATA[10]]></page_rpp>
		<created><![CDATA[20160905165444]]></created>
	</data>
</kboard_board_setting>
<kboard_comments>
	<data>
		<uid><![CDATA[2]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 이번 세미나를 통해서 restricted Boltzmann machines (RBM) 과 Boltzmann machine의 구조를 정확히 이해 할 수 있었습니다. RBM은 각 레이어 의 unit 간의 연결이 없는 구조로 Visible layer 와 hidden layer 간의 weight를 정의한 energy function의 안정성과 이에 반비례하는 데이터의 확률 분포의 관계를 이해할 수 있었고 이에 대한 수식을 이해할 수 있는 경험이 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111172820]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[3]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현 : 새로운 모델 RBM에 대해 간략하게 배우는 시간이었다. Hidden layer의 수를 정한 후 가장 안전화 되는 bias와 weight 값을 찾는 RBM의 목적부터 시작하여 energy function, 확률 분포까지 알아보는 시간이었다. 각 layer의 노드끼리는 연결되어 있지 않고 독립적이라는 점이 매우 흥미로웠다. 앞으로 전개될 내용이 기대된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111172926]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[4]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[류나현 : RBM은 에너지 기반 확률모델로써 에너지 상태의 확률 분포인 볼츠만 분포가 적용된 neural network이며 input layer와 hidden layer로 구성된 unsupervised 학습 모델이다. input layer와 hidden layer는 undirected 가중치로 연결되어 있고 각 layer 내 node끼리도 연결되어 있다. 각 node는 binary이며 node마다 bias를 갖는다. 사실 유튜브 강의를 통해 RBM을 이해하는게 많이 어려웠는데 그럼에도 불구하고 이번 세미나를 준비해주어 RBM을 보다 잘 이해할 수 있었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173049]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[5]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동 : 이번 세미나 내용에는 증명해야 하는 수식이 두 개나 있었다. 꽤 길고 복잡한 증명과정을 포함하고 있었지만 발표자였던 김준홍 학우가 판서를 하며 차근차근 설명해줬다. 발표자의 판서를 따라 쓰며 미리 학습했던 내용을 다시 복습하니 이전보다 수식이 훨씬 명쾌하게 이해 되었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173122]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[7]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 금일 세미나에서는 RBM의 전반적 구조와 Energy function, bias와 weight 업데이트를 위한 기본 과정에 대해 다뤘다. Phase를 가장 안정적이게 하는 w, b, c를 찾는 것이 목적이라고 배웠다. 기존에 알고있던 deep learning과 구조적 차이가 있어서 이해하는데 어려움을 겪었지만, 세미나를 통해 들은 내용을 바탕으로 앞으로 공부하면 큰 도움이 될 것 같다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173200]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[8]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 오늘 세미나에서는 RBM(Restricted Boltzmann Machine)에 대한 세미나를 들었다.
RBM은 Boltzmann Machine의 각각의 Hidden layer와 Input layer 내의 node들이 연결되지 않은 network를 말한다.
통계물리학 기반의 energy function을 이용하여 input layer와 hidden layer에서의 확률값을 구할 수 있다.

기존의 Feedforward neural network로는 hidden layer의 개수가 많은 경우에는 가중치 학습이 어려운 단점이 있는데, 
RBM을 기반으로 하는 Deep belief Networks에서는 hidden layer의 개수가 많은 경우에도 각 층마다 Pre-training을 통해 비지도 학습을 
실행하고 Fine-tuning을 통해 마지막 단에서 backpropagation 알고리즘으로 훈련함으로써 deep learning에 유용하게 사용되는 방법이다.

발표자이신 준홍형이 p(h|x) 확률값에 대한 계산을 칠판에 증명하셔서 동영상 강의 내용을 복습할 수 있었고,
중간에 세미나를 준비하면서 궁금했었던 질문을 같이 생각해보면서 RBM에 대해서 좀더 생각할 수 있었고, 교수님의 설명을 통해서
feature extraction에서는 다른 non-linear 방법으로는 layer들 간의 연결이 안 되기 떄문에 RBM을 사용한다는 것을 알게 되었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173239]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[9]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 오늘 세미나에서는 RBM의 contrastive divergence에 대한 형석형의 세미나 발표를 들었다. 세미나는 저번시간에 했었던
RBM에 대한 내용을 복습하고 contrastive divergence에 대해서 진행되었다. 세미나를 통해서 stochastic gradient descent는
각각의 데이터에 대해서 gradient 값을 구하는데 순서에 따라 달라지기 때문에 확률적(stochastic)이고 gradient descent는
total data나 batch data같은 묶음에 대한 gradient descent 값을 구하기 때문에 stochastic 성질이 없는 것을 알게 되었다.

RBM을 학습시키는 과정에서 negative log-likelihood 값을 theta값에 대해서 편미분 하게 되고 input layer에 대한 정보가 있는
postive phase항과 input layer와 hidden layer를 모두에 대한 정보가 없는 negative phase 항으로 나뉘게 되는데
negative phase항의 확률을 구하기가 어렵기 때문에 임의의 초기값은 ~X 값이 주어졌을때 gibbs samling 방법을 이용하여
p(h|x)와 p(x|h) 계산을 반복적으로 함으로써 분포를 추정하고 parameter을 업데이트 하는 방법이 contrastive divergence이다.

혼자 공부하면서 gibbs samplingd에 대한 설명이 잘 안 와닿았었는데 교수님의 설명을 통해서 p(h=1|x)이 주어졌을때 이 값을
threshold로 하여서 hidden layer node 값들에 대해서 uniform random generation 값들과의 비교를 통해서 threshold 보다 작으면
1로 할당되고 크면 0으로 할당 된다는 사실을 알게 되었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173458]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[10]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동 : 오늘 김형석 학우의 발표 중 가장 먼저 생각나는 좋았던 점은 RBM의 정의를 자세히 알려준 것이다. 발표자는 세미나의 주학습자료인 Hugo Larochelle교수님의 강의에서 알려주지 않았던 Energy-Based Model이 무엇인지, 그리고 Restricted라는 용어가 의미하는 바를 그림을 이용해서 이해하기 쉽게 알려주었다. 발표 중간에 ‘Stochastic’의 의미를 교수님이 정확히 알려주기도 하였다. 또한 Gibbs Sampling이 실제로 어떻게 작동하는지 상세히 소개되었다. 종합해 보면 이번 발표에서는 RBM을 이해하기 위한 기초를 탄탄하게 쌓았다고 생각한다. 용어의 정의를 구체화 하고 자칫하면 ‘그렇구나’하고 넘어갈 수도 있었던, 하지만 RBM의 핵심인 Gibbs Sampling이 작동하는 자세한 방법까지 배웠다. RBM을 깊게 공부하기 위해서 꼭 짚고 넘겨야 할 기초를 익혔다는 점에서 유익한 세미나였다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173525]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[11]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 금일 세미나에서는 RBM에서 bias와 weight를 업데이트하는 절차에 대해 다뤘다. 층 간의 연결은 있지만 층 내에서의 연결을 제한함으로써 독립성을 얻어 식을 전개하는 것이 흥미로웠다. Gibbs sampling, CD(Contrastive Divergence)을 통해 업데이트가 진행되었는데, 과정을 직접 그려가며 설명을 들어서 기억에 더 잘 남는다. 수식을 더 깊게 파고들어서 숨겨진 내용을 익히는데 더 노력하기로 생각했다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173555]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[12]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[류나현 : 유튜브 강의에서 RBM의 샘플링이 쉽게 이해가 가지 않았는데 이번 세미나에서 RBM 학습 과정을 적절한 자료와 예시를 들어 설명해주어서 이론 및 관련 식을 직관적으로 이해하는데 많은 도움을 받았고 식들의 유기적 관계를 더 잘 이해할 수 있었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173618]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[13]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현 : 오늘 세미나에서는 RBM의 training 방식에 대해 배웠다. Negative log-likelihood를 최소화 하는 negative phase 추정에 필요한 Gibbs sampling에 대해 새로 알게 되어 유익한 시간이었다. 또한 CD-k에서 원래는 k값이 커야 함에도 불구하고 k값을 1로 맞춰주고 방향만 파악하더라도 잘 작동한다는 점에서 RBM은 흥미로운 모델인 것 같다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173645]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[14]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : RBM 학습 하는 방식인 contrastive divergence는 training example의 hidden unit 기대 값과 샘플링을 통한 model의 의한 hidden unit의 기대 값의 차이를 최소로 하는 방식을 이해 할 수 있었고 negative sample의 의한 h, v의 모든 경우의 수를 고려하기에는 계산적으로 어려운 점이 있으니 MCMC 메트로폴리스 방식의 특별한 케이스인 독립적인 gibbs sampling 방식을 RBM에 적용하여 파라미터 업데이트를 한다는 것을 이해할 수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173715]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[15]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : 오늘 세미나에서는 RBM의 Review와 Contrastive Divergence (이하 CD)에 대하여 다루었다. Gibbs sampling approximation에서 로직이 이해가 잘 되지 않았는데, 세미나 시간에 간단한 예제를 들어서 돌아가는 로직에 대하여 이해하였다. 
 개인적으로 Energy function에 대한 최적화가 Network의 성능과 이어진다는 부분이 신기하며, Contrastive Divergence에서 한번만 하더라도 초기 파라미터 셋팅에 크게 상관없이 파라미터를 찾아간다는 점이 흥미롭다. 관련 학습을 보충하여 찾아봐야 되겠다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111173750]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[16]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석 : 기존 supervised Neural network과는 완전 다른 방식의 접근인 RBM에 대한 정의와 그 원리를 잘 파악할 수 있었다.
실제로 RBM을 통해서 일종의 Feature Extraction효과를 얻을 수 있는데서 무척 흥미로웠다.
또한, Pre-training으로의 RBM의 역활과 그 원리에 대해서 이해 할수 있었다.
DBN에서의 RBM의 도입은 현재의 대 딥러닝시대가 도래하는 데 큰 역활을 했다고해도 과언이 아니라고 생각한다.
 필자 또한 지금의 딥러닝과 같은 미래의 학계에 어떠한 파급력을 끼칠 수 있는 Data scientist가 되고 싶다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111174103]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[19]]></uid>
		<content_uid><![CDATA[2]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 기존에 접하지 못했던 unsupervised 방식의 학습인 Restrict Boltzmann Machine에 대해서 공부하는 시간이었다. 김준홍 학우가 Boltzmann Machine과 Restrict Boltzmann Machine의 차이에 대해서 설명하고 Restrict Boltzmann Machine이 가지는 계산상의 이점이 나타나는 부분을 수식으로 보여주었다. 또한 교수님의 지도를 통하여 Restrict Boltzmann Machine이 pre-training으로 사용될 수 있다는 것, 또한 다른 비선형 Embedding 방법류는 왜 쓰일 수 없는지를 알려주시어 많은 생각할 거리를 얻은 세미나였다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111175735]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[20]]></uid>
		<content_uid><![CDATA[3]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 오늘 세미나에서는 Restrict Boltzmann Machine의 Training 방법에 대해서 공부하는 시간이었다. 먼저 Restrict Boltzmann Machine은 Generative model로서 고차원의 벡터를 잘 표현해내는 latent variable의 벡터를 생성해내는 데있어서 가장 잘표현해내는 bias와 weight의 배열을 학습방법이라는 것을 알 수 있었고, 모든 배열의 경우를 학습하는 경우에 있어서 computation time이 길어지기때문에 gibbs sampling을 응용한 Contrastive Divergence를 활용한다는 것을 알 수 있었고, 조금 놀라웠던 점은 Contrastive Divergence가 증명된 이론이 아닌 경험적으로 성능이 좋았기에 사용한다는 것이었다. 이를 통해 느낀점은 꼭 증명된 이론뿐만아니라 경험적으로 보장되는 이론도 가치가 있다는 점을 깨달을 수 있었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160111180232]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[21]]></uid>
		<content_uid><![CDATA[4]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 기존부터 진행되어오던 연구인만큼 새로운 아이디어를 발견해기 힘들었음에도 불구하고 EDA를 통해 몇몇 아이디어를 발전시킨 것이 인상적이었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114143529]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[22]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 교수님도 칭찬했다시피 발표자료에 사용된 도표가 직관적으로 DEA를 이해하는데 많은 도움을 주었다. 교수님이 연구 주제를 설명해주실 때 frontier line이 무엇인지 이해가 안됐었는데 도표를 보며 발표를 들으니 쉽게 받아들일 수 있었다. 하지만 DEA를 따로 공부하지 않았기 때문에 발표를 원활하게 따라가는데 어려움이 있었다. DEA는 산업공학의 핵심영역인 최적화의 한 가지 이기 때문에 이 기회를 빌려 DEA를 따로 공부해 놓으면 두고두고 유용하게 쓰이리라 기대된다. 다음 연구 세미나 때는 DEA를 공부해 가야겠다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114144629]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[23]]></uid>
		<content_uid><![CDATA[4]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 주어진 상황에서 여러 파생변수를 만들 때 논리적 결함이 없어야 된다는 점을 다시 상기하는 시간이었습니다. 또한 실험에 어플리케이션이 필요해서 개발했다는 얘기를 듣고 스케일에 놀랐습니다. Outlier 처리에도 그저 지표로만 접근하는 것이 아니라 논리적으로 타당한 처리를 한다는 생각이 들어 많이 배웠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114171124]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[24]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : clustering을 평가하는 글로벌 지표가 없는데 그것을 DEA를 기반으로 접근하는 것이 정말 기발하다고 생각했습니다. input(X)을 최소화 하고, output(Y)을 최대화 하여 효율적인 상황을 만드는 것을 기본 모티브로 생각하는 것이라 이해했습니다. 세미나 시간의 직관적 자료와 좋은 설명을 들은 후 추가로 발표자에게 주제에 대해 들으니 더 기억에 남습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114173243]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[25]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 발표 초반에 간단히라도 용어 설명을 잘 해주어서 DEA에 익숙하지 않았는데도 불구하고 전개되는 내용을 잘 따라갈 수 있었다. 또한 각 모형에서의 가능한 해의 영역을 그래프로 보여주어 직관적으로 이해할 수 있었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114173311]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[26]]></uid>
		<content_uid><![CDATA[4]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 이미 많이 진행된 연구이기는 하지만 문제를 쪼개서 구체화 하고 해결해 나가는 과정이 인상적이었다. 변수를 구체화 한 방법과 연구 수행 도중에 생기는 문제점에 대해서도 정의와 목적을 되짚으면서 (극단치에 대한 정의, 분석의 목적 등)고민하고 해결 방안을 찾아가는 과정이 인상 깊었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114173406]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[32]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : DEA의 개념을 정확히 정리하고 이를 설명하기위해 열심히 고민한 흔적이 보여서 고마웠습니다. 해당 설명시 좋은 예시들과, 알기쉽게 수식을 풀어 설명해준 덕분에 이해하는데 도움이 많이 되었습니다. 해당 연구의 목적은 객관성을 갖기위해 다양한 데이터에 Global한 performance measure index를 찾는것 이라고 이해하였습니다. 개인적으로 Input과 Output의 집합을 어떻게 구성하는지에 따라 변동이 생길수 있다고 생각하는데, 이에 대한 연구가 기대되고 기다려 집니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114191430]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[29]]></uid>
		<content_uid><![CDATA[7]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : 개인적으로, Co-Training을 수업시간이외에 사용해본 경험이 없어서, 집중하며 들었던 발표였습니다. PPT에서 보여준 결과는 Label 데이터의 비율이 10~80%로 갈때의 정확도가 점점 상승하는 상식적인 결과였는데, 해당 결과는 어떻게 나타날지, 선정한 데이터의 분야가 조금씩 다를수 있는데 각각을 Base model로 만든 결과와 제시한 해당 모형의 성능의 차이가 얼마나 나는지, 궁금해지는 연구였습니다. 
해당 연구의 프레임웍과 결과가 궁금하며, 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114185836]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[34]]></uid>
		<content_uid><![CDATA[4]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석: 이제 실제로 스마크폰을 통한 키스트로크 데이터를 수집하기 전 마무리 조정단계를 진행이 완료 됬음을 확인 하였다. 기존 키보드를 통한 실험의 피실험자로써, 스마트폰을 통한 데이터 수집이 쉽지 많은 않을 것이라 판단합니다. 랩실 구성원들과 함께 좋은 결과를 얻을수 있도록 많은 협조가 필요할 것입니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114211406]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[33]]></uid>
		<content_uid><![CDATA[6]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : 형석이가 진행하고 있는 ' Research Trend Analysis : Deep learning'의 발표에 대한 설명을 들을 수 있는 시간 이었습니다. 
먼저, 검색 키워드를 정의한후, {아카이브}와 {저널,컨퍼런스,리뷰}와의 트랜드 격차를 증명 하고자 하는데, 앞으로의 research framework과 그 결과가 궁금합니다. 현재, 개인적으로 딥러닝 학습 초기단계에 있는데, 해당 결과물을 보았을때, 한눈에 가독성 있는 딥러닝 트랜드를 산출해낼 것이라고 생각하며, 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114191655]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[36]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석: 처음 접해보는 DEA에 대하여 많은 준비를 해온것을 느낄수 있었으며, 도식화를 통해 직관적으로 개념을 잘 전달하였습니다. 이같은 도표를 활용한다면 논문에서도 원하는 바를 잘 전달할 수 있으리라 생각 됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114212354]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[37]]></uid>
		<content_uid><![CDATA[7]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석: 실제로 지난 학기에 비즈니스 어낼리틱스 수업시간에 배웠던 방법론들을 가지고 논문이 진행되는 상황애서 튜토리얼을 맡았던 semi-supervised 에 대한 부분이라 많이 관심을 가지고 지켜보고 있습니다. labeled Text-data를 확보하는 과정이 쉽지는 않았을텐데 수고가 많았을거라고 생각 됩니다. 관심을 가지고 지켜보며 떠오르는 의견들은 전달하여 좋은 결과가 나올수 있도록 기대하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114212417]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[38]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 사전 연구에서 찾은 자료와 앞으로의 실험 계획에 대해 표와 flow chart로 표현하여 한눈에 파악하기 쉬웠다. 다른 연구주제도 마찬가지겠지만 앞으로 연구를 진행하면서 실험이 정말 중요할 것 같다는 생각이 들었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114230804]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[39]]></uid>
		<content_uid><![CDATA[4]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[류나현: 연구 과정에서 드러날 수 있는 다양한 문제점을 고민하고 또한 드러나는 문제점을 해결하면서 새로운 아이디어를 구상하는 것을 보면서 많은 노력이 엿보였고 배울 점이라 생각한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114230831]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[40]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[류나현: DEA 개념을 이해하고 이를 정리하기 위해 노력한 것 같았고 그래프를 이용하여 직관직인 개념의 이해를 도왔다. 하지만 관련 용어들이 생소해서 그래프 출처가 되는 도서를 참고하고자 한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114233955]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[41]]></uid>
		<content_uid><![CDATA[6]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 연구에 필요한 키워드를 소개하기 전에, Nature에 소개된 Deep learning paper과 관련된 여러 키워드들과 그 트렌드에 대해 설명하여 더 관심 있게 볼 수 있는 발표였다. 딥러닝 관련 연구가 어떤 트랜드를 가질지 결과가 매우 기대되는 주제이다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160114234235]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[42]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : Regression Feature Selection 에 객관적인 결과를 위한 연구를 진행하는것으로 이해하였다. 개인적으로 Ridge Lasso Elastic Net에 대해서 정확히 알고 싶었는데 다음주부터 하나씩 발표함으로써 이해를 할 기회가 있을것 같다. 그리고 Nominal input variable도 고려, 속도와 각각의 Feature selection간의 객관적인 성능을 비교하는 결과가 기대된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160115010528]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[43]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : feature selection는 분석에서 빠질 수 없는 파트라고 생각합니다. 각 기법들이 여러 데이터 속에서 여러 지표를 통해 어떻게 보여질지 기대됩니다. 이번 기회를 통해 lass, ridge, elastic net, genetic algorithm까지 잘 정리하는 계기가 됐으면 합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160115013516]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[44]]></uid>
		<content_uid><![CDATA[6]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : deep learning의 트렌드를 알아보는 주제에 대한 설명을 들었습니다. 트렌드를 알고 미리 그 분야에 대해 공부하면 좋겠다는 생각을 했습니다. 또한 여러 keyword들을 보며 아직 알지 못하는 부분도 많다는 것을 느꼈고 각 분야를 정리하며 공부해야겠다 생각했습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160115013541]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[45]]></uid>
		<content_uid><![CDATA[6]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 앞서 다른 연구실원들이 코멘트 해준 것과 같이 이 발표를 통해 딥러닝의 주요 추세에 대해 유익한 정보를 얻었다. 이외에 추가로 제가 받은 인상은 연구가 굉장히 매끄럽게 진행되고 있다는 점이다. 연구에 쓸 키워드를 모두 정했고, 어느 웹사이트에서 어떤 방식으로 자료를 찾을지도 정했기 때문에 앞으로 연구 진행 방향이 명확해 보인다. 딥러닝에 대한 어떤 연구결과들을 찾아올지, 이번 팀의 다음 발표가 기대된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160116144558]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[46]]></uid>
		<content_uid><![CDATA[4]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: 만약 좋은 성능을 가지는 모델이 된다면은 비즈니스 측면에서 가장 필요한 기능이 아닐까 싶습니다. 수집되는 키스트로크의 데이터의 자체도 노이즈가 많을 것으로 예상이 되었는데 전처리가 모델의 성능을 높이는 데 중요한 요인일 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160116144714]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[47]]></uid>
		<content_uid><![CDATA[7]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 최근 따근따근한 분야인 텍스트에 대한 연구이기 때문에 앞으로의 결과가 매우 기대되는 연구이다. 관련문헌 탐색 결과 겹치는 연구가 없다니 다행 이었다. 하지만 다른 팀에 비해 앞으로 해야 할 실험과 작업이 많아 걱정이 된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160116145030]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[49]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: input 대비 output 의 양을 최대화 하는 방식으로 효율적인적인 측면으로 클러스터의 방식을 의사결정해주는 것 같습니다. 실험 설계를  하는 방식의 따라 좀 더 나은 방식으로 접근 할 수 있다고 생각됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160116145320]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[50]]></uid>
		<content_uid><![CDATA[6]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화:  Research Trend Analysis에서 가장 중요한 부분이 키워드 선정인 것 같습니다. 얼마나  양질의 데이터가 모이나에 따라 해석이 다양해 질 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160116145442]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[51]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 이 발표에서 가장 좋았던 부분은 장표 도중에 앞으로 연구계획을 플로우차트 형식으로 일목요연하게 정리해준 점이다. 앞으로 연구가 어떻게 진행될지 뿐만 아니라 이 연구가 목표로 하는 바가 무엇인지 까지 말끔하게 정리되는 느낌 이었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160116145601]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[52]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: 변수선택의 방식을 연구하는 끝판왕으로 생각됩니다. 이 주제에서 발표되는 여러 변수선택 과정에서 많은 것을 배울 수 있을 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160116145639]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[53]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : ridge, lasso, elastic net은 학부시절 데이터마이닝 수업때 잠깐 들었던 개념이었는데 연구주제에 올라온 것을 보면서 다시 흥미를 가지게 되었습니다. 이번 실험을 통해서 실제 데이터에서 multicollinearity문제 해결과 variable selection의 성능이 기존의 linear regression에 비해서 얼만큼 차이가 나는지 알고싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160117152721]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[54]]></uid>
		<content_uid><![CDATA[7]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 지난학기 수업에서 배웠던 co-training을 LDA와 Doc2Vec을 이용해서 multiclass에 대해서 classification하는 실험이라 듣게 되었다. 필요한 데이터를 찾는과정과 많은 실험들이 걱정되지만 선행연구가 없기에 희소성있고 가치있는 연구가 될것이라는 기대가 된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160117154528]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[56]]></uid>
		<content_uid><![CDATA[5]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : clustering algorithm을 evaluate하고 optimal solution을 찾는 방법에서 DEA라는 개념을 가지고 사용한다는 것이 인상적이었다. 전에 경영과학/OR쪽 수업을 들어본적이 없어서 연구주제에 대해서 이해하기 어려웠었는데 발표를 들으면서 DEA가 어떤 개념인지 알게 되었고 앞으로의 연구결과가 기대 됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160117160244]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[57]]></uid>
		<content_uid><![CDATA[4]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 :  지난학기 교수님 수업에서 알게 되었고 keystroke.kr 에서 직접 해보면서 관심을 가지게 된 연구주제이다. 발표에서 연구를 하면서 생기는 문제점에 대해서 고민을 많이 한 점이 좋았고 스마트폰상에서도 keystroke로 사용자 분류가 가능한 결과가 나올지 기대된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160117160808]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[58]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 :  금일 세미나에서는 persistent CD와 그와 관련된 markov chain monte carlo 등의 발표 내용을 들었습니다. MCMC 에서 gibbs sampling이외에도 metropolis, metropolis-hastings 방법이 있다는 것을 알게 되었습니다. 그리고  CD-K와 Persistent CD의 차이를 명확히 알게 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160118203238]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[59]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 오늘 세미나에서는 markov chain monte carlo기법과 persistent CD에 대해 배웠습니다. 각각의 개념에 대해 잘 이해할 수 있었던 발표였고 적절한 예시와 연구 동향까지 알게 되어 더 흥미로웠습니다. CD-K와 persistent CD-K의 차이점을 명확하게 이해하는 데 다소 어려움이 있었지만 어려운 만큼 더 고민을 많이 해봐야겠다고 느끼게 되는 세미나였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160118234717]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[60]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 세미나를 통해 MCMC의 개념을 듣고 예시를 통해 이해하였습니다. 많이 활용되는 기법이기에 미리 공부를 해둔다면 다른 공부를 할 때 좀더 수월할 것 같다는 생각이 들었습니다. 그리고 CD-K와 persistant CD에 관한 부분에서는 gibbs sampling 결과를 다시 쓰는 차이가 있다고 말씀해 주셨습니다. 아직 완벽히 이해가 안된 부분이라 좀 더 고민해봐야겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160119103448]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[61]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석: 오늘 다룬 주제는 persistent CD와 기존 CD-k 의 차이를 비교하며 왜 persistent CD가  좀더 글로벌 optimal에 가까운지를 이해 하였습니다. 특히 이러한 CD-k의 gibbs_sampling 과정에서 MCMC란 어떠한 방법이며 예시를 통해 이해를 도울수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160119123236]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[62]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : 이번 세미나에서 MCMC에 대하여 좀더 다세히 다루었다. 중간에 1:1방식으로 Mapping되어 있는 암호를 해독하는 예제를 준비하였는데 중간에 이해가 되지 않아서 세미나가 끝난후 발표자인 동화와 토론 끝에 기존 방식을 이해하게 되었고 더불어 한가지 방식을 더 생각할수 있게 되었다. 아직 최적화 방식에 걸음마 단계라, 하나씩 배우고 있지만, 이를 어디에 적용시키는 아이디어와 발상도 중요하다고 느끼는 세미나였다. 중간에 Persistant CD와 CD-k에 차이에 대하여 교수님께서 설명과 동시에 짚고 넘어가 주셨다. 말씀하신바와 같이 Persistant CD의 경우 Memory상 크게 증가하는 부분이 아니기 때문에 실제적으로 사용해 볼 수 있는 방법이라고 생각했다. 아직 큰 데이터를 사용하여 Deep neural network를 핸들링 해본 경험이 없는데, 해당 기회가 빨리 온다면 이것저것 실험해 보면서 실제 상황에서의 시간, 예측력을 탐색해보고 싶다는 생각이 들었던 세미나 였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160121003607]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[63]]></uid>
		<content_uid><![CDATA[6]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 논문 세미나가아닌 딥러닝 세미나에서 현재 강의를 듣고 있는 것에서 발전된 딥러닝 알고리즘이 무엇이 있는 지 개략적으로나마 확인할 수 있어서 좋았고 실험에 대한 계획이 잡혀가는 것이 보여 다음 발표가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160121011353]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[64]]></uid>
		<content_uid><![CDATA[7]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : semi supervised learning을 평가하는 방식에 대해서 알 수 있어서 좋은 발표였다고 생각합니다. 이 팀의 실험계획이 가장 어려울 듯 한데, 어떠한 방식으로 실험계획을 수립해올 지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160121011654]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[65]]></uid>
		<content_uid><![CDATA[8]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : ridge, lasso, elastic net에 대해서 개략적으로 나마 어느정도 공부하여 그 원리는 알고 있었고 딥러닝 세미나를 통해서 regularizer로서 같이 토의했던 적도 있었기 때문에, 위 방법론을 어떤식으로 다음 시간에 설명할 지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160121011859]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[66]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 이 번 세미나를 통해서 MCMC에 대해서 개략적으로 나마 이해하게되어서 좋았습니다. 또한 전 학기 수업시간에 배운 Markov Chain이 정말 여러 곳에서 쓰이고 있구나라는 것도 느낄 수 있었고 개인적으로 조금 아쉬웠던 점은 MCMC에 대한 수식적 증명이 장표에 실려있지 않았다는 것이 조금 아쉽습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160121012314]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[67]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: Hugo Larochelle교수님의 강의에는 없었던 MCMC를 소개해줘서 좋았습니다. Larochelle의 강의만 듣고는 MCMC가 어떻게 사용되는지 알 수 없었지만 김동화 학우의 발표로 인해 MCMC가 RBM의 배경에서 사용되는 이유를 짐작하였습니다. RBM을 이해하기 위한 기본 배경을 학습했다는 점에서 매우 유익한 발표 였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160121221916]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[68]]></uid>
		<content_uid><![CDATA[10]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : KDA관련 논문에서의 연구의 가설을 먼저 언급하고 세부적인 논문 내용을 들으니 보다 더 이해가 잘 됐습니다. 또한 KDA의 전통적 방식, 실험절차를 그림과 함께 설명한 점도 이해를 도왔다 생각합니다. 일반적으로 많이 접하는 transformation이 아닌 rank transformation을 배웠는데, 패턴을 보다 확실하게 만들어주는 점, 이상치를 패턴에서 멀지 않게 만들어주는 점이 특징이었습니다. 구성원 모두 실험에 대해 늘 고민하고 여러 방향으로 접근한다는 느낌을 받았습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160121233911]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[71]]></uid>
		<content_uid><![CDATA[10]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석: 앞선 발표에 대해서는 키스트로크에 대한 최근 동향을 잘 파악할수 있었으며, 향후 진행될 실험에 대해서 디테일 한부분까지 고려 하였음을 확인할 수 있었습니다. 실제 상용화에 대한 기대감을 높여주었으며 앞으로 실험에서 좋은 결과를 통해 상용화에 좋은 밑거름이 되었으면 합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160122181128]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[72]]></uid>
		<content_uid><![CDATA[11]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석:  실험설계를 매우 구체적으로 잘 수립해온 것 같았으며, 앞으로 실험이 쉽지 않을 것 같다고 생각합니다. 다양한 방법론들을 활용하는 실험이기에 많은 컴퓨팅 시간이 소유 되며, 이에 따른 많은 수고가 필요할거라 생각됩니다. 파이팅 입니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160122181509]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[70]]></uid>
		<content_uid><![CDATA[10]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 첫번째 논문 조사에서는 language context와 preprocessing으로 keystroke 성능을 개선한 사례와 연구주제와는 상관없지만 keystroke 을 활용하여 적용할 수 있는 분야들에 대한 소개를 들었습니다. 논문소개에서 핵심문장을 한줄을 나타내고 그에 대한 목표들을 간략하게 설명한 것이 좋았습니다. 두번째 발표에서는 실험에 필요한 안드로이드 앱의 개선사항에 대한 발표를 들었습니다. 실험설계에 대한 내용이 탄탄하여 앞으로의 연구가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160122174748]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[75]]></uid>
		<content_uid><![CDATA[10]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: KDA와 관련된 논문을 이해하기 쉽게 잘 설명해 주었고 발표자가 공부하면서 들었던 의문까지 제시하여 다방면에서 생각할 수 있었습니다. 또한, 진행하고 있는 논문과 직접적으로 관련 있지 않는 내용까지 추가적으로 설명하여 더욱더 관심 있게 들을 수 있었습니다. 논문의 목적에 맞게 구체적으로 실험을 설계한 부분이 인상적이었고 결과가 매우 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160122234519]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[76]]></uid>
		<content_uid><![CDATA[9]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[류나현: 우선 예시를 통해서 MCMC에 대해 자세히 알 수 있는 시간이어서 좋았습니다. 그리고 강의만으로는  명확히 이해하는데에 다소 어려움이 있었는데 세미나를 통해 많은 도움을 받았습니다.   CD-k와 PCD의 차이를 배웠는데 실제 어떻게 작동하는지 좀 더 구체적으로 고민해봐야 더 잘 이해할 수 있을 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160123020845]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[74]]></uid>
		<content_uid><![CDATA[11]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : 세미나에서 실험 경우의수를 엑셀로 정리하여 알려주었습니다. 해당 경우의수를 예상해 보니 약 7,000회의 실험이 필요한것으로 개인적으로 계산이 되었는데, 이를 세미나에서 현실적으로 줄였고, Term의 갯수가 세미나의 화두였었는데, 그대로 사용할시의 경과가 궁급합니다.앞으로의 실험이 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160122193027]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[77]]></uid>
		<content_uid><![CDATA[11]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 실험 계획을 구체적으로 짜왔기 때문에 교수님으로 부터 유익한 피드백도 많이 받지 않았나, 생각합니다. 수행해야 할 실험의 경우의 수를 많이 줄였음에도 불구하고 해야할 작업이 많기 때문에 우려가 되며 응원을 해주고 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160123193049]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[78]]></uid>
		<content_uid><![CDATA[14]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: Research Treand Analysis 팀은 이번 발표에 의하면 이미 데이터 수집이 끝난 단계이다. 다음 연구 세미나때는 프로토타입의 연구결과를 볼 수 있을것이라 기대한다. 딥러닝 연구의 흐름이 어떻게 흘러가고 있을지 결과가 매우 기다려진다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160123193247]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[79]]></uid>
		<content_uid><![CDATA[14]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 관련 내용을 크롤링하고, 데이터 전처리 한 내용을 중점으로 들었습니다. 데이터 전처리 과정 전반에 걸쳐 디테일에 많은 신경을 쓴 것을 느낄 수 있었습니다. 제가 생각한 것보다 deep learning 관련 논문이 많지 않다는 느낌을 받았는데, 이런 궁금증 또한 이번 주제를 통해 해결될 것이라 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160124212259]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[80]]></uid>
		<content_uid><![CDATA[11]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 연구세미나를 통해서 여러 연구가 진행되고 있는데 개인적으로는 굉장한 의지력이 필요한 것 같습니다. Excel sheet를 통해 구체적으로 눈으로 필요한 실험의 수를 확인시켜준 점이 좋았다고 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160124225154]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[81]]></uid>
		<content_uid><![CDATA[10]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 개인적으로 KDA라는 분야가 굉장히 생소했던 만큼 KDA에서 쓰이는 용어들이 사실상 개인적으로는 정립이 안되고 있었는데 김해동 학우의 발표를 통해서 KDA에서 쓰이는 기본 변수들과 개념을 이해할 수 있어서 좋았습니다. 또한 개인적으로 김해동 학우의 논문을 읽어내고 중요한 점을 캐치하는 능력은 본 받아야겠다고 생각하였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160124225403]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[82]]></uid>
		<content_uid><![CDATA[14]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 진행과정을 가볍게 summary한 느낌의 발표였으며 진행상황이 매우 순조롭게 흘러가고 있다고 판단됩니다. 해당 연구를 직접 참여하지는 않고 있지만 후에 같은 연구를 진행하게되면 어떤식으로 진행해야되고 데이터를 수집해야하는지 파악할 수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160124225626]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[86]]></uid>
		<content_uid><![CDATA[15]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 연구에 대한 계획과 중간결과 뿐만 아니라 클러스터링 기법에 대해 전반적인 설명이 같이 수반되어서 참 좋았다. 데이터 연구에서 클러스터링 기법은 빠질 수 없는 감초이다. 그런 방법론에 대해 새로운 연구의 진행과정을 보면서 영감도 얻고 클러스터링의 직접적인 지식도 얻어 일거양득 이었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160125205958]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[87]]></uid>
		<content_uid><![CDATA[16]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 나현이의 발표력이 급상승 하는 모습을 현장에서 목격하여 참 좋았다. Ridge Regression은 Deep Learning 세미나 바로 이전, 시계열분석을 주제로 진행한 세미나에서 배웠음에도 불구하고 아직 이해가 잘 안되는 부분이 많다. 다시 과거에 했던 세미나 자료를 들춰보며 복습을 해야겠다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160125210406]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[88]]></uid>
		<content_uid><![CDATA[16]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 :  통계학의 estimate의 bias, variance decomposition에 대해서 알고 있었기 때문에 이해가 어렵지 않았습니다. Ridge regression, L1 regularizer가 linear regression에 적용된 형태로 Ridge 뿐만아니라 L1 regularizer의 기능이 무엇인지 알 수 있는 설명이었다고 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160126000853]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[89]]></uid>
		<content_uid><![CDATA[16]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 :  나현이의 Ridge regression에 대한 발표를 들었습니다. 개인적으로 Regression의 Ridge Lasso 에 대하여 자세히 들어보고 싶었는데 좋은 기회였습니다. 재미있는 부분은 Unbiased estimator보다 biased estimator가 더 좋은 모델이 될 수 있다는 점이 재미있었습니다.
다음번에 R을 이용한 Function을 직접 생성하여 실험을 진행 하기로 하였는데, 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160126003356]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[92]]></uid>
		<content_uid><![CDATA[15]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : 클러스터의 성능지표에 대한 정리와 Input과 Output에 들어가지 못하는 지표들의 대한 이유를 설명하여 앞으로의 방향을 서술하였습니다. 개인적으로, 인공데이터와 실제 데이터의 각 개별적 지표의 결과와 조합한 결과의 성능이 궁금하며, 결과가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160126003842]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[91]]></uid>
		<content_uid><![CDATA[14]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : Search Keyword 집합을 구축하고 이미 크롤링을 통해 데이터를 수집한 상태라는 것을 발표하였습니다 .그리고 앞으로 Frequency Analysis, Network Centrality, Topic Modeling, 등을 행할 것인데, 산출물에 대한 Deep learning에 Domain Knowledge를 결합시켜 서술할 결과물이 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160126003627]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[93]]></uid>
		<content_uid><![CDATA[16]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : Ridge regression에 관한 발표를 들었습니다. 회귀계수를 shrinking 하게 하지만 완전히 0으로 보내지는 않아서 완전 소거를 하지 않는 것을 보았습니다. 제약식의 형태가 sum[(beta_i)^2]으로 원형의 형태라서 그런 것이 아닐까라는 생각을 해보았습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160127102936]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[94]]></uid>
		<content_uid><![CDATA[17]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : RBM보다 Autoencoder의 계산이 더 쉬운 장점이 있음을 수식 증명을 통해 이해할 수 있는 시간이었습니다. Overcomplete의 경우 확률을 주어 노이즈를 추가하는 방식인 denoising 방식과 데이터 내의 노이즈에 대해서만 고려하는 contractive 방식에 대해 그림으로 설명을 들었습니다. 특히 contractive autoencoder 방식은 잘 이해되지 않은 개념이었는데 깨우치게 되는 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160127103442]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[95]]></uid>
		<content_uid><![CDATA[17]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: Hugo Larochelle교수의 강의 만으로는 Autoencoder의 내용을 소화하는데 무리가 있었지만 김보섭 학우의 발표로 내용을 이해하는데 큰 도움이 되었다. 가장 인상 깊었던 점은 Denoising  Autoencoder와 Contractive Autoencoder를 Graphical하게 설명한 부분을 이해하기 쉽게 알려준 부분이다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160127223005]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[96]]></uid>
		<content_uid><![CDATA[11]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 뉴스기사, 스팸메일 데이터, 의료 데이터 등의 다양한 도메인의 텍스트 데이터를 가지고 와서 기대가 됩니다. 실험량이 많을것 같아서 걱정 되지만 잘 해내리라 믿습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160127233859]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[97]]></uid>
		<content_uid><![CDATA[15]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : Clustering 성능 지표에서 elbow 이외에 Max나 Min 값을 갖는 지표들을 사용한다고 들었습니다. 연구계획이나 데이터 수집 계획이 좋았습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160127234302]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[98]]></uid>
		<content_uid><![CDATA[16]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : ridge regression의 B계수에 대한 유도식을 증명해 주어서 쉽게 이해 할 수 있었습니다. 다음번에는 다양한 데이터에 대한 실험결과들을 보고 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160127234530]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[99]]></uid>
		<content_uid><![CDATA[17]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 지난 학기 수업 발표에서 들었던 Autoencoder에 대해서 다시 상기 할 수 있어서 좋았습니다. Linear autoencoder , undercomplete 방법들을 새롭게 알 수 있었고, overcomplete 에서는 compressed가 되지 않는데 denoising 을 하여 autoencoder를 만든다는 것을 알게 되었습니다. 발표자가 이해하기 쉽게 설명하여 만족스러운 세미나 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160127235438]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[100]]></uid>
		<content_uid><![CDATA[10]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: 키스로크에 대한 전반적인 이해를 돕는 literature review가 돋보였습니다. 해당 논문에 가치는 어떠한 변수를 선택을 할 것 인지 아니면 새로운 알고리즘을 사용할 것인지가 중요한 쟁점이 될 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128173858]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[101]]></uid>
		<content_uid><![CDATA[14]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: 딥러닝 대한 궁금한 점들이 있었는데 그것에 답을 찾는 재미있는 논문이 될 것같습니다. 어떤 키워드를 사용하는냐에 따라 텍스트의 질이 다양할 수 있고 다양하다는 것은 새로운 결과를 뽑아내는 필수조건이라고 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128174307]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[102]]></uid>
		<content_uid><![CDATA[16]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: 리지 리그레션에 대한 이해를 도울 수 잇었고 시각적으로 베타가 어떻게 찾아가는 지 이해를 돕는 세미나가 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128174422]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[103]]></uid>
		<content_uid><![CDATA[15]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: 최적의 효율을 가지는 measurement 를 찾는 것을 평가하기 위한 여러 데이터 셋을 확보하는 것 이 중요한 것 같습니다. 알고리즘을 항상 그때그때 맞쳐가야  했다면 효울적인 방식으로 해를 찾아가는 좋은 연구가 될 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128174719]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[104]]></uid>
		<content_uid><![CDATA[17]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화: 오토 인코더란 비지도학습으로 원래 값을 다시 바꿀수 있는 잠재적 변수를 찾지 방법이라고 생각합니다. 이러한 오토 인코더 학습 방식에서 잠재변수는 많이 할 수록 성능이 좋아지지만  over-complete 될 경우 과적합에 경우가 있어 실증적인 에러가 커질수 있습니다,  어러한 문제점에 대한 학습방식에서 어느정도 노이즈를 추가해 좀더 robust한 모델을 만들수 있으며, 또 다른 방법은 contrast 방식으로 x가 변할때 h잠재변수가 얼마 변하는지에 대한 편미분방식으로 학습하는 방식이 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128175256]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[105]]></uid>
		<content_uid><![CDATA[18]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 오늘 세미나에서는 텍스트 데이터에 대한 Lemmatization 방법들과 결과값들을 확인할 수 있었습니다. 제가 진행하는 연구에서도 사용할 수 있다는 것을 배우게 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128205015]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[106]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 오늘 세미나에서는 다양한 Clustering algorithm에 대한 소개를 들었습니다. 예전에 관심을 가지다가 공부를 오랫동안 못하였는데 어떤 clustering algorithm들이 있는지 상기하는 계기가 되었습니다. 인공데이터셋에서  clustering이 잘 안될 위험이 있는데 새로 바뀐 데이터셋에서는 어떠한 결과가 나올지 기대가 됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128205348]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[111]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 여러 종류의 clustering algorithm에 관해 들을 수 있는 시간이었습니다. 처음 들어보는 이름의 K-medoids를 포함하여 DBSCAN을 예로 들은 Density-based clustering, Grid-based clustering까지, 알지 못했던 clustering 기법들을 소개받은 느낌을 받았습니다. 세미나 중 들은 내용을 토대로 공부한다면 금방 습득할 수 있을 것이라 생각합니다. 이제 실험이 시작될텐데, 머지않아 global index가 나올 현장을 기대해 봅니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160129000551]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[108]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 금일 세미나에서는 발표자가  각 case 별로 data를 생성하기 위해 노력한 모습을 알 수 있었습니다. 프로젝트에 사용할 수 있는 알고리즘을 소개한 후 차차 근거를 제시하며 하나하나 소거하였습니다.그 결과 clustering알고리즘의 특징과 장, 단점에 대해 간략히 알게 되어 좋은 기회였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128233946]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[109]]></uid>
		<content_uid><![CDATA[18]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 바로 지난 코멘트에서도 언급했다시피 이 팀은 방대한 실험 때문에 걱정이 되었지만 오늘 발표 결과를 보니 걱정은 기우에 그칠 듯 하다. 일주일이란 짧다면 짧은 기간 동안 두 가지 언어로 여러 툴을 사용하여 가장 적합한 방법을 찾아낸 점이 인상 깊었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128234030]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[110]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 학부에서 개설되는 데이터마이닝 수업에서는 클러스터링 기법 중 가장 간단한 k-means와 hierarchical, 두 가지만 배웠기 때문에 알고리즘 선택과 튜닝에 대해서 조언을 해주긴 힘들었다. 대신 클러스러팅에 대해 유용한 지식들을 되려 얻을 수 있었다. 특히 density-based model은 처음 접하는 기법 임에도 불구하고 친절한 설명으로 쉽게 이해하였다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160128234124]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[112]]></uid>
		<content_uid><![CDATA[18]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : lemmatization의 시간상의 문제 때문에 사간이 다소 소요되었지만, 앞으로의 살험 계획은 팀원 모두가 doc2vec 컴퓨터 환경이 갖춰진것으로 알고 있기 때문에 실험에 차질이 없을것으로 예상되어 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160129001530]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[113]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김준홍 : 여러가지의 뿌리로 나누어진 clustering 기법을 정리하여 알기 쉽게 설명해 주었다. 앞으로의 프레임웍을 계획한대로 나온 결과가 궁급합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160129001722]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[114]]></uid>
		<content_uid><![CDATA[20]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 김준홍 학우의 데이터를 파는 끈기는 정말 본받을만한 점이라고 생각합니다. KDA 세미나  발표를 들으면서 이제 어느정도 KDA에 대한 개념이 정리가 되어 다음 세미나때부터는 코멘트를 줄수 있도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130111655]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[115]]></uid>
		<content_uid><![CDATA[22]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 기본적으로 연구실험결과에 대한 표를 쓰는 것은 경험이 없어 저에게도 어려운 일이었습니다. 오늘 세미나에서는 해당 실험결과를 정리하는것에 대해서 생각해볼 수 있는 세미나였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130111815]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[116]]></uid>
		<content_uid><![CDATA[18]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : 텍스트마이닝에 대해서 접해본바는 있으나 제대로 공부해본적이 없기때문에 생소한 개념이 있었지만 대략적으로 실험의 설계는 이해하기쉬웠습니다. 그리고 향후 텍스트마이닝을 연구하게 된다면 쓸만한 스킬들을 알게되어서 유익한 세미나였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130112603]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[117]]></uid>
		<content_uid><![CDATA[22]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : Lasso의 제약식이 사각형 모양이어서 뾰족한부분(축)에 LSE가 타원형으로 퍼지면서 닿아서 변수가 0으로 되어 변수선택 효과가 있음을 확인했습니다. 또한 실험 결과를 보여줄때는 data이름, 변수 갯수 등의 설명과 결과물(error, 계산시간, 등)을 싣는 것임을 상기하는 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130151058]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[118]]></uid>
		<content_uid><![CDATA[20]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 실험에 대해 끊임없이 고민하고 어떻게 하면 더 좋은 결과를 얻을 수 있을지 여러 방면으로 생각하고, 어떤 예외도 놓치지 않으려고 하는 의지를 볼 수 있었습니다. 또한 outlier로 생각할 수 있는 부분에 대해서도 깊게 고민하여 새로운 접근을 하려는 모습을 볼 수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130151330]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[119]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 여러 클러스터 예시에 적합한 measure가 나올지 기대가 됩니다. 개인적으로 너무 복잡한 클러스터 일 경우에는 이를 만족하는 해를 찾을 수 있을지 의문점이 생깁니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130161825]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[120]]></uid>
		<content_uid><![CDATA[20]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 키스트로크를 bigraph 방식으로  어휘 분석에 2-gram 방식과 유사하게 접근하는 것 같습니다. 어휘분석에 skip-gram 있는 것처럼 키스토로크를 딥러닝의 skip-gram의 형태로 구현 할 수 있을 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130162137]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[121]]></uid>
		<content_uid><![CDATA[19]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석 : 다양한 clustering 방법론에 대해서 전반적으로 가볍게 알아가는 좋은 시간이였으며, DBSCAN등의 생소한 방법론에 대해서 개인적으로 알아가는 동기부여가 될수 있는 좋은 시간 이였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130162404]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[122]]></uid>
		<content_uid><![CDATA[22]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 가장 통계에 기초가 되는 변수 선택에 ridge 와 lasso 는 가장 중요한 개념인것 같습니다. 일반적으로 모든 변수를 넣어서 분석하는 것이 좋다고 생각하지만 시간과 비용을 고려 헀을 때 실제로 그렇지 않으므로 본 연구는 유용한 정보를  제공할 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130162423]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[123]]></uid>
		<content_uid><![CDATA[23]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 딥러닝에 관심이 있고 이 분야에 어떤 저자가 어떤 국가에서 활발히 연구 되고 있는지 시각적으로 알수 있었던 시간이 된것 같습니다. 이러한 정보를 어떻게 네트워크로 표현될지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130162541]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[124]]></uid>
		<content_uid><![CDATA[20]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석: 저역시도 사용자의 Outlier 키스트로크 패턴에 대해서 이상치인지 혹은 개인의 특성인지에 대한 고민이 많이 되고 있습니다. Outlier까지 고려하며 최대한 그 variance를 높이면서 분류의 성능을 높이는 방안에 대해서 저도 계속해서 고찰해보도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130162924]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[125]]></uid>
		<content_uid><![CDATA[22]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석 : 기존에 사용하는 패키지에 준하는 coding을 구현하는 일은 매우 어렵다는 것은 모두들 잘 알고 있습니다. 다양한 예외처리며 효율적인 연산 등 많은 것들을 고려해야 하기 때문입니다. 실험계획이 좀더 구체적으로 설계되어 진다면 코딩 또한 좀더 효율적으로 구성되어 실험이 원할하게 진행될거라 생각됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130163339]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[126]]></uid>
		<content_uid><![CDATA[22]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: LASSO와 RIDGE를 비교 설명한 부분은 좋았지만 아쉬운 점은 실험결과를 보여준 부분 이었다. 교수님도 코멘트 해주신 대로, 실험결과가 나타내고자 하는 바가 명확하지 않았고, 거기에 필요한 정보들이 누락되어 있었다. 한 주 동안 열심히 실험을 하는 모습을 연구실 지근거리에서 지켜봤는데, 그 노력의 결과를 올곧이 담아내지 못해서 아쉬웠다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130174027]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[127]]></uid>
		<content_uid><![CDATA[23]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 중국이 딥러닝 분야에서 굴기하고 있는 모습이 매우 인상깊었다. 이제 중국의 힘이 미치지 않는 분야가 없는듯 하다. 국가별, 연구 기관별, 그리고 연도별과 같이 범주에 따른 여러가지 결과가 있었다. 이러한 결과들을 한 번에 일목요연하게 보여줄 수 있는 시각적 표현방법을 고민해보고 구현한다면 연구결과의 품과 격을 높이는데 큰 도움이 될 것 같다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160130174049]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[128]]></uid>
		<content_uid><![CDATA[20]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 이번주에 예비 실험자로서 keystroke 실험에 참여해 보았습니다. 문장이 길고 소요시간이 길어서 사람들이 지칠 가능성이 크다고 느꼈고, 컴퓨터 실험과 비교해보았을때 화면이 작고 타이핑이 어렵기 때문에 분석하기 어려운 데이터가 나올수도 있을것이라 생각합니다.  어떤 과정을 통해서 의미있는 실험 결과가 나올지 기대가 됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160131230040]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[129]]></uid>
		<content_uid><![CDATA[22]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : Lasso에 대해서 배울 수 있었고, ridge와의 차이점을 알 수 있어서 좋았습니다. 교수님이 말씀해주신 변수선택법에 대한 비교기준 표가 명확히 제시된다면 훌륭한 연구가 될것이라 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160131230317]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[130]]></uid>
		<content_uid><![CDATA[20]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 논문이 많이 진행된 만큼 좀더 깊게 심화하여 실험을 진행시키려는 모습을 볼 수 있었습니다. Outlier에 대해 진지하게 고민하는 모습이 인상 깊었습니다. 결과가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201090525]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[131]]></uid>
		<content_uid><![CDATA[22]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: Ridge와 Lasso에 대한 기본개념을 정리하고 가는 좋은 기회였습니다. 진행과정은 쉽지 않지만 연구를 마무리 할 때 결과를 명확하게 제시한다면 유용한 연구자료가 될 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201090945]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[132]]></uid>
		<content_uid><![CDATA[23]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: data를 모으는 과정에서 논문의 저자가 겹치는 등의 복잡하게 얽힌 문제를 차근차근 해결해 나가는 모습을 볼 수 있는 연구였습니다. 평소 논문을 찾을 때 중국에서 쓴 논문이 많은 것을 어렴풋이 느끼고 있었지만 이렇게 직접 확인하게 되니 놀라웠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201091326]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[133]]></uid>
		<content_uid><![CDATA[17]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 발표자가 수식의 의미를 하나하나 오토인코더의 구조와 연결시켜 설명하여 이해하기 수월했습니다.  또한  denoising과 contractive  오토인코더의 장, 단점과 특징을 한눈에 알아보도록 깔끔하게 정리하여 많은 도움이 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201091759]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[134]]></uid>
		<content_uid><![CDATA[24]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김형석 : 이번시간에는 딥러닝의 한계점과 이를 극복하기 위한 방법론을 위주로 다루는 시간을 가져보았습니다. pre-trainging 단계에서 data-representation의 강점을 다시 한번 생각해볼수 있었으며, Dropout이라는 간단명료하지만, 이러한 approach가 가지는 효과가 매우 인상적이였습니다. Dropout의 approximation에 관해서는 스스로 한번 더 알아보구 짚고 넘어가도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201214050]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[135]]></uid>
		<content_uid><![CDATA[24]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[서덕성 : 금일 세미나에서는 “distributed representation”의 개념을 먼저 잡았습니다. Deep learning 전까지의 이미지 분석에서는 중요한 feature를 사람이 설정한 후 분석하였지만 Deep learning의 경우 구성된 network가 자동으로 연속형의 가능한 feature를 뽑아내어 분석하는, raw data 그대로 입력 가능함에 대한 장점을 배웠습니다. 점, 선의 저차원의 feature로 시작해서 조합이 되어 고차원의 feature를 각 unit이 잡아내는 것을 사람의 인지 구조와 비교하며 개념을 상기하는 시간이었습니다. 또한, 이제까지 배웠던 pre-training(RBM, Auto Encoder)의 층층이 쌓이는 구성과 label이 필요 없기 때문에 가용한 모든 데이터를 pre-training에 사용 가능하다는 장점이 있음을 알 수 있는 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201215518]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[136]]></uid>
		<content_uid><![CDATA[24]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김해동: 세미나에서 간과하고 넘어갔지만 매우 중요한 개념, 또는 용어인 'distributed representation'을 명확히 하고 넘어갔다. 강의 가장 처 부분에 나오는 내용 이지만 이미 deep learning의 의의를 '안다고' 착각하고 있었기 때문에 대수롭지 않게 넘어갔다. layer를 다층으로 쌓았고 인간의 뇌의 작동 방식을 모사한다는 내용등을 안다고 했지만 그것이 ANN에서 구현 되었을때 우리에게 주는 시사점을 몰랐지만 어설프게 안다고 생각하였다. 이번 세미나가 아니였다면 옥석을 옆에 두고 진흙에 묻어둘뻔 하였다. 각 layer가 연속형의 재현이고 ANN이 중요한 feature를 사람이 아닌 network가 뽑아준다는 점이 deep learning이 이전 machine learning algorithms과 가장 큰 차이라는 점은 앞으로 ANN을 단순히 이용할때가 아니라 연구하고 개선하고자 한다면 꼭 가슴에 담아두어야 할 점이라 생각한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201231618]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[137]]></uid>
		<content_uid><![CDATA[24]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[박민식 : 이번 세미나에서는 Hugo 교수님의 deep learning 주제에 대한 세미나 발표를 들었습니다. distributed representation에 대해 깊이 고민해보면서 딥러닝이 사람의 인식 과정과 비슷한 방법으로 학습한다는것을 알게 되었습니다. Fine-tuning과 pre-training에 대한 대략적인 개념은 알고 있었지만 그것이 가지는 특징, 장점에 대한것을 잘 몰랐었는데 pre-training에서는 label이 필요하지 않고 Fine-tuning에서만 label이 있는것으로도 좋은 학습 결과를 낼 수 있다는 것을 알게 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160201233731]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[138]]></uid>
		<content_uid><![CDATA[24]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 이번 세미나 시간에는 딥러닝이 무엇인지, 또 training하는 방법에 대해 배웠습니다. 발표 초반에 Multilayer과 뇌의 구조를 서로 연결시키는 그림을 보여주어 전반적인 큰 그림을 확인하고 들을 수 있었습니다. 딥러닝에서는 network가 먼저 구성된 후 layer마다 feature가 뽑힌다는 distributed representation의 개념과 마지막 단계에서만 label이 필요하다는 fine tuning의 장점을 확실히 배울 수 있는 유익한 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160202133443]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[139]]></uid>
		<content_uid><![CDATA[24]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김보섭 : Drop out에 대해서 큰 틀에서 개념을 정립할 수 있어서 좋았습니다. Drop out은 일반화 성능을 확보하기위한 하나의 수단인데 여기서 히든 유닛을 Random하게 배제하므로써 신경망의 다양성을 확보해서 Aggregating 하는 점이 마치 Ensemble method를 연상시켰습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160203083725]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[140]]></uid>
		<content_uid><![CDATA[27]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: 연구를 혼자 진행하고 있는데도 불구하고 많은 실험을 하려고 노력한 모습이 보였습니다. Data를 실험에 필요한대로 잘 정제하였고 input, output을 결정하는데 하나하나 근거를 제시하여 좋았습니다. 마지막으로 발표자가 실험결과 수치를 실제 clustering 그림으로 보여주면서 설명하여 더 잘 이해할 수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160205163939]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[141]]></uid>
		<content_uid><![CDATA[26]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[조수현: ElasticNet의 기본 개념을 Lasso와 관련하여 알 수 있는 시간이었습니다. Lasso의 high correlation과 같은 한계를 극복하기 위해 ElasticNet이 사용된다는 점과 더 나아가 Naïve ElasticNet까지 여러 개념을 배울 수 있었습니다. Ridge, Lasso, ElasticNet을 한 그래프에 동시에 보여주며 비교하여 이해하기 쉬웠던 발표였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160205164550]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[142]]></uid>
		<content_uid><![CDATA[24]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 오늘 세미나에서는 RBM의 형태를 겹겹히 쌓아올리는 뉴련 냇의 구조를 공부하였습니다. 학습을 supervised와 unsupervised방식을 동시 고려하면서 한다는 것이 기발한 발상인것 같습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160205225605]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[143]]></uid>
		<content_uid><![CDATA[25]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 키스트로크실험에서 Unigraph, Digraph가 텍스트 마이닝에 uni-gram과 bi-gram과 유사한 것처럼 느껴집니다. 키정보가 필요없는 K-S,C-M과 필요한 R,A,RA에 대해서 정확히 이해가 되지않아 이부분에 대해서는 따로 공부를 해야 할 부분인것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160205231053]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[144]]></uid>
		<content_uid><![CDATA[26]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 : 엘라스틱넷이 한번에 나온 개념인 줄 알았는데 나이브 엘라스틱넷이 나왔던 점이 인상깊었습니다. 개인적으로 연구라는것이 한번의 발상으로 이루어지는 것이 아니라 여러번의 실험과 반복으로 이루어 지는 것 같습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160205231514]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[145]]></uid>
		<content_uid><![CDATA[27]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[dsba_admin]]></user_display>
		<content><![CDATA[김동화 :  연구를 혼자 진행하고 있는데도 불구하고 연구 가치에 대해 긍정적인 태도를 보이고 있는 것 같습니다. 여러가지 measure들을 이용하여 output/input 의 효율성으로 최적의 클러스터 방식을 찾아 간다는 것이 산업공학적인 연구 분야라고 여겨집니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160205232353]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[147]]></uid>
		<content_uid><![CDATA[26]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[Lasso의 한계점을 통해 Ridge와 Lasso가 더해진 개념인 Elastic net이 나왔음을 알 수 있었습니다. 또한 Elastic net 이전에 Naive elastic net의 개념이 있었지만 over shrinkage의 문제가 있음도 알게 되었습니다. 개념을 정리하고 variable selection 실험을 보니 더 흥미가 생깁니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160206220533]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[148]]></uid>
		<content_uid><![CDATA[27]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[지난 시간에 데이터에 관해 교수님께서 말씀하신 내용을 잘 준비해 온 모습을 볼 수 있었습니다. I index와 SD validity index에 관한 얘기를 할 때 각각의 measure를 깊이 이해하고자 노력하고 연구에 집중하는 모습이 보였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160206221616]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[149]]></uid>
		<content_uid><![CDATA[25]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[세미나 발표중 K-S, C-M, A, R 에 관한 내용이 있었는데 잘 모르는 내용이었습니다. ppt를 보고 난 후 조금은 이해가 되어 다음번 세미나부터는 더 집중해서 들을 수 있을 것이라 기대하고 있습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160206222039]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[150]]></uid>
		<content_uid><![CDATA[26]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[나현이의 지금 까지의 연구발표를 통해 회귀분석에서 쓰일 수 있는 유용한 변수 선택법 세 가지를 학습 하였다. 하지만 분명 개요를 소개해주는 발표 몇 번으로 내용을 모두 소화하긴 힘드므로 추후에 개인적으로 추가 학습이 필요하다. Elastic net의 설명 마지막 부분에 예제가 있었다. 이 예제를 나현이 본인이 실험한 결과로 직접 만들어서 보여 주었다면 발표의 흥미를 돋구는데 도움이 되지 않았을까, 생각이 든다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160207103214]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[151]]></uid>
		<content_uid><![CDATA[27]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Machine Learning Algorithms들을 다룰때 대부분의 경우에서  Hyper-parameters를 사용자가 설정해줘야 하는 어려움이 따른다. 통합된 Clustering Indices를 고려하는 DEA는 Efficiency를 기준으로 가장 최적이라 여겨지는 모델을 사람이 아닌 Algorithm이 선택하여 준다. 따라서 DEA를 사용하면 사람이 Parameters를 선택해야 했던 상황을 타개하는데 큰 도움이 되지 않을까 생각했다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160207104337]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[152]]></uid>
		<content_uid><![CDATA[28]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[바로 이전 연구세미나에서 실험 해야 할 data-set을 줄였음에도 불구하고 이 팀이 실험 해야 할 양이 아직도 상당히 많다고 느꼈다. 2월 안에 실험을 한 바퀴 다 돌릴려면 새로운 해결방법이 필요하지 않을까 생각된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160207104855]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[153]]></uid>
		<content_uid><![CDATA[26]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[기존에 어렴풋이 알고있던 개념을 정리할 수 있었어서 좋았고 grouping effect에 관한 설명이 조금 부족한 것 같아 개인적으로 찾아서 개념을 확인해보려고 합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160208162155]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[154]]></uid>
		<content_uid><![CDATA[25]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 EER에 대해서 조금 헷갈렸는데 결국은 기존의 Machine learning에서 사용하는 방식으로써 생각한다면 ROC curve와 비슷한 개념을 사용하고 있음을 이해할 수가 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160208162310]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[155]]></uid>
		<content_uid><![CDATA[28]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[잘 몰랐던 TF-IDF로 Term을 선택하는 방법에 대해서 상세히 알 수 있어서 좋았고 전체적인 실험의 틀에 당일 세미나로 잡혔기 때문에 차주의 실험결과가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160208163319]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[156]]></uid>
		<content_uid><![CDATA[25]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[관리자]]></user_display>
		<content><![CDATA[박민식 : 발표를 들으면서 처음보는 내용들이어서 내용을 이해하기는 쉽지 않았지만,  ppt를 다시 보면서 전처리 과정에서 어떠한 고민을 하였는지 알게 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160210222520]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[157]]></uid>
		<content_uid><![CDATA[26]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[관리자]]></user_display>
		<content><![CDATA[박민식 : elastic net에 대한 소개를 들었고, ridge와 lasso와 어떠한 차이가 있는지 알게 되었습니다. 실험결과표에서의 채워진 내용들은 좋았지만, dataset의 이름이나 그 dataset이 어떠한 dataset인지 상세히 설명되면 좋을것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160210222950]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[158]]></uid>
		<content_uid><![CDATA[27]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[관리자]]></user_display>
		<content><![CDATA[박민식 : 데이터 변경에 대한 발표를 들었고, 실험결과들을 보았는데 k-means에서 초기중심 initial문제 때문에 k값이 커져도 eff값이 감소하는 문제가 있다는 것을 알게 되었습니다. 다른 k-medoids, hierarchichal, DBSCAN에서의 결과값들이 잘 나오는것들을 확인할 수 있었고, 앞으로의 연구결과가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160210223517]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[159]]></uid>
		<content_uid><![CDATA[28]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[관리자]]></user_display>
		<content><![CDATA[박민식 : 실험결과에 대한 발표를 들었습니다. K-NN과 Naive-bayes에서 multiclass인 경우에는 낮은 성능을 보였는데, binary classification으로 변경하였을 때 어떠한 성능이 나올지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160210224059]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[161]]></uid>
		<content_uid><![CDATA[33]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[연구세미나 발표중 이 팀의 발표가 가장 재미있다. 연구에서 내놓는 결과 족족 참 흥미롭다. Topic Modeling결과를 Visualization 할 수 있는 방법이 있다면 더욱 좋을것 같다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160211224401]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[162]]></uid>
		<content_uid><![CDATA[34]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[발표의 초반부에는 아주 이상적인 결과가 나왔다. 연구를 시작하면서 세웠던 가설과 거의 일치하였지만 뒤에 Jain data set에서는 결과물이 조금 이그러지기 시작했다. Jain과 같이 Gaussian 분포를 따르지 않는 독특한 분포에서는 사용하는 indicies를 조금만 수정해 주면 결과가 좋아지지 않을까?]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160211224555]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[163]]></uid>
		<content_uid><![CDATA[37]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[과거에 sequence가 두 개의 자료의 분포의 유사성을 비교하는 것에 있어서 kolmogorov smirnov 또는 Cramér–von Mises criterion 등을 활용하는 것에 의문을 품고 있었는데 실제로 sequence가 있는 kda에 활용하는 것을 보고 의문이 풀렸고 해당 방법론에 대해서 이해하게 되어 좋았습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160215093538]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[164]]></uid>
		<content_uid><![CDATA[34]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[좋은 example 에 대해서는 좋은 결과를 보였지만 좀 더 복잡한 예제에 대해서는 새로운 아이디어가 필요하다고 사려됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160215204057]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[165]]></uid>
		<content_uid><![CDATA[37]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[data 수집이 어려운 만큼 평가기준 set을 고려하는 방식이 중요한 문제가 될 것 같습니다. 시퀀스 별로 set을 잡게 되면 데이터의 샘플이 많아지는것만큼 학습시간을 어떻게 효울적으로 해야할지 생각해 봐야 될것 같습니다,]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160215204323]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[166]]></uid>
		<content_uid><![CDATA[33]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[시각화 방식이 생각보다 받아들이는 사람에 입장에서 좋은 방식이었던것 같습니다. 토픽 모델링 네이밍 방식도 한번도 해보지 않은 부분이어서 앞으로 관심이깊습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160215205130]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[167]]></uid>
		<content_uid><![CDATA[34]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[앞선 예제에서는 의도했던 바가 명확히 보였지만 Jain의 경우 아쉬운 결과를 나타냈습니다. Jain 뿐만 아니라 여러 예제 데이터를 조금 더 수정하여 의도했던 바를 얻어낼 수 있기를 바랍니다. 또한 bound 된 indicies를 새로이 적용했을 때 결과물이 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160215214536]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[168]]></uid>
		<content_uid><![CDATA[33]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[토픽을 정성적으로 이름 붙인 것이 특히 인상깊었으며, 일반적으로 사용되는 단어(일반적인 동사나 논문에서 거의 무조건 쓰이는? 단어)에 대해 제거를 반복한 결과가 더 토픽을 잘 설명하는 단어들만 나오지 않을까 기대하고 있습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160215215441]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[169]]></uid>
		<content_uid><![CDATA[38]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[사전에 class label이 imbalance한 문제를 반영하여 cut-off를 바꿔준다면 결과가 어떻게 나올지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160216103218]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[170]]></uid>
		<content_uid><![CDATA[38]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[해당 팀의 실험설계는 거의 완성된 듯 하다. 실험결과가 모두 나오기 까지 생각해 볼 만한 점은 시각화라고 생각한다. 그 이유는 첫 째, 보여주어야 할 실험 결과가 많기 때문이다. 또한 Learner, 그리고 Training 방식, 두 가지의 요인을 변화 시켜가며 결과를 추출하기 때문에 한 번에 결과를 효과적으로 보여주기 위해선 심사숙고가 필요해 보인다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160218153719]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[171]]></uid>
		<content_uid><![CDATA[39]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이 팀의 연구는 결과가 순조롭게 나오고 있기 때문에 연구와 실험 설계에 대해선 딱히 첨언 할 필요는 없을듯 하다. 그래도 한 가지 추가로 조언을 한다면 Topic Modeling 결과를 결국은 연구자들이 직접 정성적인 방법으로 Naming하게 될 텐데, 적확한 이름을 달아주려면 사전에 Deep Learning에 대한 최신 뉴스를 꼼꼼히 살펴보면 도움이 될거라 생각한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160218153952]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[172]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[실험하고 있는 변수선택 방법들의 결과를 오차만을 기준으로 단순 나열 한다면 실험 결과가 심심해 보일것 같다. 실험 내용이 평범한 측면이 있기 때문에 결과 해석을 얼마나 잘하냐에 따라 전체 연구의 성과의 격이 결정될거라 예측된다. 각 변수 선택법의 장단점을 잘 독해하고 논문에 서술해 주면 좋을것이라 생각한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160218154416]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[173]]></uid>
		<content_uid><![CDATA[41]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[김형석: Measure들을 통해서 이를 입력변수로 가지는 Novelty Detection이라는 아이디어가 실제로 좋은결과가 있기를 기대하며, 지난 학기에 수강한 비즈니스 어낼리틱스 Novelty Detection의 방법론들을 활용할 수 있는  좋은 기회라고 생각됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219005414]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[174]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[GA같은경우 반복실험이 제한되기는 하지만 매우 흥미로운 결과가 있을것이라 생각됩니다. 일반적으로 알려진 다양한 변수 선택법의 경험적인 기준이 될거라 생각되어, 차후 이를 활용하는 본인에게도 기대가 되는 부분입니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219005556]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[175]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[표로 봐서는 실험이 얘기하고자 하는 바를 쉽게 이해하기 어려운데, 적절한 시각화를 통해 독자들에게 한눈에 인사이트를 준다면 유용한 실험으로써 인정받을수 있을 것이라 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219130355]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[176]]></uid>
		<content_uid><![CDATA[42]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[연도별 비율로 확인하니 좀 더 시각적으로 쉽게 확인 가능했습니다. topic같의 network를 구성하는것이 어떻게 될지 궁금하며 topic의 corr.을 구하는 부분은 아직 이해 되지 않아 좀 더 고민해볼 생각입니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219131037]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[177]]></uid>
		<content_uid><![CDATA[41]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[두 그래프 사이의 유사도를 계산하는 index를 이용해 조합을 만들었을때 어떤 결과가 나올지 기대됩니다. 또한 %에 대한 개념을 정립하는 좋은 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219131146]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[178]]></uid>
		<content_uid><![CDATA[39]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[당일 세미나에서 Topic modeling의 결과는 일반적으로 많이 쓰이는 동사어구들이 들어가있어서 그 부분만 불용어 처리를 하여 다시 Topic modeling을 시행한다면 좀 더 좋은 결과가 있을 것이라 기대된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219161138]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[179]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[선택된 설명변수가 적을 경우 예측 성능이 좋아졌다는 것은 결국 모형의 복잡성에 기인한 문제이지 않을까 생각이됩니다. 또한 류나현 학우의 연구에 대해서 실험결과를 정리하는 법에 대해서 여러부분에서 배우고 있습니다. 다만 그래프로 표현하는 부분이 관건인데 이 부분을 어떻게 대처할지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219161659]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[180]]></uid>
		<content_uid><![CDATA[41]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[KS, CM, RA 등의 measure를 stacking의 방법론으로 활용하여 classifier를 구성하는 점이 인상적이었습니다. 또한 개인적으로 기대되는 방법론은 LSR을 대체하는 방법론인데 valid user의 keystroke를 전부다써서 hierarchical cluster analysis로 새롭게 변수를 만드는 방법론은 결국 cluster의 타당성이 중요한데 이는 optimal cluster의 개수를 찾을수만 있다면 정말 좋은 방법론일 것 같습니다. 또한이 방법론의 cluster는 같은 사람의 keystroke에서 추출하므로 결국 구형을 이루게 될 가능성이 클 것이라고 생각되기 때문에 최적의 cluster를 찾는 것도 가능할 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219162449]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[181]]></uid>
		<content_uid><![CDATA[42]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[Topic 간의 Network를 구성함에 있어 Topic 간의 유사도를 계산한뒤 어느 정도의 Threshold를 가지고 edge를 제거한 뒤 Network를 구성하느냐가 중요할 것 같습니다. 해당 Framework에서 Topic간의 Network는 처음에는 Undirect weighted graph가 그려지게되므로 중개중심성 등을 이용한 군집화는 힘들 것 같고, 결국 graph기반의 clustering algorithm을 사용하게 될 것인데 그 동안 연구해본결과 graph based clustering 방법론들이 잘 작동하지않는 것같아 그 부분만 조금 걱정이됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219163024]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[182]]></uid>
		<content_uid><![CDATA[43]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[co-training에서 각 classifier의 BCR을 활용하여 classifier별로 가중치를 주어 label의 신뢰도를 계산한뒤 label을 부여한다는 idea가 인상적이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219163551]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[183]]></uid>
		<content_uid><![CDATA[25]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[조수현: 발표 내용 중 생소한 내용이 많아서 이해하기 쉽지 않았지만 연구를 진행하면서 발생하는 문제에 대해 끊임없이 고민하고 해결하려는 모습이 대단하다고 느꼈습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219163941]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[184]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[DBM, DNN, DBN 의 차이를 알수 있는 계기가 된것 같습니다.  RBM의 형태와 시그모이드 빌리프 네트 워크의 결합된 구조로 pre training 하는 방식이 인상적이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219171045]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[185]]></uid>
		<content_uid><![CDATA[34]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[Jain data의 경우 의도한것과는 다르게 제대로 clustering이 되지 않았지만 다시 수정한 후 실험하여 더 좋은 결과가 나오길 바랍니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219172751]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[186]]></uid>
		<content_uid><![CDATA[34]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA["well-separated " case에서 같은 k값의 k-means로 실험을 하였을때도 반복횟수마다 다른 eff 값을 보이는 경우가 있었는데 k-means의 초기값 설정에 따라서 결과값이 많이 달라진 다는 것을 알게 되었습니다. 그리고 "jain" case 에서는 k-minimum spanning algorithm이 구형의 클러스터가 아닌 경우 좋은 결과가 나오기 어려운것을 확인할 수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219173144]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[187]]></uid>
		<content_uid><![CDATA[33]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[토픽 모델링 한 결과를 한눈에 파악하기 쉽게 버블 차트와 network flow로 잘 visualization 하였고  년도별 논문 수를 범주에 묶는 등 디테일 한 부분까지 신경쓰는 것 같아 인상깊었습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219173218]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[194]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[variation inference의 개념에 대해서 명확히 이해할 수 있어서 좋았습니다. 아직 RBM으로 DBN을 stacking하는 부분은 수식을 보았을때 완벽히 이해하지는 못했지만 전체적으로 DBN으로 pretraining한다는 것이 어떤 의미인지 알았고 fine tuning 부분에서 class-RBM을 활용해서 tuning하는 부분을 눈으로 확인할 수 있는 예제를 구해와 이해하기가 쉬웠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219175212]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[189]]></uid>
		<content_uid><![CDATA[39]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[network visualization과 버블 차트를 수정한 결과 훨씬 더 보기 깔끔했다. 토픽 모델링 후 뽑힌 토픽들을 다시 한번 검토하여 제외할 부분은 제외할 필요가 있는데 특히, 일반 단어와 딥러닝 관련 단어 사이에 있는 오묘한 단어들을 제외할지 말지에 대한 고민이 필요하다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219174302]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[191]]></uid>
		<content_uid><![CDATA[44]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[현재 index들은 구형에 대한 것(거리, 분산 관련)인데 반해 구형에 대한 내용을 고려하는 것이 아닌 index를 이용하면 더 성능이 좋아질지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219174646]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[192]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[아직 실험이 다 완료되지 않았기 때문이겠지만 엑셀 표만으로는 실험 결과를 파악하는데 한계가 있는 것 같습니다. 적절한 시각화 자료가 뒷받침 되어야 연구 논문의 가치가 높아질 것 같습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219174804]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[193]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[확률을 이용한 DBN을 알게되는 시간이었습니다. 아직 top-down, bottom-up이 어떤식으로 작동하는지 완벽한 이해를 못하여서 좀더 수식과 예시를 통해 공부할 생각입니다. 또한 likelihood의 최적화를 현 상황에서 못하는 경우 우회적으로 bound를 설정하여 최적화 하는 방식에 대해서도 알게 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219174905]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[195]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[DBN에 대한 설명  이외에도 추가적으로 DNN(buttom up 방식)과 DBN(Top down 방식)의 차이점을 짚고 넘어가 명확하게 개념을 이해할 수 있었습니다. 또한, 동영상 강의에서 언급만 하고 넘어간 Fine tunning 에 대한 보충 설명과 시뮬레이션을 통해 모델이 기존 데이터를 얼마나 잘 재생성하는지에 대한 이해도 쉽게 할 수 있었습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219175449]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[196]]></uid>
		<content_uid><![CDATA[42]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Topic Modeling의 결과로 추출한 Topic별 출간된 연구 수를 꺽은선 차트로 제시해 주어서 최근 활발히 연구가 이루어지고 있는 Deep Learning의 Topic을 일목요연하게 볼 수 있었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219201806]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[197]]></uid>
		<content_uid><![CDATA[43]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[더 효과적으로 결과물을 보여주기 위해 MATLAB이라는 익숙하지 않은 Tool에 새롭게 도전한 점이 매우 인상 깊었다. 바로 이전 세미나에서 엑셀을 사용하여 만들었던 꺽은선형 차트보다 MATLAB으로 얻은 3차원 Surface가 훨씬 더 결과를 이해하기 쉬웠다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219202043]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[198]]></uid>
		<content_uid><![CDATA[44]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[각 Index별로 분류를 잘 해내는 분야가 있다. 목적이 서로 다른 Indices를 혼합 하였을때 더 좋은 결과물이 나올수도 있지만 나는 되려 각Index가 가지고 있는 장점이 혼합 과정에서 서로 일그러져 성능향상이 없거나, 나쁜 경우에는 결과물이 더 나빠질 수도 있지 않을까, 걱정이 되기도 한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219202416]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[199]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[DBN을 공부하면서 가장 먼저 떠올랐던 점이 왜 하필 'Belief'라는 단어가 쓰였을까 궁금 했었다. 발표의 첫 부분에서 이점을 해설 해 주었다. 또한 Fine-Tuning, KL Divergence등 본 강의 에서는 자세히 다루지 않았기 때문에 간과하기 쉬웠던 부분도 세세히 세미나에서 설명해 준 부분이 인상 깊었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219203643]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[200]]></uid>
		<content_uid><![CDATA[39]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[불필요한 키워드 제거를 해야 명확한 토픽네이밍이 될 것 같습니다. 이 다음으로 무엇을 할지 기대됩니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219204356]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[201]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[부분적으로 실험 조건들을 명확해 주는것이  듣는 사람의 입장에서 좋을 것 같습니다. 기초적이이지만 다시 집고 넘어가는 시간이라서 유익한 시간아 된것 같습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219204534]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[202]]></uid>
		<content_uid><![CDATA[41]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[복잡하지만 디데일한 큰 변화가 어느정도 실험 효과를 보이는 것 같습니다. 옆자리에 않는 김준홍 학우의 노력이 전해지네요]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219205018]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[203]]></uid>
		<content_uid><![CDATA[42]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[개파이의 성능이 좋다는 것을 느끼수  있엇습니다. 기회가 되면 한번 다뤄보고 싶은 tool 입니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219205146]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[204]]></uid>
		<content_uid><![CDATA[44]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[비선형적인 문제를 해결하는 관점이 앞으로  이 논문에 쟁점이 될 것 같 습니다. 향후 어떻게 될지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160219205252]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[205]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[deep belief network에 대한 발표를 들었습니다. feed-forward network 구조와 다르다는 것을 알게 되었고 vari ational bound(inference)에 대해서도 알게 되었습니다. 강의에서 설명하지 않은  fine-tuning 에 대해 설명한 것이 좋았습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160220174903]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[206]]></uid>
		<content_uid><![CDATA[44]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[준홍 김]]></user_display>
		<content><![CDATA[Valid index중 Inner Variance와 Outer Variance를 이용한 방식이 구형에 형태에 적합한 방식이라
Cluster의 분포가 구형의 꼴로 나타날경우 efficiency가 상대적으로 높게 나타나는 현상을 발견하여 구형의 형태가 아닌 특별한 항태의 적합한 Index와 조합할 경우 어떠한 결과가 나타날지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221012319]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[207]]></uid>
		<content_uid><![CDATA[43]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[준홍 김]]></user_display>
		<content><![CDATA[실험의 Computing Power의 한계점으로 계속되는 모델링을 차근차근 실행해 나가는 진행 속도가 배울점이라 생각되며, 이를 해석하여 Data에 dependent 하지 않고 객관적인 모델의 결과를 해석할수 있는 결과가 나오기를 기대합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221012443]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[208]]></uid>
		<content_uid><![CDATA[42]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[준홍 김]]></user_display>
		<content><![CDATA[LDA의 각 Document의 Probability based on each topics의 결과를 하나의 토픽으로 보는것이 아닌 여러가지의 확률값으로 보는 발상이 인상적이었습니다. 이는 Input parameter의 Alpha에 따라서 달라질수 있는 부분인데, 이를 어떻게 해석할지 궁금합니다. 개인적으로 Biblometrics를 진행 했던 경험에 발상의 전환을 준 부분이 인상적이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221012720]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[209]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[준홍 김]]></user_display>
		<content><![CDATA[개인적인 졸업식 참여의 상황으로 저번주에 발표할 내용을 덧붙여서 형석이가 발표해 주었는데 고마운 마음이 큽니다. 이번주에 발표한 PT를 바탕으로 복습, 학습하는 시간을 가지도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221012851]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[210]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[준홍 김]]></user_display>
		<content><![CDATA[GA의 부분에서 상당한 시간이 소요되는것을 보고 하였는데, GA이 Parameter 특정상 한계점이라고 생각할수 있는데 이를 어떻게 해석하여 진행할 것인지 궁금합니다.
그리고 결과를 보면 특정 데이터에서는 Error율이 비슷하지만 변수 선택율이 반도 안되는것을 볼 수 있었습니다. 이를 왜 그런 특성이 나타났는지 해석하면 굉장히 흥미로운 해석이 되지 않을까 생각하며, 연구가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221013247]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[211]]></uid>
		<content_uid><![CDATA[37]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Train과 Test 데이터의 생성, 설정방법들에 대해서 많은 이야기를 들었고, 실험설계의 중요성을 인지하게 되는 계기가 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221144041]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[212]]></uid>
		<content_uid><![CDATA[38]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[supervised learning에 대한 실험 결과들을 보았습니다. 대체적으로 좋은 결과들이 나오는 것을 확인할 수 있었는데, self-training, co-training, multi-training에서의 결과값에서 어떤 차이들이 나올지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221150935]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[213]]></uid>
		<content_uid><![CDATA[40]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[데이터 설명(이름, 출처, 레코드수, 변수 수, 간략한 소개)들이 있으면 좋을것 같고, 표의 내용이 많기에 한눈에 결과를 볼 수 있도록 시각화 하면 좋을것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221151316]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[214]]></uid>
		<content_uid><![CDATA[41]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[실험설계, 기계학습 방법론에 대한 깊은 고민을 들을 수 있던 발표였습니다. 교수님의 % 증가감소에 대한 설명을 들을면서 이 내용을 다시 상기하게 되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221151709]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[215]]></uid>
		<content_uid><![CDATA[43]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[BCR을 Matlab을 이용한 3차원 그림으로 표현하여 알아보기 쉬웠다. Weight 값을 적용할 때 test 데이터는 검증할때만 사용하고 train 데이터를 사용해야 된다는 점을 유의하게 되었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221153849]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[216]]></uid>
		<content_uid><![CDATA[44]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[clustering 그림에서 위치에 따른 색깔을 동일하게 해야 해야 학회에서 발표할 수 있다는 교수님 말씀을 들었는데 일을 하는데 있어서 디테일 하게 하는것이 중요하다는 것을 느꼈습니다. 또한 구형을 잘 잡는 index, 구형이 아닌것을 잘 잡는 index등 다양한 조합의 index 값이 사용되었을 때 어떠한 결과가 나올지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160221155455]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[218]]></uid>
		<content_uid><![CDATA[45]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[DBN, DBM, DNN의 차이를 정리하고 variational bound에 대해서도 잘 설명해 주어서 좋았고 전반적으로 이해하는데 도움이 되었습니다. 아직  명료하게 이해되지 않은 부분은 교수님이 알려주신 유투브 강의를 보고 좀 더 공부해보겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160222215856]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[225]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[sparse coding에 대한 목적과 정의 네트워크 구조를 알수 있는 시간이었습니다. 마지막으로 RBM, Auto-Encoder와 sparse coding 에 차이점을 개략적으로 알수 잇는 의미있는 시간이 된것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160228205155]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[226]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[PCA와 Sparse coding의 차이중 하나인 bases에 대해 정리를 들었는데 앞으로 공부하면서 도움이 될 내용이었다고 생각합니다. 또한 Dictionary와 hidden unit에 대한 개념을 완전히 반대로 잡고있는 상태였는데, 강의 후반에 교수님께서 RBM, Auto Encoder, Sparse coding을 비교정리 해주셔서 그때 개념을 잘 잡을 수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160229001851]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[227]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[준홍 김]]></user_display>
		<content><![CDATA[Sparse coding에 목적 함수가 기존 방식과 다르다는 점을 알 수 있었습니다.
아직 확실하게 이해 되지는 않아서 따로 더 공부를 하여야 할 것 같습니다.
평소에 알지 못했던 생소한 방법론이라 흥미로웠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160229052315]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[228]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[overcomplete와 dictionary 부분이 어려웠었는데 세미나를 통해서 이해하게 되었습니다. 다른 unsupervised 방법인 rbm과 autoencoder에 대해서도 상기하게 되는 시간 이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160229090042]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[229]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 세미나에서는 RBM, auto-encoder, sparse coding을 비교, 분석하여 그 차이점에 대해 배웠습니다. 대략적인 차이점과 특징은 이해됐지만 자세한 부분은 좀 더 공부해야 이해될 것 같습니다. 공부를 하면서 큰 구조로 파악하는 것이 중요하다고 느낀 세미나였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160229092136]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[223]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Hugo 강의내용을 다루기 전에 Overcomplete basis를 PCA와 함께 설명해 주어서 Sparse coding의 개념을 잡는데 도움이 되었다. 지금까지 학습한 총 세 가지의 Unsupervised learning을 세미나 마지막에 교수님이 친절히 정리해 주었다. 비슷해 보이지만 전혀 다른 구조를 가지고 있기 때문에 이 셋 Algorithm을 비교하면서 같이 공부하니 서로의 차이점이 더욱 명확히 보였고, 개별 Algorithm을 따로 복습하는것 보다 더욱 효과적 이었다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160227224440]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[224]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[overcomplete layer를 가정하고 사용하는 이유는 결국 classifier로 활용하기위하여 shatter 개념이 도입된 것이라 추측된다. 강의 전반에 있어서 Sparse coding의 structure가 매우 헷갈려서 feed-forward 방식이라고 생각하고 있었는데, 이번 세미나를 통하여 Sparse coding의 Structure를 파악할 수 있어서 좋았다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160228102904]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[230]]></uid>
		<content_uid><![CDATA[48]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[가장 해야 할 실험이 많았지만 가장 먼저 끝냈기 까지 본 팀의 각고의 노력이 있었을 것이다. 좋은 실험결과가 나온것에 대해 먼저 축하를 하고싶다. 교수님께서 실험결과가 만족스럽게 나왔다고 하셨지만 아직 연구의 배경 이론에 대한 지식이 없어서 그런지 나는 어떤점이 좋아졌는지 명확하게 파악하기 힘들었다. 이번학기 수강할 비정형데이터분석 수업 시간에 텍스트분석을 열심히 공부해야겠다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160229234137]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[233]]></uid>
		<content_uid><![CDATA[48]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[준홍 김]]></user_display>
		<content><![CDATA[부지런함으로 완성된 실험결과를 산출한 동화에게 축하하고 싶다.
오늘 세미나에서 연구의 목적을 Document Classifier의 비교를 주축으로 한다고 설명 들었는데
오늘 보여준 Boxplot에서 제안된 방법이 Robust하게 BCR기준 좋은 결과를 나타남을 유추해 볼 수 있었습니다.
다음 세미나시간에서 구체적인 결과와 설명을 듣게되면 앞으로 Document Classification를 함에 있어서 사전 지식으로 사용할 수 있음에 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160301034912]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[234]]></uid>
		<content_uid><![CDATA[48]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[dimension에 대한 고려, ratio of unlabeled data를 고려하기위해 matlab을 사용해서 받아들이기 쉽게 표현한 점이 인상적이었고 해당 팀의 연구를 보며 제 연구도 빨리 진행시켜야겠다는 자극을 받았습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160301152909]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[235]]></uid>
		<content_uid><![CDATA[49]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[[3]번 사항의 Parzen 방법을 사용하겠다는 Idea의 기저에 대해서 절대적으로 공감하는 부분입니다. 사용자마다 digraph의 분포는 달라지기 때문에 해당 사용자의 digraph distribution의 density값을 이용하겠다는 점이 인상적이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160301153405]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[236]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[구형에 형태에 덜 민감한 지표를 찾고, 그 조합중 실험적으로 조합한 결과를 선보였으며 기존 데이터에 DEA의 efficiency 값이 알맞게 산출되는 모습을 발표하였다.  발표후 사용한 단일지표의 트렌드와 비슷하였는데 실재로 비교한 자료가 기대되어집니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160301193851]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[237]]></uid>
		<content_uid><![CDATA[51]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[평소에 토픽 네트워크의 노드가 너무 많다고 생각하고 있었기 때문에 토픽을 군집화 한다는 아이디어에 적극적인 지지를 보낸다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302123201]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[238]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Non-Gaussian 분포의 데이터를 분류하는 지점에서 실험이 난항을 겪고 있는것 같다. 하지만 이번 발표 결과를 보면 eff가 1일때 다른 경우보다 더 좋은 분류 결과를 얻었기 때문에 앞으로의 실험 결과도 낙관적일 것이라 예상된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302123736]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[239]]></uid>
		<content_uid><![CDATA[51]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[네트워크를 잘 움직여서 각 클러스터별로 유의해 보이게끔 했지만 연결선이 너무 많아서 보기 힘든 부분이 있었습니다. 교수님께서 선이 너무 많은 것도 정보가 없는것과 마찬가지라는 말씀해주신 것이 기억에 남습니다. 적당한 threshold로 유의한 네트워크를 보기를 기대합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302160942]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[240]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[index 조합을 찾고자 여러 방향으로 노력했음이 느껴졌습니다. 연구에서 논리적 흐름에 대한 고려 또한 중요한 사실임을 알게 되었습니다. 다음번 세미나에서는 교수님께서 말씀하신 조합이 어떤 결과를 보이는지 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302161054]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[241]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[실험결과가 많은 만큼 이를 어떻게하면 잘 표현할수있는지가 가장 큰 이슈사항이 될것 같습니다.
 Heat-Map 뿐만 아니라 논문 탐색을 통해 효율적인 Visualization을 통해 잘 표현하기를 바랍니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302172336]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[242]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[heuristic적으로 index 조합을 2가지로 줄인 점과 연구방향과는 조금 핀트가 나가지 않았나 싶습니다. 기존연구의 방향은 여러 인덱스들을 통해서 가장 최적의 멀티인덱스를 재조합하는 방향인데 이를 너무 단조롭게 구성된다면 연구방향의 의미가 퇴색되지 않을까 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302172729]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[243]]></uid>
		<content_uid><![CDATA[49]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[Meta-Learning에 대한 자세한 성능에 대해서 경험해보지 않아, 추후 Meta-Learning 관련 부분이 기대가 됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302173005]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[244]]></uid>
		<content_uid><![CDATA[48]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[김동화군의 지금까지의 고군분투를 느낄수 있었습니다. 
실험결과에 대한 Visualization에 대해서 조금만 더 신경쓴다면, 좋은 결과를 얻을수 있을것이라 생각됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302173233]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[245]]></uid>
		<content_uid><![CDATA[47]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[기존 Unsupervised-learning 과 달리 Sparse-coding이 실제에서 어떤 부분에 잘 사용되는지와 어떤 분야에 좀 더 특화는지 좀더 알아볼 필요가 있겠다 생각됩니다. 지금까지의 네트워크들의 구조를 이해한다면 각각의 특징들을 잘 이해할 수있겠다 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302173514]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[246]]></uid>
		<content_uid><![CDATA[49]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[다양한 실험 방법이 존재하는 만큼 뭐가 잘 될 것 인지 의문점을 나타냅니다. 개인적으로는 오히려 단순한 방식들이 좋은 결과들을 보이는 사례가 많은 만큼 간단한 방식들의 실험방식들을 생각해 보면 어떻까 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302185751]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[247]]></uid>
		<content_uid><![CDATA[51]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[시각화하는 방식이 결과물이 잘 되었다라는 것을 잘 보여줄수 있는 방법이 아닐까 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302185858]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[248]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[다양한 변수선택의 비율에 따른 정확도가 얼마나 향상이 되는지 시각적으로 쉽게 알수 있으면 좋은 연구 결과물이 될것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302190040]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[249]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[결과가 긍정적인 결과를  보이는 것 같습니다. 어떠한 measure 를 사용함에 따라서 결과가 좋아진다는 것을 논리적으로 설명할수 있다면 좋은 결과물이 될 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160302190336]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[250]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[데이터를 3차원으로 묶은 다음 Heat-Map으로, 그리고 비모수 통계검정 까지 유용한 실험결과 표현 방법을 배웠다. 실험결과를 보여줄 수 있는 방법이 생가보다 다채롭다고 생각했다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160303220546]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[251]]></uid>
		<content_uid><![CDATA[51]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[토픽간의 유사도를 계산하는 방식에 따라 군집화의 결과가 많이 달라질 것같아 염려되는 부분이 있습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304000818]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[252]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 조언을 하자면 각각의 결과들을 비모수방법으로 비교함에있어서 유의수준을 설정할 때, Bonferroni correction 방법을 사용하여 유의수준을 변경하여 사용하기를 바랍니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304034418]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[253]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[순위 검정시 1:1로 비교하는 것 뿐만 아니라 여러 대상에 대해서 비교하는 것이 있는 것으로 들은 기억이 있는데, 해당 내용을 저도 찾아보고 도움을 드리도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304051612]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[277]]></uid>
		<content_uid><![CDATA[54]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[Stepwise selection이 가장 좋은 선택법이었다는 사실이 놀랍지만 regularize를 붙이는 방법들은 대게 parameter가 있기 때문에 best parameter를 고르지 않았다는 점에서 나올 수 있는 결과라고 생각한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160305194138]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[278]]></uid>
		<content_uid><![CDATA[55]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[Alphago와 이세돌의 대국이 기대되는 시점에서 Alphago의 개략적인 형태를 알 수 있어서 좋았습니다. 최근에 python 공부를 시작했는데 Tensorflow를 구동할 수 있는 수준이 될 때까지 노력해야겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160307141419]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[279]]></uid>
		<content_uid><![CDATA[57]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[토픽간의 유사도로 그래프를 이을 때, 결국 어떠한 Threshold로 complete weighted graph에서 edge를 끊어내므로 undirected weighted graph의 형태가 되는데 이를 graph기반의 클러스터링을 해보면 어떨까 싶다. 중개중심성 군집화라든지 고유벡터 중심성 군집화같은 방식을 추천해보고 싶다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160307141558]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[256]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[DEA의 efficiency가 1인 최적의 구조를 찾아낸 점은 긍정적인 연구 성과라고 할 수 있지만 heuristic하게 찾아낸 index인 만큼 그 논리와 타당성이 보완되어야 할 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304062411]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[257]]></uid>
		<content_uid><![CDATA[51]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[사실 네트워크를 보면서 복잡해보인다고는 느꼈지만 노드와 연결선의 수를 줄인다는 생각은 미처 하지 못했는데 군집화에 따라 달라질 결과가 매우 기대됩니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304062824]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[258]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[실험 결과를 비교하는데 단순히 performance를 나열하여 비교하는 것 이외에도 비모수 검정법 등 여러가지 방법이 사용될 수 있다는 것을 느낄 수 있었습니다. 결과를 통계 수치로 비교하는 것 뿐만아니라 시각적 자료로 어떻게 표현할지  기대됩니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304063722]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[259]]></uid>
		<content_uid><![CDATA[48]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[많은 결과를 visualization 하기까지 많은 시간과 노력이 들었을 것으로 보입니다. 실험 후 분석 및 결과가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304073743]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[260]]></uid>
		<content_uid><![CDATA[48]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[실험 결과에 대한 노력을 볼 수 있던 발표였습니다. 3차원 데이터 결과 표현에서 scale 값 조정시 어떤 결과가 나올지 궁금하고 발표자료가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304083629]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[261]]></uid>
		<content_uid><![CDATA[50]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[index 지표 선정에서 구형에 잘 맞는것 맞지 않는것 input과 output에서 각각 2개씩 선정하여 결과를 도출하였을때 어떠한 결과가 나올지 기대가 됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304153357]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[262]]></uid>
		<content_uid><![CDATA[52]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[예측오차나 변수선택율을 시각화 할때 표나 히트맵 형식으로 표현하면 더욱 효과적으로 보일 수 있을 것이라 생각됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304153948]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[263]]></uid>
		<content_uid><![CDATA[55]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[목표 했던 딥러닝 세미나 과정의 반절 이상 지난 이 시점에서 다시 딥러닝을 소개하는 내용을 발표해야 했기 때문에 세미나를 구성하는데 어려움이 있었을 것이라 예상한다.딥러닝이 다층 신경망이고 어떤 종류의 알고리즘이 있는지는 이미 랩 구성원들 대부분 알고 있을거라 생각한다. 따라서 개인적으로 이번 세미나에서 기대 했던것은 딥러닝의 생물학적 모티베이션을 다시 자세히 살펴보고 기본적인 개념, 예를 들면 Generative model, 또는 Graphical model,을 다시 짚어보면서 기초를 다지는 시간을 가졌으면 했다. 애초의 기대와는 발표 방향이 많이 달랐지만 앞으로 연구실의 공식 프레임워크로 사용될 텐서플로우를 소개 했다는 점에서 기대치와는 어긋나지만 유익한 세미나 였다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304165858]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[264]]></uid>
		<content_uid><![CDATA[53]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[지표의 분포 분자가 들어가는 부분을 이때까지와 다르게, 구형의 적합한 Measure Index의
분모 분자 부분을 쪼개서 넣는 방식을 생각해보았는데, 결과가 궁금합니다.

이 연구의 목적처럼 DEA를 통해 단일 Measure Index보다 보편적인 성능을 가진 연구 결과가 나오기를 기대합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304173739]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[265]]></uid>
		<content_uid><![CDATA[54]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Elastic Net과 Lasso Ridge의 보편적인 성능을 알아볼수 있었습니다.
개인적으로 궁금했던 부분이었는데, 생각보다 Elastic Net와 Lasso의 성능이 보편성을 띄게 좋지 않아서 의아했습니다.
결과를 보았을 경우 Ridge의 경우 성능이 보편적으로 좋지만 여러 변수를 선택하는 경향을 보였고 오히려 Local variable selection에서 가장 복잡도가 높은 Stepwise가 변수의 양도 줄이면서 Linear Regression의 성능을 높혀주는 결과를 보여서, 앞으로 Linear Regression을 사용할때, 변수 선택기법의 우선순위를 참고할수 있는 자리였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304174103]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[267]]></uid>
		<content_uid><![CDATA[56]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[제한된 데이터 상황에서 파라미터 써치를 위한 validation_set구성을 효율적으로 잘 나눈점이 인상깊었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304205655]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[268]]></uid>
		<content_uid><![CDATA[55]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[제가 알기로 유다 시티(http://www.udacity.com)에서 텐서플로우 오픈 강좌를 개설했다고 합니다. 
혹시 관심있으신분은 강좌를 통해 학습을 하실수 있을것입니다. 발표된지 얼마 안된 시기(1년?)임에도 불구하고 벌써 오픈소스 강좌가 개설된걸 보면 향후 범용적인 Tool로서 가능성을 기대해 본다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304210139]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[269]]></uid>
		<content_uid><![CDATA[54]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[확실히 논문화된 방법론들이 항상 Best가 아님을 확인할 수있는 실험 결과이였습니다.
실험결과를 Visualiztion하는 부분에서도 효율적으로 잘 파악할수 있었습니다. 각 축의 text사이즈 및 조금의 섬세함을 더해준다면 금!상첨!화!]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304210351]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[270]]></uid>
		<content_uid><![CDATA[53]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[각 인덱스를 좀더 분해하여 분모 분자 단위로 Index를 나누어서 이를 재조합한다면 좀더 정교한 DEA성능을 발휘 할 수 있을것이라 기대하며, 너무 비관적으로 볼 실험결과가 아니라 생각됩니다.
'문제 그 자체를 사랑하라! 중요한건 모든것을 살아보는 일이다.' - 라이너 마리아 릴케]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304210814]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[271]]></uid>
		<content_uid><![CDATA[57]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[자료의 표현은 오늘 세미나에서 교수님이 가이드 해 준 부분만 수정하면 거의 끝날 것 같다. 눈 여겨 봤던 부분은 클러스터링 결과가 납득하기 어려운 군집을 보여줬다는 점이다. 예측과 다른 결과가 나온 이유가 군집 과정에서 발생한 테크니컬한 오류인지 실제 데이터의 구조가 우리의 예상과 다른 것인지 궁금하다. 다음 세미나 발표가 기대 된다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160304211805]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[272]]></uid>
		<content_uid><![CDATA[57]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[다음주의 결과인 전체 논문 대비 해당 topic의 비율이 hot인지 cold인지를 통해 현재 가장 사랑받는 주제가 무엇인지 매우 흥미롭습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160305190600]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[273]]></uid>
		<content_uid><![CDATA[54]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[Lasso와 Elastic net이 분명 변수선택에 있어서 좋은 성능을 보일 것이라 생각했지만 전혀 그렇지 않은 모습에 놀랐습니다. elastic net에서 사용됐을 alpha나 Lasso, ridge의 lambda를 최적의 값으로 설정했다면 결과가 달라졌을지 궁금합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160305191126]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[275]]></uid>
		<content_uid><![CDATA[55]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[이번 세미나를 통해 Tensorflow를 좀 더 가까이 느끼게 되었습니다. 앞으로 프로젝트에서도 사용될 예정이라는 교수님의 말씀까지 듣고 빨리 환경을 구축하고 Tensorflow를 잘 사용할 수 있는 분석가가 되고싶다는 생각을 했습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160305191448]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[276]]></uid>
		<content_uid><![CDATA[53]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[각 index의 세부사항을 더 확인하고, compactness와 separation부분을 구분하여 분자, 분모에 넣은 후 convex, non-convex까지도 다 수용할 수 있는 그런 지표가 나오기를 기대합니다. 특히 부분 클러스터에 대해 좀 더 민감하게 찾아낸다면 너무 좋을 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160305191730]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[280]]></uid>
		<content_uid><![CDATA[56]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 Meta Learning이 가장 기대되는 방법입니다. 좋은 결과가 나올 것으로 기대합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160307141639]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[281]]></uid>
		<content_uid><![CDATA[55]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[딥러닝에 대한 Intro였음에도 불구하고 그동안 배웠던 알고리즘들의 장, 단점과 예시를 언급해주어 정리할 수 있는 시간이었습니다. 뿐만아니라 Tensorflow를 다룰수 있도록 더 노력해야겠다고 생각하게되는 세미나였습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310195135]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[282]]></uid>
		<content_uid><![CDATA[57]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[히트맵으로 표현된 hot, cold토픽의 결과가 매우 흥미로웠고 clustering에 따른 새로운 결과도 기대가 됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310201403]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[283]]></uid>
		<content_uid><![CDATA[54]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[예상과 달리 stepwise selection의 성능이 가장 좋은 점이 의외였습니다. Data visualization의 노력도 좋았지만 히트맵의 색상과 사이즈 등 미세한 부분들을약간 더 깔끔하게 수정하면 더 좋을 것 같습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310202545]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[284]]></uid>
		<content_uid><![CDATA[53]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[최적의 Index구조가 나오지 않더라도 그 실험에 대한 결론을 정리하면 되니  Index를 나누고 조합하여 더 좋은 clustering결과를 찾았으면 좋겠습니다. 다음주에 compact와 separation부분을 포함한 결과가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310203517]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[285]]></uid>
		<content_uid><![CDATA[56]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[항상 best 일수 없을지라도 robust결과물이 나온다는 것도 의미있는 결과가 될것 같습니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310230214]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[286]]></uid>
		<content_uid><![CDATA[57]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[topic의 비율이 hot인지 cold인지를 유의미한 결론이 나올것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310230336]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[287]]></uid>
		<content_uid><![CDATA[55]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[Deep Belief Network, Autoencoder, Convolutioinal Neural Network, Recurrent neural networks, Deep reinforcement Learning 등에 대한 내용을 쉽게 이해할수 있는 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310230412]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[288]]></uid>
		<content_uid><![CDATA[54]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[stepwise가 생각보다 좋고 나머지는 생각보다 안좋게 나온것 같습니다.
나온 결과를 어떻게 정리할것이지만이 문제로 남은 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310230520]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[289]]></uid>
		<content_uid><![CDATA[53]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[convex, non-convex로 나누어 집어넣고 실험한것이 유의미한 결과가 나올것으로 고대됩니다.
그리고 김형석님의 댓글이 인상깊네요]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160310230644]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[290]]></uid>
		<content_uid><![CDATA[53]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[실험결과 에서의 index 값을 비교한 것을 보니 클러스터링 구조에 따라서 잘 작동하지 않는 case도 존재하고, 모든 상황에서 최적의 지표 값을 얻는 것이 쉽지 않다고 느껴졌습니다. 그렇지만 이전에 없던 연구를 진행한 것이어서 연구로서의 가치가 높다고 생각하고, 향후 연구에서 발전될 것이라 기대해 봅니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160311063418]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[291]]></uid>
		<content_uid><![CDATA[56]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Train Data Set에서 Imposter data를 생성한 것처럼 기존 연구에서의 관점을 조금 벗어나 본인만의 연구결과를 보여준것이 인상적이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160311063923]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[292]]></uid>
		<content_uid><![CDATA[58]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[진행이 많이된 연구결과에 대한 발표를 볼 수 있었습니다. 알고리즘에 대한 분포그림, t-test 를 추가 함으로써 연구의 타당성을 가질 수 있을것이라 생각됩니다. 발표논문 초안이 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160311064204]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[293]]></uid>
		<content_uid><![CDATA[54]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[데이터 시각화 부분을 가독성 좋게 수정하고, improvement에서의 알파값 선택에서 다양한 알파값의 비교를 통해 알파값을 설정한다면 좋은 연구 결과가 될것이라 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160311064402]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[294]]></uid>
		<content_uid><![CDATA[57]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[hot&amp;cold로 토픽을 보여주어 흥미로웠고 결과가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160311091444]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[296]]></uid>
		<content_uid><![CDATA[55]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[다시 한번 딥러닝에 대해 정리할 수 있었던 시간이었으며 tenserflow에 대한 소개를 받았습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160311092037]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[297]]></uid>
		<content_uid><![CDATA[57]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Topic Clustering의 결과도 중요하지만 그것을 어떻게 해석하고 설명하느냐가 더 중요한데 그 부분에 대하여 해당 Domain Knowledge로 설명이 기ㅣ대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160314035217]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[300]]></uid>
		<content_uid><![CDATA[68]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[히트맵 부분에서 점더 명확한 그라데이션 효과를 통해 청중들에게 빠른이해를 도우는게 좋을것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160318163158]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[301]]></uid>
		<content_uid><![CDATA[59]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[25번 슬라이드의 표에서 O / X 의 좀더 명확한 구분을 가지면 좋을거 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160318163500]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[299]]></uid>
		<content_uid><![CDATA[58]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Multi co training 결과가 흥미롭게 나와서 집중해서 보았습니다.
따라서 다음에 해다분야와 비슷한 Document Classifier을 생성할때 참고가 될 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160314035336]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[302]]></uid>
		<content_uid><![CDATA[68]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[12페이지의 실험결과에서 가장 좋은 결과는 다른 색깔로 채우기를 한다든지 하여 청중들이 어떤 경우가 가장 좋은 결과값을 주는지 표현해 주면 좋겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160318164918]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[303]]></uid>
		<content_uid><![CDATA[59]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[연구배경에 청중들의 관심을 끌 재미있는 소재를 선택하여 수정한 부분이 인상 깊다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160318165055]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[304]]></uid>
		<content_uid><![CDATA[69]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[발표 자료입니다.
https://www.dropbox.com/s/v54zr7s7yf3zxe6/%ED%86%A0%ED%94%BD%20%EB%AA%A8%EB%8D%B8%EB%A7%81%EA%B3%BC%20%EC%82%AC%ED%9A%8C%EC%97%B0%EA%B2%B0%EB%A7%9D%EC%9D%84%20%ED%86%B5%ED%95%9C%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EC%97%B0%EA%B5%AC%EB%8F%99%ED%96%A5%20%EB%B6%84%EC%84%9D.pdf?dl=0]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320194824]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[305]]></uid>
		<content_uid><![CDATA[69]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[뉴질랜드에는 딥러닝이 활발히 연구 된 것 같지 않아 아쉽습니다. 
연구자 김형석님께서 디자인에 상당히 공을 들이 신것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320230951]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[306]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[연구 주제에서 가장 설명히 많이 필요한 부분이지 않을까 싶습니다. 발표자 분께서 많이 신경쓰셔서 발표하시거라고 믿습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320231054]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[307]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[대체로 PT구성이 깔끔한것 같습니다. 깔끔한 만큼 부분적으로 설명히 필요한 부분들이 있는 것 같은데 발표 당시 잘 설명해 주시면 될 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320231247]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[308]]></uid>
		<content_uid><![CDATA[72]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Multi Co Training에 대한 결과를 바탕으로 Document Classfier을 할시에 생성하여 해보면 좋겠다는 생각을 하였습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320235358]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[309]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[전하고자 하는 내용이 많아 이를 얼마나 효과적으로 잘 전달할수 있는가가 중요한 사안이 될것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320235414]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[310]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[GA가 빨리 돌아가서 전체적 비교를 통하여 결과를 해석해 보면 앞으로의 상황에서 좋은 사전지식을 얻을것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320235449]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[312]]></uid>
		<content_uid><![CDATA[69]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[캐나다의 Node가 크게 나온것이 재미있는부분으로써 발표할때 주위를 환기 시킬수 있을것 같습니다. Topic이 많아서 Hot Cold를 아카이브와 컨퍼런스 저널의 트렌드로 분석한 결과가 궁금합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320235606]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[313]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[많은 실험을 진행 한만큼 이에 대해서 청중들에게 잘 어필하여 그동안의 노고를 인정 받을 필요가 있다고 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320235629]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[315]]></uid>
		<content_uid><![CDATA[72]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[많은 실험에도 불구하고 다른 여타 팀들에 비해 발표자료 까지 높은 완성도를 가진 점을 칭찬하고 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160320235738]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[317]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Variational Inference의 대한 정의와 이때까지 행해왔던 방식이 궁금합니다. 교수님께서 말씀해 주셨던  Jordan Boyd-Graber선생님의 강의 (https://www.youtube.com/watch?v=2pEkWk-LHmU)를 들으면서 이해해보면 재미있을것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321000013]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[318]]></uid>
		<content_uid><![CDATA[69]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[피드백 받은대로 가독성만 더 좋아지면 나무랄데 없는 디자인이 될 것같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321000026]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[319]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[김 동화]]></user_display>
		<content><![CDATA[김준홍 학우의 조언 감사합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321000110]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[320]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[Variational inference는 많은 분야의 optimization problem에서 사용되므로 교수님께서 추천하신 강의를 통해서 이를 확실히 알아갈 필요가 있겠다 생각됩니다. 다음에 또 한번 접하게 되었을떄는 확실히 이해하고 넘어갈수 있도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321000114]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[321]]></uid>
		<content_uid><![CDATA[69]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[적절한 색깔과 모양으로 그래프를 잘 표현한 것 같습니다. 특히 토픽 네트워크 그림은 감탄이 나옵니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321124618]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[322]]></uid>
		<content_uid><![CDATA[69]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[최근 알파고로인해 각광받고있는 분야의 트렌드를 준비함으로써 많은 사람들의 관심이 집중될 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321124702]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[323]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[앞페이지에는 주황색으로 강조한 단어들이 있는데, 뒤로갈수록 전혀 없는 것 같아 페이지의 내용을 임팩트 있게 보여주기 위해서는 단어를 강조해주는 여러 방법들을 추가한다면 내용 전달시 도움이 되지 않을까 싶습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321125743]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[324]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[세미나를 통해서 bayesian statistics에 대한 공부를 해야 겠다는 생각이 들었고, 교수님의 설명을 통해서 variational inference의 사용 목적에 대해서 알 수 있었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321130151]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[325]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[keystroke 연구 주제는 처음 발표를 듣는 사람들에게는 생소하고 이해하기 쉽지않은 주제여서 한눈에 알아볼 수 있게 발표자료를 구성하고 설명이 필요한 부분에서 잘 설명해주는 것이 중요하다고 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321130351]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[326]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[bias를 얻는 대신 variance를 줄이는 방식(ridge, lasso)중 변수 선택을 하는 lasso가 성능이 좋지 않은 이유가 무엇일지 개인적으로 궁금합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321130538]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[327]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[데이터 설명에서 데이터에 대한 요약, GA에 대한 실험결과가 들어가고 발표에서 청중들에게 내용을 잘 전달할 수 있다면 좋은 발표가 될것이라 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321130617]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[328]]></uid>
		<content_uid><![CDATA[72]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[실험결과와 아이디어에 대한 설명을 잘 하는것이 중요하다는 것을 느꼈고, 데이터마이닝 학회와 산업공학학회에서의 두 발표자의 발표가 기대됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321130800]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[330]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[세미나를 통해 KL divergence가 true dist.와 예측한 dist.와의 차이를 거리개념으로 적용하였으며, 수식에서는 어떻게 표현되었는지 이해하게 되었습니다. 앞으로 여러 관련 분야 공부를 하면서 이런 아이디어(KL뿐만 아니라, variational inference의 사용 이유 등)가 많은 도움이 될 것이라 생각합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321131203]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[331]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[PPT 디자인이 깔끔하여 설명 해야 할 부분만 잘 풀어낸다면 좋은 발표가 될 것 같습니다. 남은 실험까지 잘 마무리하여 추가하길 바랍니다]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321143550]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[332]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[실험결과 표를 색깔로 구분하여 전달하고자 하는 바를 명확하게 한 것은 개인적으로 매우 인상깊었지만 PPT의 글씨들 또한 중요한 부분들을 좀 더 명확하게 구분짓는다면 좋을 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321144710]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[333]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[GA의 결과가 기존의 실험들과 조화롭게 잘 나오기를 기원합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321212609]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[334]]></uid>
		<content_uid><![CDATA[72]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[주 실험자가 아닌 다른 사람이 발표를 해야하기 때문에 많은 연습이 필요할거라 생각 합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321212929]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[335]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[발표에 의하면 Variational inference의 목적중 likelihood의 과적합을 방지하는 것이 있다고 하였습니다. 하지만 의문이 드는점이 있습니다. Likelihood 만으로는 가지고 있는 Data에만 과도하게 의존, 즉 과적합 될 우려가 있기 때문에 Prior probability로 이를 보정해준 결과가 Posterior probability가 아닌가 생각이 듭니다. 그렇다면 Variational inference와 Bayes' rule을 적용하여 구한 사후확률과의 비교가 필요하지 않나, 생각 합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160321213415]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[337]]></uid>
		<content_uid><![CDATA[71]]></content_uid>
		<parent_uid><![CDATA[326]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[lasso는 변수선택도 하기 때문에 비교적 적극적으로 variance를 줄이는 방법입니다. 실험에 사용한 데이터는 대부분 n&gt;p이고 주로 variance가 크지 않기 때문에 비교적 소극적 방법인 ridge의 예측성이 더 좋았던 것으로 보입니다.
좋은 질문 감사합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160323203454]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[338]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[제 발표자료는 단순해서 전달방법이 중요하다고 생각하는데 키스트로크 발표자료는 내용이 많아서 전달방법이 중요할 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160323204104]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[339]]></uid>
		<content_uid><![CDATA[70]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[제 발표자료는 단순해서 전달방법이 중요하다고 생각하는데 키스트로크 발표자료는 내용이 많아서 전달방법이 중요할 것 같습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160323204107]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[340]]></uid>
		<content_uid><![CDATA[72]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[짧은 시간에 많은 실험을 하고 발표자료까지 완성하여 많은 노력이 있었던 거라고 생각합니다. 남은 시간도 열심히 준비해서 두 발표자도 좋은 결과 있었으면 좋겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160323204721]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[341]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[maximum likelihood 와 expectation likelihood, variational inference와 variational inference 목적함수의 조건 대해 배웠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160323211617]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[342]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[Variational inference는 결국 기계학습 알고리즘에 있어서 목적함수가 매우 복잡한 함수일 때 그와  유사한 컨셉이지만 다소 간단한 새로운 함수를 정의, lower bound가 되게 함으로써 다소 간단한 새로운 함수를 최적화 시키는 방법이라고 이해할 수 있습니다. 이는 기계학습 알고리즘의 기본적인 컨셉인 target function approximation과 굉장히 유사한 개념이라고 생각됩니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160323225711]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[343]]></uid>
		<content_uid><![CDATA[76]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[인공데이터에 대해서는 교수님의 말씀을 빌리자면 최선은 아니지만 차선은 됐다고 들었습니다. 최고의 index는 아니지만, 공동1등까지 되는 상황이었지요. 현실 데이터 셋은 보다 복잡하며, 그 클러스터의 갯수를 정확히 알지 못하기에 더욱 이 지표가 필요하지 않을까 생각합니다. 좋은 결과 있길 기대해봅니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160324161251]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[344]]></uid>
		<content_uid><![CDATA[73]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[maximum likelihood에 대해 동영상 강의에 덧붙여 추가적으로 준비한 예시들을 통해 더 잘 이해할 수 있었습니다. variance inference 에 대해  공부해봐야겠다는 생각을 하게되었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160325082055]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[345]]></uid>
		<content_uid><![CDATA[76]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[더 복잡한 실제 데이터에 적용했을 때 인공데이터보다 더 좋은 결과가 나올 수 있을지 우려가 되기도 하지만 그래도 이번 실험에서는 좋은 결과가 나오길 기대합니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160325082522]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[346]]></uid>
		<content_uid><![CDATA[80]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Newton's method는 saddle point를 인식하지 못한다는 문제가 있었다. 또한 특정 경우에 우리가 원하는 error function의 range를 벗어나 발산할 때도 있었다. 이 차 미분계수값을 weight로 사용했을때 이런 문제점이 생긴다면 3차 이상의 고차 미분계수로 앞서 서술한 문제점을 극복할 수 있지는 않은지 궁금하다. neural net에서 사용하는 activation function과 output function이 선형이거나 exponential function이기 때문에 고차 미분계수를 구하는게 어렵지 않을 것이기 때문에 Newton's method를 보완하는데 더 수월할거라 생각했다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160327153431]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[347]]></uid>
		<content_uid><![CDATA[76]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[DEA는 선형, 비선형, 구형, 그리고 추가로 몇 가지의 다른 군집을 포착하는 indices를 포함하고 있기 때문에 data에 상당히 robust할거라 예상된다. 따라서 여러 실제 data에서도 좋은 결과가 나올거라 기대한다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160327153702]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[348]]></uid>
		<content_uid><![CDATA[81]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[예상과 달리 실제 data에서 좋지 않은 결과가 나왔다. 실험자가 너무 실망하지 않고 힘을 내기를 바란다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160327153846]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[349]]></uid>
		<content_uid><![CDATA[81]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[인공 data에서는 좋은 성능을 보이던 DEA가 실제 data에서 좋지 못한 결과를 얻었는데, DEA의 약점을 잘 파악하고 개선해 나가는 방향이 찾아지기를 기대해봅니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160328100537]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[350]]></uid>
		<content_uid><![CDATA[80]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[이번 세미나에서는 Deep Neural Net의 기본이 되는 backpropagation과 그것의 수학적 배경이 되는 gradient descent에 관해 중점적으로 익혔으며, 여러 함수의 형태별 미분에 대한 결과를 다시 복습하는 시간이었습니다. DNN에서 기본적으로 쓰이는 수학적 기법이므로 더 확실한 이해가 필요하겠다 생각하였으며, 교수님 말씀대로 머리속에 완벽히 정리되어 모르는 사람에게 설명해 줄 수 있을 정도로 기본사항만큼은 확실히 익혀두어야 겠다는 생각을 했습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160328101355]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[351]]></uid>
		<content_uid><![CDATA[80]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 세미나에서는 neural network 학습에 필요한 backpropagation에 대해 배웠습니다. 그 중에서도 gradient descent와 newton방법을 복습했는데 예전에 교수님께서 새로운 내용을 배우는 것 만큼 이전에 배운 내용을 기억하는 것도 중요하다고 말씀하셨던 것을 실감하는 세미나였습니다.다시 한번 확실하게 정리해서 막힘없이 써내려갈 수 있는 경지까지 도달하도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160331133407]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[352]]></uid>
		<content_uid><![CDATA[81]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[비록 기대와 달리 결과가 좋지 않더라도 이번 결과를 잘 정리해서 마무리했으면 좋겠습니다. 더 많은 데이터에 적용했을 땐 좀 더 좋은 결과가 나왔으면 좋겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160331133638]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[353]]></uid>
		<content_uid><![CDATA[80]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[Neural Net의 학습에서 중요한 backpropagation 과정에 대해 다시 공부할 수 있었으며 decent에 대해 자세히 배울 수 있는 시간이었습니다. 금주 세미나를 계기로  backpropagation을 다시 정리하여 되새기도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160331172714]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[354]]></uid>
		<content_uid><![CDATA[84]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[같은 연구이니만큼 서로 부족한 부분을 잘 캐치해줘서 모두 윈윈하는 다음주가 되면 좋겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160407165912]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[355]]></uid>
		<content_uid><![CDATA[85]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[발표 때 좀 급해보였지만 전달해야 할 내용을 모두 잘 말한 것 같습니다.  학회 당일 까지 힘내길 바랍니다!!]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160407170213]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[356]]></uid>
		<content_uid><![CDATA[84]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[첫 발표인만큼 상당히 열심히 준비를 해온것 같습니다.
보고 반성하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160407171303]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[357]]></uid>
		<content_uid><![CDATA[85]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[발표준비를 완벽히 잘 준비해온모습을 보고 제 스스로를 되돌아 보는 계기가 되었습니다.
데이터마이닝에서 좋은 발표 기대하도록 하겠습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160407171425]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[358]]></uid>
		<content_uid><![CDATA[88]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Weight update에 관한 다양한 관점에서의 시각을 살펴 볼수 있는 좋은 시간이었습니다.
실제적으로 Deep Neural Network를 사용할 경우가 생겨서 실제적으로 체험해 보고 싶었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160413020114]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[359]]></uid>
		<content_uid><![CDATA[88]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[gradient vanishing이 수식에서 어떻게 드러나는지 확실히 알게되었으며, 이를 해결하기 위해 activation function이 어떻게 변화하며 적용되었는지 알게되는 시간이었습니다. 여러 gradient descent optimization을 시각적으로 살펴보는 시간이었습니다.]]></content>
		<like><![CDATA[]]></like>
		<unlike><![CDATA[]]></unlike>
		<vote><![CDATA[]]></vote>
		<created><![CDATA[20160418123615]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[360]]></uid>
		<content_uid><![CDATA[114]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Filter를 이용한 Convoultioning 이라는 개념이 옥스포드 강의 만으로는 이해하기 쉽지 않았다. 세미나 발표에서 한 가지 이상의 소스를 이용해 다양한 측면에서 Convolution의 개념을 설명해 주어서 개념습득에 큰 도움이 되었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160430205756]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[361]]></uid>
		<content_uid><![CDATA[114]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[뉴럴넷을 배우면서 자연스레 듣게되는 convolution 구조에 대해 이해하는 시간이 되었습니다. 혼자 공부하기에 많이 벅찼는데, 이번 세미나를 통해 앞으로 convolution neural net을 공부할 때 큰 도움이 될거라 생각합니다. 자료를 보며 좀더 깊게 공부하고, TensorFlow또한 욕심내보고자 합니다. 너무 유익한 시간이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160502225108]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[362]]></uid>
		<content_uid><![CDATA[114]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[convolutional neural network의 구조에 대해서 잘 설명해 주셔서 좋았고, 기본적인 구조만 알고 있었는데 변형된 cnn 구조들을 배울 수 있어서 도움이 많이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160503163903]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[363]]></uid>
		<content_uid><![CDATA[114]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[과거 선형대수시간에 잠깐 스쳐지나가면서 배웠떤 kronecker product가 cnn의 back-propagation 과정에서 활용되는 점이 흥미로웠고 세미나 준비에 앞서 이론만 공부하는 것이 아니라 실제로 구현을 해보는 자세에 배울 점을 느꼈습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160505095618]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[364]]></uid>
		<content_uid><![CDATA[114]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[Convolution network를 처음 접하고 이해하기 쉽지 않았는데 이번 세미나에서 구체적으로 설명해주어서 잘 이해할 수 있었습니다. 특히, 마지막에 연구실 홈페이지에 있는 실제 사진에 적용했던 것이 신기하고 인상깊었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160505164539]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[365]]></uid>
		<content_uid><![CDATA[114]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[cnn을 이해하는데 큰 도움이 되었고 실제 이미지를 이용해 구현한 것이 흥미로웠습니다. 설명도 잘 해주셔서 여러가지로 배운 것이 많았던 시간이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160505210813]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[366]]></uid>
		<content_uid><![CDATA[114]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[지난 한주동안 서로 공부하면서 많은 의견을 나누어가면서 공부해서 간만에 재미있게 한주 스터디를 진행 해왔던것 같습니다. 특히 서로 이해가 부족한 부분에 대해서 혼자보단 둘이 나으다는 느낌을 확실히 얻었습니다. 앞으로도 계속해서 서로 부족한 부분 채워가면서 앞으로 남은 어마어마한 양의 지식을 저희의 것으로 만들어 갑시다!!]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160506140407]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[367]]></uid>
		<content_uid><![CDATA[120]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[공부하면서 저역시 많이 어렵다고 느낀 part였는데, 많은 시간을 투자해서 잘 설명해준점 감사합니다.
추가적으로 마지막에 언급한 LSTM의 경우 각종 패키지에서 이미 잘 구현되어있으며, 좀더 궁금하신 부분은 &lt; http://whydsp.org/280  &gt; 여기에 한글로 잘 설명되어 있어 이해하시는데 도움이 되리라 생각됩니다.  다들 좋은 주말 보내시기 바랍니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160506141909]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[368]]></uid>
		<content_uid><![CDATA[120]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[input의 개수가 유동적이라는 점은 매우 큰 장점을 가질 것이라 생각합니다. 세미나를 통해 여러 구조가 가능하다는 것과 어떻게 학습이 이루어지는지, 적용 분야는 어떻게 되는지 알게되는 시간이었습니다. 교수님께서 마지막에 현재 추세가 RNN, CNN등을 섞어서 사용하는 등 점점 복잡한 구조를 구축하는 것이라고 말씀하신만큼, 특히 시간을 내서 공부해야 하는 분야구나 라는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160508013908]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[369]]></uid>
		<content_uid><![CDATA[120]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 매우 도움이 되는 세미나 였습니다. 특히 흥미롭다고 생각한 점은 GRU(Gate Recurrent Unit)에서 현재의 값에 1- f(t)가 곱해지는 부분이 있는데 그 부분에 어떤 개념이 녹아들어가 있는 지 궁금한 것이 있습니다. 현재 정보를 완전하게 못 받아들이는 부분이라고 해석할 수 있는지 개인적으로 고찰해보아야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160511161706]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[370]]></uid>
		<content_uid><![CDATA[120]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 세미나에서는 RNN이 굉장히 다양한 종류의 구조를 가진다는 것을 알수 있었고 각 구조를 알맞게 선택하여 사용할 줄 아는 융통성이 필요하다고 느꼈습니다.마지막의 LSTM부분은 혼자 이해하는데 어려움을 겪고 있었는데 많은 도움이 되었습니다.추가적으로 변형된 LSTM에 대해서도 더 공부해봐야겠습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160512232930]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[371]]></uid>
		<content_uid><![CDATA[123]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[컴퓨터 비전에 쓰이는 여러 개념을 소개해 주어서 앞으로 비전을 공부하게 된다면 오늘 세미나 발표가 유익하게 작용할 것이다. 기존의 컴퓨터 비전 방법들이 필터를 사람이 설계 하였다는 점이 흥미로웠다. CNN은 필터를 자동으로 학습한다는 점에서 CNN이 어떤 의미에서 딥러닝 방법이 기존의 기계학습 방법론들과 차별화 되는 특징을 가지고 있다는 점을 이해 하였다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20160516151406]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[372]]></uid>
		<content_uid><![CDATA[123]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[랩에서 잘 다뤄지지 않은 Image에 대한 발표여서 유심히 청취하였습니다.
아직 잘 이해가 되지 않는 부분은 convolution와 cross correlation의 차이점과 명확한 이해인데, 이 부분에 대해서는 좀더 공부 해야 할것 같습니다. 그리고 앞으로 각 상황에 맞는 Image처리와 요즘 많이 진행되고 있는 image,text,나아가서 영상과의 조합을 통한 학습 방법과 그 application도 공부해야 되겠다고 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160516202813]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[373]]></uid>
		<content_uid><![CDATA[123]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 세미나 시간에서는 컴퓨터 비전에 대한 개념과 다방면으로 어떻게 사용되는지에 대해 배울 수 있었습니다. 컴퓨터 비전과 관련된 다양한 종류와 사례위주의 설명으로 쉽게 이해할 수 있었습니다. 한가지 아쉬운 점은 개인적으로 hugo강의 후반부의 수식부분에 대한 자세한 설명을 기대했는데 수식에 대해서는 간단하게  넘어가서 아쉬웠습니다. 이 부분에 대해서는  좀 더 공부해야 할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160517162822]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[376]]></uid>
		<content_uid><![CDATA[123]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[컴퓨터 비전과 관련한 여러 개념과 응용사례를 설명해주어서 쉽게 이해할 수 있었던 세미나였습니다. 기존에 자주 접하지 못한 분야라 어려웠을 거라 생각하지만 강의자료에서 다룬 내용도 자세히 다뤘다면 더 좋았을 것 같습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160517172136]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[377]]></uid>
		<content_uid><![CDATA[123]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[컴퓨터 비전이라는 주제에서 사용되는 여러 용어들과 기본 개념을 알게된 시간이었습니다. 앞으로 이미지 처리 분야가 더욱 발전할 것이라고 믿는데, 그런 측면에서 보면 꼭 필요한 세미나 시간이었다고 생각합니다. 여러 예시를 통해 이미지가 처리되는 것을 보았는데, 각각의 필터로 어떻게 픽셀 값이 계산이 되는지, CNN에서는 수식에 따라 어떤 흐름으로 계산이 되어지는지에 대한 내용이 추가되었다면 더욱 완벽한 세미나가 되었을 것이라 생각합니다. 질문(축에 대한 미분)에 대한 답변으로는 차분이 아닐까 라는 생각을 다같이 했었는데, 미분의 정의에서 delta x부분이 1(픽셀단위에서 가장 작은 움직임)이 되어 차분의 개념이 적용된 것을 확인할 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160518001646]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[378]]></uid>
		<content_uid><![CDATA[120]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[RNN은 주로 활용 되는 자연어 처리 뿐만 아니라 이미지와 같은 비정형 데이터, 특히 시퀀스를 가지는 자료형에 대해 강력한 성능을 보여주는 유용한 도구이다. 개인적으로 Teacher forcing이 어떻게 Back propagation through time (BPTT)문제를 해결해 주는지 궁금 했는데, 거기에 대한 설명이 조금 부족한득 하여 아쉬웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160519131108]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[379]]></uid>
		<content_uid><![CDATA[123]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[컴퓨터 비전분야에서 사용되는 용어들을 쉽게 설명해주어 이해하는 데 도움이 되었던 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160520215812]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[380]]></uid>
		<content_uid><![CDATA[127]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[겨울방학에 진행한 실험에서 text data를 사용하며 여러 문서 표현 방법론을 접할 기회가 있었는데 배경지식(워드 벡터, 워드 임베딩 등)이 부족하여 논문도 잘 안읽히고 방법론들이 쉽게 다가오지 못한 부분이 있었습니다. 세미나를 통해 관련하여 익히게 되어 유익한 시간이었으며, 델타를 이용해 weight를 update하는 방식에서 열띤 강의를 해준 모습이 인상깊었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160525124121]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[381]]></uid>
		<content_uid><![CDATA[127]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[소개해준 델타를 이용하는 "켈리포니아 방식"의 update이 인상깊었습니다. 복잡한 여러 수식 대신에 델타 하나로 취급하여 편리하다고 느꼈습니다. 다음에 비슷한 주제로 세미나를 진행하게 될 때 NLP와 관련된 딥러닝 방식을 집중적으로 듣고싶습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160527112616]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[382]]></uid>
		<content_uid><![CDATA[127]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[발표에서 흥미로웠던 것은 기존 Text classifier에서의 문제점으로 제기되는것이 새로운 데이터에 대한 모델의 유연성문제가 있는데, 이 부분에 대하여 새로운 term이 생성되었을때 이전의 weight를 사용하고 새로운 node를 연결시켜 다시 학습 시킨다는것이 재미있었습니다.
이 부분은 실험을 해보지 않아서 여러 의문점이 생기는데, 시간이 난다면 해봐야 할 것 같습니다.
(Q1) 새로운 node에 연결되어 있는 새로운 weight들은 initialization을 을 어떻게 할 것인가?
(Q2) 다른 weight가 어느정도 fitting이 되어 있는 상태에서 학습이 잘 될까? 라는 의문이 있습니다. 왜냐하면 output node에서 나오는 cost function자체가 작아진 상태이기 때문에 과연 제대로된 weight를 찾을까?
라는 의문이 있습니다.  유익한 발표 잘 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160527113121]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[383]]></uid>
		<content_uid><![CDATA[127]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[Local signal을 정의해서 활용하는 방법과 Kronecker product를 이용하여 역전파를 식으로 풀어낸 것이 흥미롭고 익숙해지면 아주 좋은 표현인듯하다. 또한 이번학기 비정형수업을 들으면서 배웠던 개념들을 조금 더 깊이있게 들을 수 있어서 좋았다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160527123720]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[384]]></uid>
		<content_uid><![CDATA[127]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[NLP와 딥러닝에  대해 더 자세히 배울수 있어 좋은 시간이었습니다. 세미나를 들으면서 xc가 딥러닝에서 실제 어떻게 쓰이는지 궁금했습니다.  추후 더 공부해보아야 겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160527135024]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[385]]></uid>
		<content_uid><![CDATA[132]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[분석을 하는 사람들에게서 실용적으로 쓰이고 있지만 거의 알지 못하는 Bayesian Network의 첫 세미나 시간이었습니다. 오늘 보섭이가 발표해준 내용은 인과관계가 있는 내용으로 설명하였지만 서술하였듯이 결국 전체 Node에 대한 network에서 마지막 output을 만들어내는 과정이 궁금해졌습니다.
후에, 새로운 데이터 상에서 분류기를 만들었을때, 시간적 순서가 있을때 등의 경우에서 어떤식으로 어느정도의 성능으로 만들어지는지가 점점 궁금해 집니다. 발표 잘 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160528205602]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[386]]></uid>
		<content_uid><![CDATA[132]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Bayesian 방법은 정말 보면 볼수록 well-define된 정리이다. 엄밀히 증명해 보지는 않았지만 이번 단원에서 소개된 reasoning pattern의 관계식들이 결국은 Bayesian정리의 정의에서 따라나온다고 생각된다. 여기에 의거해 분석하기 까다로운 복합적인 현상을 요인별로 분해하여, 즉 factorization하여 보기에도 간편한 network방식으로 나타낼 수 있다는게 인상 깊었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160529010635]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[387]]></uid>
		<content_uid><![CDATA[132]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[새로이 시작하는 Bayesian Network에 대한 세미나의 첫 단추를 방장으로서 잘 맞춘 세미나라고 생각됩니다. 비록 준비하는 시간이 부족하였지만 강의에 대한 이해를 스스로의 해석으로 연구실 구성원들에게 잘 전달하였습니다. 이러한 베이지안 네트워크같은경우 인과과계의 방향성을 가지는 네트워크로서 질병 과 증상 사이의  구조를 파악하는데 도움이 되리라 판단하여, 현재 SAS에서 개최하는 마이닝 대회의 주제("한국인의 질병 네트워크를 그리다-동반 질병 위험도 예측’)에 부합하는 분야가 아닐까 생각됩니다. 혹시 공부하시면서 관심 있으신분들은 스터디와 더불어 실습하는 차원에서 지원하는 것도 좋은 방안이라 생각됩니다. 이상입니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160602204748]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[388]]></uid>
		<content_uid><![CDATA[127]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[자연어처리와 자연어처리가 딥러닝에서 어떻게 쓰이는지 알 수 있었습니다. 딥러닝 개념을 적용한 내용이 아직까지는 쉽게 이해되지 않지만 텍스트 마이닝 과정에서 필요한 부분이라 느꼈습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160603092314]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[389]]></uid>
		<content_uid><![CDATA[132]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[probability 개념과 graph model이 융합된 pgm에서의 중요한 개념인 Bayesian network에 대한 세미나를 들었습니다. 처음보는 개념들이어서 낯설었지만 세미나를 통해서 Bayesian network에 대해서 조금 더 알게되었고, 향후 연구에서 기계학습 분야만큼 중요하게 다루어야될 주제라는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160603092711]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[390]]></uid>
		<content_uid><![CDATA[132]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[조건부 확률들의 결합을 Bayes network를 이용해서 표현하는데, 좀 더 시각적으로 그 variable 사이의 독립, 종속성을 파악할 수 있음이 좋은 성질이라고 생각합니다. 앞으로는 이를 기반으로 여러 형태의 변형, 진화된 모습을 볼 수 있을 것인데 모두 이해하도록 노력해야겠다 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160603123329]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[392]]></uid>
		<content_uid><![CDATA[132]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[새로 시작하는만큼 기본 개념을 다루었는데 자세한 예시와 식으로 설명해주어서 이해하기가 수월했습니다.  세미나에서 주로 인과관계와 관련하여 설명하고 있는데 실제 데이터에서 이런 인과관계를 알고 있는 경우가 드문 걸로 아는데 이것이 실제로 어떻게 적용되고 있는지 궁금했습니다. 시작하는 부분이라 I-map이 좀 생소했는데  이 부분도 추가적으로 공부해야겠다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160603130644]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[393]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[이번시간에는 어느정도의 제한된 조건안에서의 네트워크에 대하여 발표해 주었습니다.
이번세미나까지의 학습 내용으로는 아직 큰 데이터 상에서 어떻게 해석과 앞으로 어디에 어떻게 적용시켜야 되겠다는 아디디어가 아직까지는 크게 와닿지 않는것 같습니다. 수현이가 발표한 부분과 앞으로 배워나갈 것들을 다시 한번 곱씹어보며 현재 프로젝트 안에서는 어떻게 적용하고 그 목적과 장단점에 대하여, 그 기저가 되는 수식에 대하여 공부해야 되겠다고 생각이 들었습니다. 유익한 시간이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160604192343]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[394]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[음성인식의 강자였던 HMM이 최근 딥러닝에 왕좌를 내줬지만 HMM의 구조를 학습하면서 기계학습 전반에 대해 배울 수 있는 통찰이 많고 또한 교수님이 말씀 하셨듯이 언제 HMM이 다시 부활할지 모르기 때문에 흥미롭게 들은 발표였습니다. HMM을 차치하고라도 Markov assumption은 확률통계를 이용한 기계학습에서 여러번 강조하여도 모자라지 않을 중요한 가정이기 때문에 다시 한 번 주의깊게 복습하였다. 궁금한 것은 Template model이 sequence가 있는 자료를 다루고 반복되는 Parameter를 가진다는 점이 RNN과 꼭 비슷한 구조를 가지고 있다. 이 둘의 관계가 궁금하다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160604224541]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[395]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[Markov assumption에 대해 아주 간단히 알고있었는데 이번 세미나에서 예시를 통해 설명을 들으니 이해가 잘 됐습니다. HMM은 특히 보이지 않는 seq.를 확률적으로 추론한다는 점에서 분명 강점이 있다고 생각됩니다. 조건부확률테이블을 만든 뒤 그것이 update되지 않고 쭉 사용된다는 점이 의아하긴 했는데, 교수님께서 말씀하신대로 추론할 모수가 더 많아지기에 trade off가 있다고 생각됩니다. 획기적인 방법이 있는지 찾아봐야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160605005305]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[396]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 논문이나 기타자료에서 어떤 모형을 설명할때 graphical representation으로 알고리즘의 구동을 설명하는 경우가 많은 데 해당 representation에 대해서 이해를 키울 수 있는 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160606145834]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[397]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[첫 세미나 발표로서 충분히 많은 준비의 시간을  투자함을 느낄 수 있는 발표였습니다.
전반적으로 발표의 내용에는 충실하였으나, 중간중간 음... 아... 하면서 매끄럽게 진행 되지못한 점은 앞으로 세미나를 계속 진행하면서 나아질거라 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160608131001]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[398]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[397]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[형석 김]]></user_display>
		<content><![CDATA[좋아용]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160608131114]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[399]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Markov models, Hidden Markov models 에 대해서 복습할 수 있었습니다. plate model이라는 새로운 구조에 대해서 배우게 되었서 좋았고, 세미나에서 배운 내용들을 토대로 연구모형에 적용해 봐야 겠다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160608161045]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[401]]></uid>
		<content_uid><![CDATA[135]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[수업에서 들었던 HMM을 다시 상기할 수 있어서 좋은 시간이었습니다. 하지만 실제 모델링에 어떻게 응용되는지 와닿지가 않아 혼자 고민하고 더 공부할 부분이 많은 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160610141221]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[402]]></uid>
		<content_uid><![CDATA[139]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[강의만 들었을 땐 체계적으로 정리가 되지 않았던 맥락들을 세미나를 들으며 정리 할 수 있었습니다. CPD모델을 시그모이드 함수로도 사용할 수 있다는 점이 인상깊었고 아직은 이론을 배우는 단계이지만 실제 문제를 해결하는데 어떻게 활용되는지 직접 확인해보고 싶다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160610193145]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[403]]></uid>
		<content_uid><![CDATA[139]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Coursera강의에는 소개 되지 않았던 Multinet, deterministic separation과 같은 개념을 찾아와 발표에서 소개해준 점을 높이 평가하고 싶다. 강의 커리큘럼 앞부분에서 지속적으로 소개해 주고 있는 Independecies의 관점에서 그래프를 단순하게 표현가능 하다는 점이 흥미롭다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160611154416]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[404]]></uid>
		<content_uid><![CDATA[139]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[그래프 구조가 신경망 구조와 흡사함이 그림으로 표현되어 있어서 이해하기 용이했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160613134753]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[405]]></uid>
		<content_uid><![CDATA[139]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[그래프 구조에서 sigm. func.을 이용하는 점에서 보섭이형 처럼 신경망 구조와 비슷하다는 점이 인상깊었습니다. PGM과 DNN이 서로 교호되는 부분이 여럿 있어서 병행해서 공부하면 좋다는 말을 본적 있는데 이번 세미나 내용이 그 한 예가 되지 않을까 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160616175743]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[406]]></uid>
		<content_uid><![CDATA[139]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[다양한 자료를 통해 발표를 진행 하려고 한점이 좋았습니다. 해당 분야를 좀더 공부할때 유용할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160625160333]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[407]]></uid>
		<content_uid><![CDATA[139]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[CPD의 내용에 대해서 쉽게 이해 할 수 있어서 좋았습니다. 강의자료 이외의 자료에서 추가적인 예시가 있으면 좋을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160629122341]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[408]]></uid>
		<content_uid><![CDATA[147]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[discriminative 모델에 이해에 도움이 되었다. 궁금한 점은 binary classification을 학습하는 인공신경망도 결국은 discriminative model이고 이는 cross-entropy &lt;=&gt; negative log likelihood를 최소화하는 문제와 같은 문제로 귀결 되는데 수식으로 써봤을때 negative log likelihood라는 naming이 맞는 것인가하는 점이다. 예를들어 
h를 우리가 생각하고있는 신경망 모델 y를 binary class, x를 설명변수라고한다면
p(y|x,h) = p(y,x|h)p(h) /( p(h)*p(x)) = p(y,x|h) / p(x)가 되고 p(y,x | h)를 h(x)^y*(1-h(x))^1-y로 생각하여 최대화 하는 것인지 아닌지 헷갈리는 부분이다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160630104308]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[409]]></uid>
		<content_uid><![CDATA[147]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[방향성의 존재여부에 따라, 그리고 그 구조가 어떻게 되어있느냐에 따라 조건부 독립이 되기도, 안되기도 한다. 여기서 chordal이라는 개념으로 Markov net에서 Bayes net으로 변환시 독립성을 보존하는 것을 확인할 수 있었던 점이 흥미로웠다. 그리고 기존에 구성된 net에서 clique와 potential function으로 변환하여 생각하는 것도 재미있는 부분이었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160701220836]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[410]]></uid>
		<content_uid><![CDATA[147]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[베이지안 네트워크보다 이해하기 어려웠다. 상호 작용을 고려하여 방향성이 없는 엣지로 네트워크를 표현하는 것 까진 받아들이기 쉬웠으나 확률분포표가 구성되는 원리가 아직 모호하다. 동영상 강의 시청후 내가 이해한 부분과 그렇지 않은 점을 발표를 통해 명확히 하였고 부족한 부분에 대해서는 개인적으로 학습 해야겠다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160705100459]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[411]]></uid>
		<content_uid><![CDATA[147]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[CRF에 대한 NER부분에 대하여서 좀더 명확히 알고 싶어진 발표였습니다.
앞으로 NER의 대한 연구가 지속됨에 따라, 해당 알고리즘과 다른 알고리즘의 대하여서 한글에서 어떤 결과를 나타내는지 궁금해 졌으며 시간이 나면 실제로 행해보려고 합니다.
발표 잘들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160705160411]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[412]]></uid>
		<content_uid><![CDATA[147]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[나현 학우의 열띤 발제 잘 들었습니다! 아직은 따라가기 벅차지만 열심히 공부해서 랩 세미나에 정상적으로 참여하도록 하겠습니다. 발제 준비하느라 고생 많으셨어요!]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160705172956]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[413]]></uid>
		<content_uid><![CDATA[150]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[variable elimination이라는 개념이 변수를 제거한다기 보다는 marginalize한다는 개념으로 다가오는데 정의된 명칭이 조금 헤깔린다는 생각을 했습니다. P vs NP에 대해 깊게 설명해준 부분이 흥미로웠으며 이후 진행될 ordering관련 발표도 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160711233917]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[414]]></uid>
		<content_uid><![CDATA[150]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[knowledge engineering,N,NP 등의 생소한 개념들과 변수 제거 방법에 따른 복잡도에 대해 배울 수 있던 세미나였습니다. 또한, MAP와 MLE의 차이점, 특징을 파악할 수 있었고 수식이 다소 어려웠지만 차주 세미나에서 남은 내용에 대해 더 명쾌한 설명이 있을 것으로 기대됩니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20160711235943]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[415]]></uid>
		<content_uid><![CDATA[150]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Knowledge engineering 파트에서 발표자가 전달하고자 하는 바가 명확하지 않다고 느껴졌다. 대부분의 추론과 학습 과정이 NP-Hard문제이란 점이 흥미로웠다. 기계학습에서 전역탐색과 같은 방법은 간단한 추론 문제도 해결할 수 없다는 점을 시사한다고 생각한다. 근사화가 기계학습에서 필수적인 요인임이 새삼스레 느껴진다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160712000840]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[416]]></uid>
		<content_uid><![CDATA[150]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[discriminative model이 가지는 강점에 대해서 공학적 설계관점에서 느끼는 바가 많았다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160712100712]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[418]]></uid>
		<content_uid><![CDATA[150]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[MAP, time complexity 등 새롭게 다시 공부할 수 있었던 시간이었습니다. variable elimination이 좀 헛갈리는데 차후 발표를 통해 더 명확한 설명을 들을 수 있을 거라고 기대합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160712101825]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[419]]></uid>
		<content_uid><![CDATA[150]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[첫 시발점인 NP hard 에서 연구자들의 생각의 흐름을 옅볼수 있는 재미있는 시간이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160712192953]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[420]]></uid>
		<content_uid><![CDATA[152]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[저번 발표에서 다루지 못했던 내용을 보충하려 노력한 흔적이 보인다. 후반부에서 네트워크 구조를 어떻게 학습할지 기대된다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160715121855]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[421]]></uid>
		<content_uid><![CDATA[152]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[지난 시간에 비해 슬라이드를 예제위주로 구성하여 이해하기가 편했다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160718202832]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[422]]></uid>
		<content_uid><![CDATA[152]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[시간 복잡도라는 것이 얼마나 중요한 것인지 이번과 저번 발표를 통해 많이 느꼈습니다. 이제까지는 코딩을 하면 무조건 output을 얻어서 볼 수 있는 문제들만 해결해 왔던 것이었음을 느꼈고, 돌아가게 짠다고 해도 효율적이지 않다면 살아있는동안 output을 볼 수 없을 수도 있음을 확실히 알게 되었습니다. PGM의 분야 뿐만 아니라 여러 분야에 대해 앞으로 여러 코딩을 진행할 때, 더 효율적인 방법을 찾고, 더 빠른 라이브러리, 패키지를 이용하는 것이 중요한 문제임을 알게 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160718220933]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[423]]></uid>
		<content_uid><![CDATA[152]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[기존에 알고있던 데이터마이닝 알고리즘과 관련된 variable selection, extraction에 대해서는 익숙하지만 뭔가 PGM에서 변수를 제거한다는 것은 익숙하지 않아서 그런지 생소하고 잘 와닿지 않습니다. 앞으로 사용해 볼 기회가 있었으면 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160719010656]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[424]]></uid>
		<content_uid><![CDATA[152]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[앞으로의 세미나에서 어디에 적용할지 생각해 가면서 들어볼것이고 다음주에 발표할 예시들이 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160719015328]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[425]]></uid>
		<content_uid><![CDATA[152]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[예시를 통해 variable elimination을 더 잘 이해할 수 있었습니다. 그리고 앞으로 세미나를 준비하면서는 각 강의에서 다뤄지는 내용들의 필요성이나 다뤄지는 이유 등에 더 기울여 공부해야겠다고 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160719125047]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[426]]></uid>
		<content_uid><![CDATA[152]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[다음 발표자로서 variable elimination에 대한 정의가 잘 다룬 발표라고 생각됩니다.
하지만 보다 보기쉬운 그림을 통한 예제를 통해서 풀어 갔다면 더욱 좋은 발표가 되었으리라 생각됩니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160719125459]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[427]]></uid>
		<content_uid><![CDATA[154]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[이해하기 쉬운 예시와 텍스트 분석에서의 예시를 통해 들어서 좀 더 와닿는 세미나가 되었다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160720223318]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[428]]></uid>
		<content_uid><![CDATA[154]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[강의자료에 있는 내용과 예시를 넘어선 발표자만의 예시와 시각 자료 덕분에 잘 이해할 수 있었습니다. 하지만 이러한 시각적 자료들로 설명을 조금만 더 천천히 했더라면 듣는 사람들이 더 잘 이해할 수 있었을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160727075522]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[429]]></uid>
		<content_uid><![CDATA[154]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[노드와 엣지로 이루어진 그래프를 클러스터로 표현하는 이유와 방법이 아직 충분히 이해 되지 않는다.  그렇지만 해당 발표를 통해 belief propagation의 대략적인 흐름을 파악할 수 있었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160727111416]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[430]]></uid>
		<content_uid><![CDATA[154]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[message parsing을 예제로 잘 설명해주어 이해가 잘됬으나, message라는 개념에 대해서는 내 나름대로 정립하기에는 애매모호한 점이 있는 듯하다. 관련 서적을 찾아봐야할 것 같다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160727144157]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[431]]></uid>
		<content_uid><![CDATA[160]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Dual decomposition으로 문제를 풀었을때 계산 효율성을 Big-O notation과 같은 지표로 보여주었으면 이 방법이 갖고 있는 강점이 더욱 부각 되었을 것이다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160728150214]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[432]]></uid>
		<content_uid><![CDATA[154]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[강의자가 쉽게 설명해 주셔서 좋은 세미나 였습니다. Message passing의 개념을 새로운 예시로 설명하여서 어떤 내용인지 이해 할 수 있었고, cluster graph와 clique tree의 차이점을 판서로 설명을 해주셔서 알 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160728161147]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[433]]></uid>
		<content_uid><![CDATA[160]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[divide and conquer 부분에 대해서 더 알게된 세미나 였고, 발표자가 쉽게 설명해주어서 좋은 세미나 였습니다. 다만 수업자료 이외의 예시가 더 있으면 좋겠다는 생각이 들었고, 개인적으로 Dual decomposition 내용이 어려웠는데 다시 공부해야 될 필요성을 느꼈습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160728161541]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[434]]></uid>
		<content_uid><![CDATA[160]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[곱셈을 덧셈으로 바꾸어 접근한다는 아이디어가 과연 가능한가 의심이 되는 부분이 있긴 하다만 어느정도의 영향을 반영하기는 할 것 같다는 생각을 했습니다. divide ad conquer 방법론은 PGM 문제를 다루는데 제한된 것이 아니기에 앞으로 문제를 해결할때 더 효율적으로 생각하도록 노력해야겠다는 생각을 했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160728162554]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[435]]></uid>
		<content_uid><![CDATA[160]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[전체적으로 지난 세미나 시간때 들었던 여러 내용들이 이번 세미나를 통해 구체화되어 이해가 더 깊어졌다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160728163632]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[436]]></uid>
		<content_uid><![CDATA[154]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[이해하기 쉬운 예시와 텍스트 분석에서의 예시를 통해 들어서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160802014757]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[437]]></uid>
		<content_uid><![CDATA[160]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[알고리즘의 원리를 쉽게 풀어서 설명하려고 하여서 좋았고 발표도 중간에 목이 잠길정도로 크게 발표해 주어서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160802014846]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[438]]></uid>
		<content_uid><![CDATA[162]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[4]]></user_uid>
		<user_display><![CDATA[관리자]]></user_display>
		<content><![CDATA[김해동: MCMC는 이전 딥러닝 세미나에서도 자주 언급 되었으나 제대로 학습할 기회가 없었는데 이번 세미나를 통해 이제까지 궁금해 하고는 있었지만 찾아보지는 못했던  MCMC를 학습한 좋은 기회 였다. MCMC의 컨셉은 알기 쉽게 잘 설명 해 주었으나 실제로 작동하는 과정을 작은 예제를 풀어 가면서 보여 줬으면 이해에 더욱 도움이 되었을 것이라 생각 한다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160806114027]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[439]]></uid>
		<content_uid><![CDATA[160]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[쉽게 풀어 설명해 주었고 가능한 한 모두 예시를 들어 주어 이해하는데에 큰 도움이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160807204522]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[440]]></uid>
		<content_uid><![CDATA[154]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[좋은 예시가 인상적이었던 발표였습니다. 굳이 아쉬웠던 점을 들자면  예시에서 관련 식에 대해 더 자세히 설명했다면 더 좋을 거라고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160807205216]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[441]]></uid>
		<content_uid><![CDATA[162]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[초반의 sampling과 MCMC, Gibbs sampling에 대한 내용을 다시 상기시킬 수 있었습니다. 발표자가 발표 내용에 대해 잘 숙지하고 발표하여서 이해하는데 도움이 많이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160810093856]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[442]]></uid>
		<content_uid><![CDATA[162]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[Sampling의 기초를 다시 볼 수 있는 기회가 되었고, 처음 접하는 MCMC, Gibbs sampling 등의 방법들도 알 수 있었습니다. 하지만 아직 와닿지 않는 부분이 있어 개인적인 공부가 필요할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160810110801]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[443]]></uid>
		<content_uid><![CDATA[162]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[기존 샘플링 기법을 지루하지 않게 적절한 예시를 들어가서 발표한것이 인상적이었습니다.
그리고 실제 프로그램을 통한 시각화를 통한 예시를 들었으면 더 좋았을것 같다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160810121754]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[444]]></uid>
		<content_uid><![CDATA[162]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[예시와 이해하기 쉬운 설명이 인상적이었고 sampling, MCMC 등 다시 공부할 수 있었던 시간이었습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160810135346]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[445]]></uid>
		<content_uid><![CDATA[166]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[BiasVariance Decompostion을 직관적으로도, 그리고 수식으로도 차근차근 풀어내려가 더 깊은 이해를 할 수 있었다. 또한 이 관점에서 바라본 Overfit과 Underfit을 알 수 있었으며, 실제 예를 다루어 쉽게 이해할 수 있었다.
Utility function의 개념을 잡을 수 있었고, 항상 PGM을 공부하면서 확신을 가질 수 없었지만, 교수님의 말씀으로 PGM이 지향하는 방향이 어떤 것인지 알게 되었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160811142535]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[446]]></uid>
		<content_uid><![CDATA[166]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Bias variance decomposition의 증명과정을 친절히 상세하게 발표자료에 실어 주었지만 발표때 구두설명은 조금 빠르지 않았나 생각한다. 효용함수에 대해 개인적인 생각을 첨언 하자면, 누구나 합의할 수 있는 객관적인 효용함수를 만드는건 불가능하고 생각한다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160814140339]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[447]]></uid>
		<content_uid><![CDATA[166]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[많은 수식의 풀이과정들을 쉽게 이해할 수 있었던 세미나였습니다. 지금까지 PGM을 공부하면서 뭔가 많은 이론들이 정립되어 있지만 어디에 사용하는지 잘 와닿지도 않고 난해하다고 생각했는데 이번 세미나를 계기로 오히려 내가 원하는대로 유연하게 사용할 수 있는 편리한 학문이란 걸 깨달았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160814163443]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[448]]></uid>
		<content_uid><![CDATA[166]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[학문을 함에 있어서 누구나 타당하다고 여길 객관성을 유지하는 것이 중요하다는 생각을 가지고 있었는데 내가 원하는 결과에 적합한 parameter를 선택한다는 것이 매우 흥미로우면서도 의구심이 남아있다. utility에 대한 개념을 접하고 이해하는데 많은 도움이 되었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160815102531]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[449]]></uid>
		<content_uid><![CDATA[166]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[현실 세계에서 비용과 이득을 따져 어떤 분류기를 고안할 때, 해당 사항을 목적함수에 반영하는 아이디어가 효용함수의 개념을 도입하면 가능할 것 같다. 하지만 누구나 보편적으로 인정하는 효용 함수가 존재하지 않는다는 점이 이슈가 될만한 문제라고 생각한다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160815110643]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[450]]></uid>
		<content_uid><![CDATA[166]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[모형 학습시 과적합 문제는 일반화와 직결되기 때문에 매우 민감한 문제인데, 이를 피하기 위한 방법론을 정리해서 열거해줌으로써 학습에 꼭 필요한 파트를 익힌 느낌이었습니다. 또한 효용함수라는 것이, 우리가 주로 사용하는 목적함수와 유사하고, 이를 최적화 하는 문제와 유사하다는 것을 대략적인 느낌으로만 이해하고 있었는데, 교수님께서 정리해주셔서 좋은 시간이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160818110938]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[451]]></uid>
		<content_uid><![CDATA[173]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[라이클리후드의 개념을 로컬 라이클리후드로 접하게 되니 반가움 반 신기함 반을 느끼며 정말 유용한 방법론이구나 생각했습니다. 또한 prior를 고려해서 값을 추정한 부분도 매우 인상깊었습니다. 부분적분에 대해 4페이지나 풀어올 정도로 노력을 많이한 것이 느껴졌습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160901162623]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[452]]></uid>
		<content_uid><![CDATA[173]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[기본 적분과정에서 하나하나 상세하게 하여 이해를 도와준점으로 보았을때 청자의 소통을 위하여 준비 한 흔적이 많다고 느꼈다. 결국 최종 식 자체는 간단하여 보고 이해하고 쓰면 끝나는 부분이지만 유도 과정을 이해하고 그 과정에서 나온 공식을 이해하는 점이 좋았다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160902123044]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[454]]></uid>
		<content_uid><![CDATA[173]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이해하기 쉬운 세미나였습니다. 특히 적분 과정에 부분적분이 끼어있어서 설명하기 쉽지 안았을 것같은데 이해가 쏙쏙 잘되었습니다. 듣기 좋은 발표이기도 하지만 가끔은 미사여구가 많아서 "자 이제 본격적으로~" 라는 말이 너무 많아 집중의 정도를 조절하기 어렵기도합니다./]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160905154023]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[455]]></uid>
		<content_uid><![CDATA[173]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[청자를 배려를 많이한 세미나였다고 생각한다. 베이즈 통계학의 기저를 볼 수 있어서 좋았다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160905191203]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[456]]></uid>
		<content_uid><![CDATA[178]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[일반적인 머신러닝 방법론에서 변수 선택/추출시 사용되는 흐름과 비슷한 느낌을 많이 받았습니다. 특히 마지막에 어떤 조건이 주어진 경우와 주어지지 않은 경우에서의 score를 비교하여 네트워크를 찾아가는 것이 많이 닮았다고 생각했습니다. 또한 여러 scoring 방법들에 대한 소개를 들었는데 수식 설명과 함께 들어서 배경이 더 잘 이해되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160907235916]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[457]]></uid>
		<content_uid><![CDATA[178]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Optimization of directed network structure을 한다고 가정하였을때 Score에 대한 합당한 지표가 중요하겠다고 생각하였습니다. 그리고 발표를 들으면서 Edge의 Complexity를 고려해야 하는 문제라면 Node간의 관계를 GA로도 풀수 있겠다고 생각하였습니다. 재미있는 발표 였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160911153733]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[458]]></uid>
		<content_uid><![CDATA[178]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[구조를 알지 못할때도 기계학습에서 일반적으로 사용되는 likelihood를 정의하여 구조를 추론한다는 점이 인상 깊었다. 몇 개의 핵심적인 개념으로 다양한 문제를 푼다는 것이 매력적이다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160918023013]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[459]]></uid>
		<content_uid><![CDATA[185]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[지난 학기에 토픽 모델링을 공부할때 배웠던 EM 알고리듬을 PGM의 시각에서 다시 공부함 으로써 새로운 식견을 엿볼 수 있었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160918023532]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[460]]></uid>
		<content_uid><![CDATA[185]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[고질적인 문제인 local maxima를 해결하기 위한 EM 방법이 인상깊었습니다. 강의에서 partially observed data와 unknown structure일 때의 문제 해결방법에  대해 다루지 않아 아쉽습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160918162922]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[461]]></uid>
		<content_uid><![CDATA[185]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[재미 있었던점은 EM의 Optimal function에 대한 reward는 하나의 scalar값인데 같은 scalar값이라도 parameter의 set의 변동은 클 수 있다는 것이다 상식적으로 당연한 결론이지만 이를 시각화로 표현하였을때 많은 생각이 들었다. 어느정도의 threshold를 가지고 나올수 있는 다양한 parameter set들의 결과를 해석하면 재미있는 결론이 나올수 있다고 생각이 들었으며 이에 대한 n차원의 입력변수와 reward scalar값을 기억해 놓는다면 비슷한 분포의 데이터에서 여러번 학습을 할때 optimal에 가까워 질수 있는 하나의 근거를 만들수 있으며 해석하여 보면 재미있겠다는 생각이 들었다. 그리고 본 세미나의 마지막과정까지 왔을때 실제적으로 모든 parameter가 정해지지 않은 상태에서 inference를 하는지에 대한 최신 트렌들에 대한 방법론이 다뤄지지 않아서 아쉬웠다. 연구들을 보면 여러가지 Inference 방법론에 대하여 Optimal function에 기반한 최적 방법론 Network를 구성하여 놓고 거기에 대하여 사전 지식과 대조해 보는 식의 연구를 보게 되는데 이를 직접적으로 접할 기회가 있으면 재미있을것 같다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160924160457]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[462]]></uid>
		<content_uid><![CDATA[185]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[EM의 다른 관점을 살펴본 듯 한 것이 흥미로웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160927210820]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[463]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[앞으로 세미나의 주제가 될 강화학습의 개요를 이해할 수 있는 기회였습니다.
강화학습에서의 주체인 agent를 이해하고 어떤 문제에 강화학습을 적용해야할 지 알 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20160929164912]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[464]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[앞으로 배우게 될 강화학습의 핵심개념의 틀을 잡을 수 있는 세미나 였습니다. 아직 강화학습이 지도학습 및 비지도학습과 다른점이 명확히 이해되지 않아 추가적인 학습이 필요하단걸 발표를 듣다 알게되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161002144859]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[465]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[앞으로의 강화학습에서의 가장 기본적인 개념에 대해서 다루었고 다음 시간부터 차근차근 하여
마지막에는 간단한 학습에 의한 결과를 내보았으면 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161002231843]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[466]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[agent의 구성요소를 설명할 때, 발표자가 약간 헷갈리게 설명한 부분이 있었으나 슬라이드에 기재된 수식으로 어떤 점에서 차이가 있는 지 확인이 가능하여 큰 부분에서 문제점은 없었던 발표였습니다. 미래에 최종적으로 얻게될 reward를 고려한다는 점에서 대부분의 기계학습 알고리즘이 greedy search 방식을 활용한다는 것과 큰 차이점이 있다는 것을 알았고 해당 방법론을 구현함에 있어 최종적인 상태까지 가려면 computation complexity에 문제가 있을 것이라 여겨져 어느 정도의 수준까지 reward를 누적시키는 가에 대한 점이 하나의 이슈일 거라고 생각됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161005195611]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[467]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[강의의 Introduction 부분으로 무거운 주제는 아닌 강의라 전반적으로 원활하게 진행되었지만,
아직도 발표를 하면서  습관적으로 "~어~" 하는 표현을 자주 사용하였습니다.
내용적으로는 강화학습과 관련된 강의 외적인 미디어를 활용하였다면  더 좋은 세미나 시간이 되지 안았을까하는 아쉬움이 남습니다.  저 포함 다음 세미나 시간에는 최대한 다양한 재밌는 예시나 미디어를 활용하여 청중으로 하여금 관심도를 높일수 있는 세미나시간이 되도록 노력하겠습니다.
이상입니다. 수고많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161005201305]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[468]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[강화학습이라는 주제는 이전부터 익히 들어왔던 주제였는데, 지금이라도 같이 공부할 수 있어서 먼저 매우 기대됩니다. 본 세미나 및 강의를 들으며 해당 분야가 지금까지 공부했던 분야들과는 조금 이질적인 느낌을 받았는데, 시퀀스를 다룬다는 것이 MC와 비슷하다는 느낌을 받았습니다. value functions에 대해서는 어느정도 이해했다고 생각하는데, 앞으로 강화학습을 익히는데 policy와 model에 대해서는 조금 더 깊게 고민해봐야한다고 생각하고 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161005203256]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[469]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[예전에 회사 내부세미나에서 Pacman 게임에 대해 석사 논문을 쓰고 졸업한 친구의 발표를 듣고 Reinforcement Learning에 대해 처음 알게 되었는데, 이제라도 학습할 수 있는 기회가 되어 기대가 됩니다. 첫 장에 대한 내용이라 앞으로 자주 사용될 개념(State, Environment, Action, Value, Reward, Policy 등)에 대해 주로 다루었습니다. 파블로프의 개 실험에 대한 질문에서 Agent 가 주체인지 아닌지 여부에 따라 강화학습인지 아닌지 결정된다는 점에 대해 배운 것 같습니다. 그 외 발표 자료를 구성할 때 청중들이 관심을 갖게 할만한 예제를 찾아야 하고 완벽히 이해해야 다른 사람에게 설명할 수 있다는 점에 대해 다시 생각하게 되었습니다. 발표 준비하시느라 정말 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161006003027]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[470]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[강화학습 세미나를 시작함에 있어서 핵심이 되는 개념들을 짚어가는 시간이었습니다. 교수님께서 파블로프의 개 사례를 말씀하시며 강화학습의 주체가 reward를 주는 자(environment 혹은 다른 agent 등)일 지 action을 하는 자일지 고민해 보면서 각 용어에 대한 정확한 정의의 중요성을 느꼈습니다. 향후 더욱 깊은 내용을 학습하여 강화학습을 바탕으로 한 논문을 작성해 보고 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161006021809]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[471]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[강화학습 주요 개념을 학습할 수 있는 뜻깊은 시간이었습니다. agent는 environment로부터 얻은 정보로 action을 취하고, environment는 agent에게 state와 reward를 줌으로써 상호작용을 합니다. agent는 현재와 미래 reward를 최대화하는 action을 취합니다. 이런 개념들은 제가 발표를 맡은 2강 Markov Decision Processes에서도 뼈대를 이룹니다. 다만 reward가 environment에 내재돼 있는 정보인지, 아니면 사용자가 설정할 수 있는 parameter인지 조금 헷갈리는데 앞으로 공부를 하면서 풀어 나가겠습니다. 전반적으로 깔끔한 발표였습니다. 발표 준비하느라 고생 많으셨습니다. 감사합니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161006113204]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[472]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[강화 학습의 첫번째 발표로서 강의와 세미나를 듣고 강화학습을 이해하는데 토대가 되는 내용들을 배웠습니다.  Agent는 observation을 통해 자신의 state를 알게되고 그 state에 맞는 action을 취합니다. action을 통해 environment에 영향을 주게 되고 environment는 agent에게 현재의 reward와 다음 state를 알려주게 됩니다. 교수님의 설명을 통해서 파블로프의 개 실험에서는 action의 주체가 agent가 아니고 외부의 action에 의해 주어진 것이기 때문에 강화학습이 아니라는 것을 알게 되었습니다.  세미나 시간에는 Bellman equation에 대해 자세히 설명하지 않아서 이해가 가지 않았었는데 지금 듣고 있는 수업인 확률동적계획법이나 인공지능이론에서도 나오는 내용이어서 알게 되었습니다.  아래의 링크들은 강화학습이 적용 및 관련 사례들인데 참고하시면 좋을 것 같습니다.
1. https://www.youtube.com/watch?v=EqXL7xC-4Y4&amp;feature=youtu.be
2. https://www.youtube.com/watch?v=iqXKQf2BOSE&amp;feature=youtu.be
3. https://www.youtube.com/watch?v=Yr_nRnqeDp0&amp;feature=youtu.be
4. https://www.youtube.com/watch?v=c-qePE1GCQY
5. https://www.youtube.com/watch?v=-L-WgKMFuhE&amp;feature=youtu.be]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161006151416]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[473]]></uid>
		<content_uid><![CDATA[194]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[강의에서 기계학습을 지도 학습, 비지도 학습, 강화 학습으로 나누었는데 강화 학습을 넓게는 시계열로도 볼 수 있다는 생각이 들었습니다. MDP처럼 interaction이 있는 시계열을 학습하는 기계 학습으로 이해하였습니다. 이번 강의에서는 전반적인 강화 학습의 정의 및 구성요소에 대해 다뤄 앞으로 진행될 세미나의 큰 구도를 잡을 수 있어 좋았고 다음 세미나가 벌써 기대됩니다. introduction이었기 때문에 앞으로의 세미나에서는 구성요소에 따른 강화 학습에 대해 더 자세히 다룰 것으로 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161007012320]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[474]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[오늘은 MDP에 대하여 다루었습니다.
오늘 배운것은 뒤에 action이 있는것을 설명하기 위해 action없이 t시점 이후의 value를 expectation을 하여 최적의 policy를 찾는것과 policy를 정하기 위해 action을 할수 있다는 가정하에 optimal policy를 찾는 과정을 backup과정을 통하여 차근차근 Silver교수님의 강의 예제를 통하여 학습, 발표 해주셨습니다. 
그리고 본 세미나에서의 핵심은 Value function이 아닌 Action-value function을 가용할수 있기 때문에 Q-value, q-learning 즉, 그에 따라서 학습을 찾아가면서 할 수 있다는점이었습니다.결국 우리는 MDP의 모델을 몰라도 강화학습을 할 수 있다는것이기 때문에 앞으로가 기대됩니다. 오늘 세미나에 대하여서 차근차근 설명하여 좋았고 앞으로의 세미나도 하나하나 짚고 넘어가서 서로 소통하는 시간이 되었으면 합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161010193609]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[475]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[첫 세미나 발표임에도 불구하고 차분한 분위기의 좋은 세미나 시간이었다고 생각합니다. 특히 예시 문제를 통해서 디테일하게 잘 이끌어 갔던거 같습니다. 특히 세미나 청중간의 토의형식의 분위기가 앞으로 세미나시간에서도 계속 되었으면 하는 바램입니다. 추가적으로 좀더 개선했으면 하는 점은 너무나 차분한 분위기에 진행되어 조금은 분위기가 루즈해 지는 경향이 이어, 발표시 어조의 변화등을 통해 발표의 강약을 조절하신다면 청중이 더욱 경청할수 있는 발표가 되어질것 같습니다.
이상입니다. 수고많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161010193802]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[476]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[금일 세미나를 통해 RL에 관해 좀 더 친숙해 질 수 있었습니다. 사실 용어가 너무 많고, 비슷해서 체계적인 정리가 필요하겠다 생각했고, iteration을 통해 최적해를 구할 때 어떻게 구해지는 것인지 궁금한 점이 있었는데, 본 세미나를 통해서 많은 궁금증을 해결할 수 있었습니다. 그 중 값이 어떻게 나오는지 설명한 부분과 트리형태의 그래프에서 노드로 state, action를 표현, 엣지로 확률값을 표현한 것을 잘 설명해준 것이 앞으로 남은 RL 세미나를 준비하면서도 많은 도움이 되리라 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161010200015]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[477]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[state value와 action value가 가진 reward의 개념을 확실히 할 수 있어서 향후 강화학습을 이해하는 데 큰 도움이 될 것 같습니다. 벨만방정식을 어떻게 최적화시켜나갈 수 있을지에 대한 다음 세미나가 기대되는 발표였습니다. 또한 지금은 action, state, transition probability 등 각각에 대한 값이 정의되어 있지만 실제로는 이런 값들은 우리가 가지지 못한 경우가 많을 것이라 여겨집니다. 그런 상황에서 강화학습이 어떻게 학습할 수 있을지 궁금해졌습니다.
세미나에 대해 말씀드리자면 지난 세미나 때 다루었던 개념을 복습하고 새로운 개념에 대해서는 하나하나 상세하게 설명하여 이해하기 편했습니다. 발표자와 청중들간의 토의형식이 이루어지는 세미나가 앞으로도 지속되었으면 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161010200750]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[478]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이전 시간의 내용을 요약하며 다시 한 번 강화학습의 핵심개념을 되짚어 보는 것은 좋았으나 복습이 너무 길지 않았나 생각합니다. 예시를 통해 한 번에 설명을 했으며 더욱 간결하고 임팩트가 있지 않았을까 하는 생각이 듭니다. 본격적으로 MDS의 내용을 설명할때는 예시를 더욱 구체적으로 만들어 개념을 명확하게 해주었습니다. 특히 State-value function과 Action-value function으로 보상을 직접 계산하는 과정이 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161010200808]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[479]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[금일 세미나를 통해 state-value function과 action-value function의 개념에 대해 확실하게 이해할 수 있었습니다. Model 전체에 대해 알고 있다는 가정하에 MDP를 푸는 것은 Backup 과정으로 이루어 지는 데, 여기서 필요한 개념 하나 하나를 차근차근 계산해서 따라 갔던 것이 많은 도움이 되었던 것 같습니다. 다음 주에 발표할 Dynamic Programming도 오늘 배운 Bellman Equation을 반복해서 푸는 것이기 때문에, 예제를 하나하나 따라 가보는 형태로 준비해 보겠습니다. 발표 준비하시느라 고생하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161010221144]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[480]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[MDP와 상태/행동가치함수, 벨만 방적식의 최적화에 대해서 다뤘습니다. 예시를 이용해서 설명해주셔서 복잡할 수 있었던 유기적인 개념도 쉽게 이해할 수 있었고 추후 다룰 강화학습을 이해하는 데에도 좋은 기틀이 될 것 같습니다. 
지난 세미나에서 '강화학습에서는 강화학습문제를 정의하는 것이 중요하다'는 말이 있었는데 동전던지기 예시와 짧은 토론을 통해 강화학습 문제에 대해 고민해볼 수 있었습니다. 발표 자료에 각 개념이 한두줄로 잘 요약되어 있어서 세미나 때 공부한 내용을 정리하고 이를 다시 상기하는 데에 도움이 되었고 발표에 있어서도 배울 점이 많았던 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161010234228]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[481]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 세미나의 학습방식을 youtube 강의수강 후 연구실 세미나 청취, 원서를 읽으며 복습을 하는 방식으로 잡고있는데 오늘 이기창 학우의 발표는 복습하는 데 많은 도움이 될 것 같습니다. 다만 개인적으로 미래보상의 합이라는 개념에서 전체 미래를 고려하는 것이아닌 몇 스텝 뒤의 미래만을 고려한다는 점을 헷갈렸는데 이를 발표자가 잘 풀어 설명해주어 이해할 수 있었습니다.. 해당 방식에서 현재 가장 궁금한 점은 감쇄계수의 역할인데 감쇄 계수의 역할이 실제로 사람이 지정하는 hyperparameter인지의 여부입니다. 개인적으로 생각하는 바는 감쇄계수의 값에 따라 좀 더 먼 미래의 보상에 가중치를 둘 것이냐 가까운 미래에 가중치를 둘 것이냐를 결정하는 parameter로 사용될 것 같은 데, 실제로 강화학습에 있어서 감쇄계수의 역할이 agent의 action에 크게 영향을 끼치는 지 궁금증을 유발한 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161011111028]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[482]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[강화학습에서 중요한 파트인 마르코프 결정 과정을 다루었습니다. 강의자료의 예시 이외에도 발표자가 스스로 새로운 예시를 들려 했던 것으로 보아 본 발표를 위해 얼마나 많은 준비를 했는 지 알 수 있었습니다.
지난 시간에 다루었던 강화학습의 기초 용어들을 복습하는 기회가 되었으며, 특히 Discount Factor에 대한 설명이 인상적이였습니다. 제게는 상태 가치 함수와 행동 가치 함수의 용어나 구조에 혼란이 있었지만, 이번 세미나를 통해 명쾌하게 이해할 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161011190317]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[483]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[발표자분께서 세미나를 미리 준비하는 모습을 보면서 배워야 될점이 많다고 느꼈습니다. 마코프 프로세스 과정에서 상태과정함수, bellman equation, policy 함수 등이 마코프 네트워크 모델에서 어떠한 방식으로 계산이 되었는지 차근차근 설명해주어서 이해하기 쉬웠습니다.  계산 과정들에 대해서도 일일히 프로그래밍 하신 결과를 보여주셨는데 배운 내용을 직접 경험해보는것이 실력을 키우는데 많은 도움이 될것이라 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161011211557]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[484]]></uid>
		<content_uid><![CDATA[197]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[발표자가 예시 하나하나 구체적으로 설명해주어 이해하기 쉬운 세미나 강의였습니다. 특히, 마코프 행렬 계산 과정을 일일히 R로 구현하고자 노력한 부분을 높이 평가합니다. 한가지 아쉬웠던 부분은, 수식이나 개념의 정의가 처음 등장했을 때 좀더 명확하게 설명하고 추가로 예시 등의 보충 설명을 해주면 더 좋은 발표가 될 것 같습니다. 하지만 첫 발표였기 때문에 앞으로의 세미나를 통해 더 나아질 것이라 기대합니다. 수고 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161012105607]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[485]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[지난 세미나 때 다루었던 bellman equation을 통해 어떻게 최적 policy를 찾아갈 수 있는지 개념을 잡아갈 수 있는 세미나였습니다. DP에 대한 지식이 많이 필요함을 느꼈고, 강화학습이 어떻게 다른 머신러닝 알고리즘과 비교하여 차이점이 있는지에 대해서도 생각해 볼 수 있었습니다. 오늘 예제는 규모가 작아서 DP를 통해 최적결과를 얻을 수 있었지만, 큰 규모의 문제에서 증가하는 계산복잡도를 어떻게 처리할 수 있을 지 다음 세미나가 기대됩니다. 발표자께서도 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161017163218]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[486]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[지난주 세미나에서 개념을 잡았다면 이번 세미나에서는 실제 RL이 진행되는 과정과 그 목적에 대해 좀더 명확히 한 시간이었습니다. 특히 정책(Policy)에 관해 최적의 정책을 찾는 방법론과 찾은 그것이 최적임을 증명하는 부분에 대해서 알아보았는데, 예시 사이트로 시각적으로 보니 신기함 반 이해 반이었습니다. update를 하는 방식에서 가장 쉽게 생각할 수 있는 방법이 synchronous 방식인데, asynchronous 방식으로 했을 때 아까 드는 생각으로는 완벽한 해답이 얻어질까 였는데, 전체를 고려할 수 없기에 sample을 쓰는 여러 방법론들을 생각해보면 이 또한 주어진 상황에서 효율성을 극대화해서 최선을 다하는 것이라는 생각에 어느정도 납득이 됐습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161017173332]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[487]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[우선 금일 세미나는  DP planning과 관련 개념에 대하여 다루었고 매끄러운 발표와 적절한 예시로 진행되어 세미나 전에 DP planning의 전반적인 과정이 정리되지 않았는데 이점이 좀 해소되었습니다. 그리고 세미나 중간중간의 질문을 다시 고민하면서 개념을 좀더 다질 수 있는 유익한 시간이었습니다. 
세미나를 마치고 고민해본 것을 몇줄 남기자면, policy는 state에 따른action의 확률이기 때문에 실제 DP의 control 과정에서는 각 state의 optimal action을 찾고 전체 optimal action을 통해 optimal policy를 찾는 것이 아닐까 생각합니다. 또한 과제가 되었지만 개인적으로 em 알고리즘은 likelihood 확률을 최대화하는 것이고 RL의 DP에서는 value 함수를 최대화하는 것으로 학률과 기댓값의 차이도 있다고 생각합니다. 앞으로도 이런 질문과 토론으로 유익한 기회가 많아지면 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161017231607]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[488]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[매 강의마다 생소한 용어들이 등장하고 있습니다. 개인적으로 공부할 때에는 이 단어들이 머릿속에서 겉돈다는 느낌이였습니다. 지난시간에 이어 이번 시간도 여러가지 예시와 함께 새로운 용어들을 직관적으로 이해할 수 있도록 설명하였습니다.
Backup이나 Synchronous를 쉽게 이해할 수 있었으며, Policy와 Value iteration를 Javascript를 이용해 웹에 구현한 예시가 인상적이였습니다. 그리고 가장 궁금했던 것은 앞 optimization 방법들의 Convergence였는데, 이에 대해서는 자세하지 않아 아쉬웠지만 갈피를 잡을 수 있게 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018005718]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[489]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[금일 세미나에 느낀 것은 reinforcement learning도 결국은 기존에 알고있던 machine learning과 매우흡사하다는 점입니다. policy iteration 같은 경우도 결국 EM algorithm과 매우 흡사한 방법으로 진행되고 value를 update하는 부분에서 synchronous는 일반 gradient descent, asynchronous는 online learning으로서 stochastic gradient descent와 비슷하게 생각할 수 있을 것 같습니다. 여러모로 통합하는 관점으로 생각해볼 수 있을만한 여러 소재를 얻은 것 같아 좋은 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018115742]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[490]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[금일은  reinforcement learning 중에서도  동적계획법(Dynamic programming)을 중점으로 다루는 시간이었습니다. 세미나 시간에 잠시 이슈였던  DP와 Divide &amp; Conquer 의 가장 큰 차이점은 제가 생각하기에 각 subgroup들이 독립이냐 그렇지 않은가에 차이가 있는거 같습니다. DP의 경우에는 각 sub probelm이 dependent한 nested problem 인 반면에, Divide &amp; Conquer의 경우에는 각 sub probelm이 independent하게 나누어 가장 효율적으로 그 solution을 결합하여 문제를 풀어나가는 장점이 있습니다. 
 세미나에서 현재 우리에게 공감이 될 수 있는 예제 활용 역시 매우 좋았습니다. 이상입니다. 첫 세미나 준비하시느라 수고 많으셨습니다. 
감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018145047]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[491]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[지금 듣고있는 수업인 확률동적계획법이 강화학습과 어떻게 연결되는지 어떻게 연결되는지 궁금했는데 이번 세미나를 통해서 좀더 알게 되었습니다.  동적계획법의 필요조건, 예제들을 통해서 동적계획법의 정의에 대해서 다시 생각해보게 되었습니다.
Contraction Mapping 부분이 수학적인 부분이 많아서 이해하기 어려웠지만 좋은 연구를 하기 위해서 필요한 사항이라고 느끼게 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018145513]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[492]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 세미나는 발표자가 새로운 개념과 관련 개념을 예시를 통해 쉽게 설명을 해주었던 체계적인 세미나였습니다. 강의에서 나오는 예시 뿐만아니라 발표자 나름대로 생각해온 예시 덕분에 쉽게 이해할 수 있던 세미나였습니다. 특히 스탠포드에서 제시한 dynamic programming demo는 인상적이었습니다. 새로운 개념을 점차 배우면서 이전에 배운 내용과 연결시키는 것의 중요성을 깨달았습니다. 개별적인 개념들을 이해하고있다고 하더라도 새로운 영역과 관련시켜 생각해볼 때 과연 내가 알고있는 그 개념이 맞다고 확신할 수 있는지 돌아보는 계기가 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018150829]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[493]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[세미나 도중에 토론도 나누었지만 아직 Principle of Optimal에 대한 의문이 가시지가 않는다. DP가 Divide and Conquer의 변형 정도로만 이해되기에는 무리가 있지 않나 생각한다. 강의자인 David Silver 교수가 오해가 있는 게 아닌가 추측된다. 그나저나 세미나 도중 발표자가 보여준 Andrej Kapathy의 데모 자료가 참 인상적이었습니다. 역시나 아쉬운 점은 최적해로의 수렴 증명과정이 빈약하다는 점 이었습니다. 강의에서도 대충 넘어가서 발표자도 준비에 어려움이 있었겠지만, 그래도 아쉬움이 남습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018155212]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[494]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[창엽 형님의 첫 세미나 발표였습니다. 발표자료에서 하나하나씩 짚고 넘어가고 
karpathy라고 쓰고 '갓파시'라고 읽는분의 예제와 여러가지의 예시를 들어주셔서 이해가 잘 되었습니다. 강조하여주신 Divide and Conquer로 다룰수 있는 문제와 아닌 문제가 있는데 이 부분에서는 앞으로 실 문제에서 잘 생각해보아야 할 문제같습니다. 실제로 저희 PC에서는 Divide and Conquer가 되는 문제이면 R과 Python에서 실제로 다룰때 이득을 볼수 있게 코드를 짤수 있기 때문입니다. 빨리 조금더 공부하여 실제로 문제를 풀어보고 싶은 마음이 드는 세미나 였습니다. 특히 Dynamic Programming을 하여도 실제 문제를 받았을때 가장 중요한 부분은 State에 들어갈 변수를 어디까지 받을수 있고 어떤 변수를 넣을것인가? 그리고 t를 어떻게 잡을것인가? 외생을 얼마나 잘 케치할수 있을것인가? 등등이 있을것인데 빨리 행해보고 싶다는 마음이 드는 세미나였습니다.
앞으로 조금씩 공부해가면서 세미나시간에도 서로 실제상황에 대하여 토의가 되었으면 좋겠습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018203701]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[495]]></uid>
		<content_uid><![CDATA[204]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[강화학습의 주요 개념들을 다시 한번 되짚어보는 유익한 시간이었습니다. 제가 지난 시간에 발표를 준비할 때만 해도 작은 규모 예시여서 최적 정책과 최적 가치를 행렬 연산으로 단번에 구할 수가 있었는데 큰 규모 문제에선 이를 어떻게 구할까라는 의문이 떠나지 않았었습니다. 이번 창엽 형님 발표를 통해서 그 첫단추를 뗀 느낌입니다. 정책을 기준으로 최적화하는 작업과 가치를 기준으로 최적화하는 작업이 결국 어느 지점에선 맞닿아 있다는 레슨이 이번 시간 중요 개념이라는 생각이 듭니다. 특히 MDP의 해가 수렴함을 증명하는 내용까지 준비해오셔서 발표자의 노력과 성의를 엿볼 수 있어서 좋았습니다. 전반적으로 깔끔한 강의였습니다. 준비하느라 고생많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161018224335]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[496]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[본 세미나에서 sample이라는 용어에 관한 토의가 있었는 데, 해당 용어에 대해서 여러가지 의견이 분분했습니다. 개인적으로는 내가 이미 가지고 있는 episode 데이터로부터 status와 reward를 가지고 있을 때, sample에 대한 bootstrapping을 진행할 시 각 경우에대해서 동등하게 확률을 주어 sampling을 한다는 식으로 이해를 하였는데 이는 model-free learning이라는 용어와는 일맥상통하지는 않은 것 같습니다. 개인적으로 예제를 통해 쉽게 이해할 수 있었고 TD(0)의 경우 다음 status에서 여러가지 status와 reward를 bootstraping한 후 transition matrix를 재구성하여 학습을 진행하는 방식으로 이해했는데 이를 원서를 통해 확인해보아야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108100042]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[497]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[지난번 제가 발표준비를 하면서 MDP의 Environment를 어떻게 구할지 약간의 의문이 들었는데 어제 발표를 통해 그 의문점을 조금이나마 해소할 수 있는 계기가 된 것 같습니다. 현실세계의 Environment를 완벽하게 파악할 수 있는 경우는 드물 것이므로 이번 발표시간이 강화학습을 구현하는 데 실질적인 도움이 될 거라 생각합니다. 다만 내용이 어려워서 제 개인적으로 추가학습이 필요할 것 같습니다. 발표 준비하느라 고생많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108161123]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[498]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[이번 세미나의 목적은 model free라는 용어에서 보듯이 우리에게 환경과 보상에 대한 정보가 없는 경우에 강화학습을 어떻게 수행할 수 있을지에 대해 알아보는 것이었습니다. MC와 TD 방식을 통한 접근이 현실적으로 타당하다고 여겼으나 부분적으로 의구심이 남는 곳들이 있었습니다.  sample의 개념과 step별 reward를 추정하는 방식들이 아직 이해되지 않았고 구체적인 예시와 논문들을 통해 더 깊이 공부해보고 싶어졌습니다. MC와 TD에 대한 개념을 통해 더 깊은 강화학습 내용에 접근할 기반을 다진것 같아 유익한 시간이었습니다. 발표자 분의 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108161553]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[499]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 주 세미나에서는 Monte-carlo learning과 TD로 구성된 Model-free prediction에 대해 다루었습니다. 점점 세부 개념이 많아질수록 이전의 개념들과 헷갈리기 시작하는데 발표자가 잘 구조화 해준 것 같습니다. 이 방법이 실제로 많이 사용되지 않더라도 실제로 구현한 예시를 직접 보고 싶다는 생각을 들었고 세미나 마지막에 토의했던 sample의 방식이 data생성까지 포함하는지 여부에 대해 궁금해졌습니다. 따로 논문이나 자료를 찾아 보아야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108164112]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[500]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[먼저, 덕성이의 발표는 항상 재미있는 예시로 중간 머리를 환기 시킬수 있는 포인트가 있어서 좋았습니다. 먼저  Sample에 대하여 토의를 하였는데 개인적으로는 Environment를 알고 있는 상황에서의 Sample의 정의는 본 세미나에서 배우는 강화학습의 근본 기저와 맞지 않는다고 생각합니다. 물론 학습은 시킬수 있겠지만 태생적으로 분포를 가정한다는 것은  Policy를 만들때의 수렴속도의 장점이자 성능의 큰 제약일 수 있다고 생각합니다. 그리고 Bootstrap이라는 용어가 헷갈렸는데, 실제로 발표의 예시를 들었던 기본적인 632 Bootstrap이랑은 일맥상통하는 내용은 아니고 아이디어를 빌려서 왔다고 이해를 하였습니다. 그리고 여러번의 반복  learning을 할 경우의 첫번째를 러닝하고 사용하지 않는다는 점이 잘 이해가 가지 않았습니다. 왜냐하면  시간적 state측면에서 볼때의 제일 과거의 데이터를 사용한다는것인데 보통 데이터를 다루면 최근 데이터가 가장 합리적인 action을 할수 있는 경우가 많기 때문입니다.
발표 측면에서는 공모전의 '거성' 답게 조리있고 재미있게 잘해주었습니다. 해당 부분에 대하여 좀더 공부하면서 앞으로 토의도 많이하고 마지막에는 구현하는 예제를 서로 공유하면서 테크니컬한 부분도 공유하면 앞으로의 세미나가 더욱더 재미있고 좀더 상호작용하면서 진행될것 같습니다. :)]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108164954]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[501]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[지난 시간에 Full Width 백업의 계산 복잡도 때문에 Asynchronous 백업과 Sample 백업에 대해서 언급만 하고 넘어 갔는 데 세부 알고리즘에 대해 다뤄 큰 도움이 되었습니다. 또 Monte-Carlo에서 업데이트하는 방법으로 Incremental Mean 방법을 쓸 수 있다는 점과 State가 유한할 때 끝 지점에서 처음 지점까지 오는 경로에 대한 값을 업데이트 한다는 점을 배웠습니다. Temporal-Difference 방법에서는 바로 다음 스텝의 상태가치를 추정하여 업데이트한다는 점을 배웠습니다. 또한 Monte Carlo와 Temporal-Difference의 차이를 설명할 때 대학생 예제를 들었던 점이 참신했던 것 같습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108165301]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[502]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[샘플의 정의에 대해서 발표시간 이후에도 댓글을 통해 이루어진 활발한 논의가 이루어지고 있는 점이 놀랍습니다. 지금까지 이루어지고 있는 논의들은 샘플을 흔히 우리가 알고있는 데이터를 추출하는 방식으로 이해하고 접근하고 있습니다. 하지만 저는 실버가 샘플을 다른 시각에서 접근하고 있다고 생각합니다. 따라서 논의의 방향도 실버가 제시한 샘플의 정의에 입각하여 진행하여야 되지 않나 생각합니다. 실버의 강의자료에 의하면 샘플과 붓스트랩핑의 정의는,
Sample: Update samples an expectation
Bootstraping: Update includes an estimate
라고 정의하고 있습니다. 이에 따르면 붓스트랩핑은 이전의 측정값을 다시 포함하는 것을 의미하므로 TD가 붓스트랩핑의 일종으로 간주된다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108172132]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[503]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[금주 세미나는 MDP보다 현실적이라고 볼 수 있는 model-free learning을 다루어서 개인적으로 기대하고 있던 세미나였습니다. 크게 위의 세가지 개념에 대해서 잘 정리해 주어서 좋았던 세미나였습니다. 모델이 아닌 샘플을 통해 학습하면서 보다 직접적으로 환경을 학습할 수 있는 방법이라 생각하는데 샘플에 대한 정의가 모호했던 점이 아쉬웠습니다. RL에서 환경이 모수라면 샘플은 추출된 것이 아닌 agent가 환경으로부터 받아들이는 것이 라고 생각합니다. 발표 준비하시느라 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161108173705]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[504]]></uid>
		<content_uid><![CDATA[214]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Dynamic programming에서는 환경에 대한 Reward function과 state transition probabilities를 알아야 되는데 실제 환경에서는 모든 환경을 알 수 없기 때문에 실재로 경험한 정보들로부터 update를 하는 monte-carlo 학습과 Temporal-Difference learning 방식을 사용한다는 것을 알게 되었습니다. Monte-carlo 개념이 이전까지 와닿지 않았었는데 이번 세미나 발표를 통해서 이해 할 수 있었습니다. 블랙잭 게임으로 Monte-Carlo policy evaluation 하는 방법을 예제로 설명하였는데 다른 보드게임에서도 비슷한 방법으로 강화학습을 적용해보는 연구를 하고싶다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161109133306]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[505]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[오늘은 Model-free learning에 대하여 발표하였습니다. 개인적으로 공부할 시간이 많이 없었어서, 새롭게 듣는 발표였습니다. 오늘 발표의 결과를 바탕으로 나현이에게 부탁하고 싶은 것이 있습니다. 첫번째로, 수식이외의 예시가 조금만 더 있으면 좋겠다는 생각이 들었습니다. 실제로 우리가 강화학습을 배우는 이유는 수식을 이해하고 깊게 이해하는 부분도 중요하지만, 이를 어떻게 적용하고 왜 이것을 사용하는지, 어떤 부분에서는 오히려 이게 왜 좋은지에 대한 부분이 있었으면 더 좋았을것 같습니다.  두번째로는, 상당히 준비를 많이하여 판서를 막힘없이 써나가는 부분에서 경의를 표합니다. 이 부분에서 조금더 PPT에 작성을 해주었으면 하는 바램입니다. 왜냐하면 우리 세미나 실의 특성상 판서를 하게되면 잘 안보이고, 의미 전달이 힘든 경우가 있기 때문입니다.
  결론적으로,  해당 부분에 대하여 저 자신도 다시 공부하여 서로 물어보고 특정 부분에 대하여 서로의 생각을 말하는 시간이 되면 더 좋을것 같습니다. 다음주부터 교수님께서 말씀 하신 부분에서 다시 재미있는 발표 기대합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161122004158]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[506]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[Q-learning은 알파고에도 적용된 방법론으로 알려져있습니다. 그래서 더욱 흥미가 갔는데, 마지막부분에서 Q-learning을 다루지 못해서 개인적인 아쉬움이 남습니다. 느낌으로 받아들이자면 Off-policy가 KL과 유사한 것인지 아직 개념이 명확히 서지 않는데, 좀 더 강의와 교재를 들여다보고, 다음 세미나시간에 집중하여 들으면 더 이해가 쉽지 않을까 싶습니다. 
그리고 지난번 제 세미나 발표때는 sample에 대해 명확히 이해하지 못했는데, 이번 강의를 들으면서 이해한 바로는 다음과 같습니다. 어떤 초기의 랜덤한 정책이 주어지고 거기에서 episode를 뽑아내는 형식으로 이해했습니다. 여기서 정책이 주어진다는 것은 state와 action이 정의됨을 의미하는것으로 저는 이해했데, 이렇게 되면 또 state가 가질수 있는 범위인 environment를 알아야 가능한 것은 아닌지, 이해가 안되는 부분이 여전히 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161122085407]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[507]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[506]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[랜덤한 정책에서 뽑아지는 것이아니라 말그대로 trial and error 즉 실제로 굴러서? 얻어진 데이터라고 보면 될 것 같습니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20161122152106]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[508]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 궁금한 점은 과거 model-based인 dynamic programming 같은 경우 명확하게 control 파트로 정책을 update시키는 과정이 존재하였는데 model-free인 경우 해당 과정이 명확하게 존재하지않고 prediction 과정과 많이 혼재되어있어서 조금 혼란스러웠습니다. 제가 이해하고 있는 바로는 model-free prediction 과정에서 이미 state-value를 update하고 있는 데 model-free control의 경우 state-value를 action-value로 대체하여 학습하는 것과 큰 차이는 없다고 여겨지는데 그렇다면 언제 optimal action을 정하는 과정은 언제 이루어지는 지 궁금한부분입니다. 이 부분이 어느 과정에서 이루어지는 지 개인적으로 공부를 해야할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161122152610]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[509]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[들을수록 David Silver의 강의의 순서가 짜임새가 있다는 생각이 듭니다. DP에서 Transition matrix가 주어졌을 때, evaluation과 optimize를 하고, 바로 전 시간에는 Sample 기반으로 Transition 없이 evaluation을 했습니다. 그리고 이번에는 optimize입니다. On-policy와 off-policy가 헷갈렸었지만, 구분할 수 있게 되었습니다. 또한 exploring에 대한 practical한 예를 알 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161122173124]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[510]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[On-policy와 Off-policy의 개념을 정리할 수 있는 시간이었습니다. 많은 수식들을 독파해서 차근차근 설명해준 나현 씨에게 감사함을 전합니다. 다만 강의가 진행될수록 난이도가 높아져서 제가 잘 따라가지 못하는 경향이 있는데, 예시나 그림 등으로 설명한다면 더 나은 발표가 됐을 것 같습니다^^; 알파고 쇼크 때문에 강화학습이 일반인들에게도 관심있는 주제가 된 만큼, 머신러닝 전공자라면 강화학습을 주제로 일반인들과 소통할 기회가 자주 있을겁니다. 지금 공부하는 내용들을 강화학습을 전혀 모르는 일반인들에게도 명쾌하게 설명할 수 있을 때까지 열심히 공부합시다(제게 주는 다짐이기도 합니다)]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161123212311]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[511]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[505]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[코멘트 감사드립니다. 말씀하신대로 판서한 부분을 추가하여 발표자료를 업로드하였습니다. 이번 세미나는 앞의 내용과 많이 중복되고 q-value와 epsilon-greedy가 앞에서 배운 DP와의 차이라는 것을 주된 내용입니다. 그래서 예시를 통해 반복되는 내용을 다루기보다는 DP와의 차이를 설명하려 했고그러다 보니 식을 많이 사용했습니다. 말씀하신대로 예시와 수식을 모두 사용하여 공부하는 것이 가장 좋은 방법이 되겠지만, 다루는 내용에 따라 때때로 예시 또는 수식에 집중하여 공부할 부분도 있다고 생각합니다. 발표에 부족한 부분이 많아 다음 세미나에서 보완할 수 있도록 하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161124172105]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[512]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[이번 세미나에서는 Model-free control에 대해서 발표를 들었습니다. Monte-Carlo learning에서 epsilon greedy exploratio을 통해 local optimum에 빠지는 것을 방지하고, GLIE 에서 epsilon이 시간이 0으로 수렴한다면 epsilon greedy로도 greedy policy를 구할 수 있는것을 알게 되었습니다. Sarsa 부분이나 Off-policy 발표 내용에 대한 준비가 부족했던것 같은데 다음 발표때 자세히 들을 수 있으면 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161128101221]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[513]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[이번 세미나는 Model-free Control 에 대한 내용이었습니다. 학회 발표 준비로 세미나를 쉬었는 데 앞서 다뤘던 MDP와 Monte Carlo, TD에 대해 복습했던 점이 좋았습니다. MDP, TD, MC 를 배우면서 의사 결정을 위한 State를 평가하는 과정과 업데이트(Control)하는 과정으로 나눠 강의 자료가 구성된 점이 이해에 도움이 되었습니다. Monte-Carlo 모델의 컨트롤은 epsilon-Greedy 탐색으로 이루어지고, Temporal Difference 컨트롤은 SARSA를 통해 이루어진다는 점을 배웠습니다. SARSA라는 알고리즘의 작명 센스에 감명을 받았고, 이름을 통해 알고리즘이 어떤 느낌인지 전달 할 수있거나 파악하는 데에 도움이 되도록 많은 고민을 해야 겠다는 생각이 들었습니다. Q-learning에 대해 많은 관심이 있었는 데 첫시간에 다루지 못한 점은 아쉬움이 남습니다. 발표 준비하시느라 고생하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161128103958]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[514]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이번 세미나의 주제인 Model-free 학습이 작동하는 핵심 원리는 State value 함수 대신 Action value 함수를 하는 하는 것이다. 이번 발표의 모든 내용이 결국 이 원리에 기대고 있기 때문에 Action value 함수의 타당성을 증명하는데 좀 더 시간을 할애 했으면 더욱 좋았을 거란 생각이 든다. Action value 함수를 대신 사용해도 학습이 가능하다는 것은 이미 검증이 된 내용이라 다른 사람에게 해당 정리의 타당성을 입증하기 위해 세세한 증명과정이 필요하진 않을 것이다. 하지만 학습자의 입장에서 자세한 증명을 통해 배울 수 있는 것 또한 굉장히 많을거라 생각한다. 앞으로 미지의 영역을 탐구해야 할 연구자들에게 어떻게 기존의 방식을 대체할 방법을 수학적 근거를 가지고 만들어 내는지 그 과정이 더욱이나 중요할 것이다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161128120217]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[515]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[지난 시간 덕성오빠의 model free prediction에 이어 이번엔 model free control에 대해 배웠습니다. 뒷부분으로 가면 갈수록 dynamic programming의 내용이 더해져 점점 이해하는데 더 많은 시간이 필요한 것 같습니다. 발표자가 이전 시간에 배운 내용을 복습해주고, policy에 따라 나누는 등의 전체 내용을 구조화하여 보여주어 큰 그림을 이해하는데 도움은 되었지만 세부 내용을 파악하는 데는 조금 힘들었습니다. 다음시간에 명쾌한 강의 기대합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161128123153]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[516]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[on-policy와 off-policy에 대한 차이점과 action value function을 이용하여 control을 수행하는 부분을 이해하는 것이 이번 세미나의 핵심이라고 생각합니다. 또한 세부적으로 exploration을 수행하기 위한 epsilon greedy exploration에 대한 내용도 학습할 수 있었습니다. 강화학습은 reward를 최대화시키는 policy를 찾는 데 그 목적이 있기에 오늘 다룬 내용은 지금까지 세미나 중 가장 중요하지 않나 싶습니다. 이 부분을 2주 동안 준비하느라 고생한 발표자에게 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161128152207]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[517]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[지난시간에 이어 금번 세미나에서의 Model-free control부분세미나 잘 들었습니다. 지난 세미나와의 기간이 좀 지나서 review차원의 앞부분을 통해서 다시금 상기시킬수 있는 시간이여서 좋았습니다. 
  특히 n-policy와 off-policy에 대한 차이점으로부터 현재 많은 주목을 받고있는 Deep q-learning의 전신인 Q-learning 까지 알아보는 시간이었습니다. 아쉬웠던 점은 실제로 예제를통해서 각 학습법의 차이를 알아보았더라면 더 좋았을 것같습니다. 특히 알파고에 대한 관심으로 인해 Q-learning에 대해 궁금한것이 많아 이부분은 개인적으로 좀더 알아보도록 하겠습니다. 스터디 준비하느라 수고하셨고 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161129172122]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[518]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 앞선 내용을 요약정리하면서 세미나를 진행한 점이 아주 좋았습니다. 또한 value function approximation을 하는 motivation을 설명하여 세미나를 듣는데 아주 도움이 되었습니다. 개인적으로 해당 파트에서 value function approximation을 위해 supervised learning 문제로 전환하기위해서 target을 정의하는 방식이 해당 챕터에서 가장 중요한 내용이라고 생각합니다. 또한 episode를  또는 &lt;(state, action), q(state, action)&gt;으로 풀어해쳐서 episode의 sequence를 뭉개서 학습시키는 것이 굉장히 인상 깊었는 데, 지금 다시 생각해보면 episode 단위로 학습을 하게되면 특정 experience를 고려하는 샘이되어 bias가 커지게 될 수 있기 때문에 저런식으로 뭉개서 학습하여 variance를 허용하고 bias를 줄이는 것이라 나름대로 정의하였습니다. 계속 곱씹을 수록 생각할 점이 많은 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161129202329]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[519]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[도입부분에 이때까지의 결과를 정리하고 어떤 문제점이 있어서 모델들이 개발되어 있고 오늘 발표할 부분이 왜 필요하고 실제 사용하는지에 대한 여부를 논리적으로 전개하여 아주 좋았습니다.개인적으로 Atari 에서 4개의 depth로 이루어진 input variable과 24로 이루어진 GO에서의 예제를 들었는데 그이유가 무엇인지에 관해서는 개인적으로 학습해야될 부분이라고 생각합니다.  내용이 적지 않은 부분인데도 불구하고 보면서 바로바로 이야기 할 수 있다는것은 그만큼 자신이 고민해 봤다는 흔적이라고 생각합니다. 발표 잘 들었습니다. 세미나에서 언급한 것과 같이 finite한 경우를 전부 table을 만든다는것이 현실적이지 않은 경우가 대부분이라고 생각합니다.
이와 같은 경우에서 실제 데이터를 만져보고 느껴보고 싶다는 생각이 드는 발표였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161130125224]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[520]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[많은 양을 임팩트있게 준비했다고 생각이 들었습니다. 앞선 발표자들의 MC, TD가 이어져 와서 실제로 value function을 어떻게 추정하는지 배우는 시간이었는데, 매우 흥미로웠습니다. 중간에 experience D가 이해가 안되서 이후 내용에 대해 깊은 이해가 되지 않은점이 아쉽습니다. 세미나 마치고 했던 얘기를 중점으로 다음시간에 설명해 준다면 다들 이해할 수 있지 않을까 싶습니다. 그리고, 영어로 피피티를 만들었다는 점도 멋졌다고 개인적으로 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161130133137]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[521]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[먼저 지난 시간에 배운 내용들을 다시 복습하면서, 결국 State가 너무 많은 경우 각각에 대한 V(S)값을 저장할 수 없는 한계가 있음을 설명하였습니다. 이 문제를 해결하기 위해 V(S) 추정이 필요하여 w라는 새로운 파라미터가 필요하고 Stochastic Gradient Descent를 사용하여 추정하는 과정을 자세히 설명해주신 점이 도움이 되었습니다. 지도학습이 오라클이 답을 알려준다고 표현하고 학습 과정에서는 그 값을 이전에 배웠던 알고리즘인 MC에서의 G_t, TD에서 TD 타겟으로 바꾸기만하면 된다고 쉽게 설명해주신 점이 좋았습니다. 그 외 Coarse Coding등 강의에서 자세하게 설명하지 않는 개념에 대해서도 따로 자료를 만드신 점이 도움이 되었습니다. 전에 발표 준비를 할 때 영어로 만들었다가 전부 한글로 다시 바꿨었는 데, 방대한 양인데도 자연스럽게 발표하시는 것을 보면서 정말 준비를 많이하셨다는 생각이 들었습니다. 발표하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161201010219]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[522]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[지금까지 배웠던 강화학습은 state나 action이 작을때만 적용시킬 수 있습니다. 실생활의 문제는 continuous state space이기 때문에 value function을 추정하는 방법이 필요한데 오늘 세미나에서 이 문제에 대해서 다루었습니다. V(S) 추정을 하기위해 w라는 새로운 파라미터를 사용하였습니다. machine learning에서 사용하는 stochastic gradient descent를 사용하여 state가 continuous인 문제에 대해서 파라미터를 업데이트 하는 방법이 있는데, 이를 통하여 최적의 atari game 플레이 방법을 찾는것이 흥미로웠습니다. 발표자가 열심히 준비해주신 덕분에 내용을 이해하는데 많은 도움이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161205100730]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[523]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[지금까지 배운 방법론을 개괄하면서 시작한 도입부가 매우 좋았습니다. 한번에 같이 비교하면서 보니 각 방법론 간의 차이점이 더욱 부각되어 보였고 왜 그러한 시도들이 이루어져 왔는지 일목요연하게 볼 수 있었습니다. 이전까지의 방법론과 달리 이번 세미나에서 다룬 DQN은 실제로 산업계에서 가장 흔히 사용되는 방법론이었기 때문에 흥미로웠습니다. Batch 방식에서 시나리오를 batching 하는 것인지 states의 일부분을 batching 하는 것인지 토론이 주고 갔었는데, 명확한 결론이 나지 않았습니다. 이후에라도 추가적인 설명이 있었으면 더욱 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161205114649]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[524]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[tablur하게 다루었던 그동안의 세미나들을 approximation하게 추정하는 부분이 매우 흥미로웠습니다. 파라미터를 통해서 특정 value값을 찾아가는 것은 익숙한 formation이며 이를 MC, TD 등 여러 다뤄왔던 개념에 접목하여 쓸 수 있다는 점을 배웠습니다. 그리고 이를 수행함에 있어 gradient를 사용하여 접근이 어렵지 않았습니다. 또한 online과 offline의 차이가 실시간으로 업데이트가 이루어지는 것이라는 점또한 배울 수 있었습니다.
많은 것을 배움과 동시에 세세한 부분까지 신경쓰는 발표자 모습이 인상적이었습니다. 많은 참고가 되었고 이를 다음 발표 때 적용해봐야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161205142422]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[525]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[Value function approximation의 이유와 그 장점을 구체적으로 설명한부분이 매우 인상적이었습니다. 또한 Batch methods 와 Off-line update의 차이와 장단점을 비교하여 알기 쉽게 이해가 가능하였습니다. 확실히 batch단위의 update시 확실이 convergence가 확보되고 generalization이 수월한 예제가 없던부분은 조금 아쉬웠습니다. 전체적으로 세미나 발표부분에 대한 이해도가 높아보였으며 알기위한 노력들이 드러나는 좋은 세미나 시간이였다고 생각됩니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161206203755]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[526]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[처음 세미나 발표를 하는 발표자임에도 불구하고 세미나 발표를 매우 잘 이끌어 간점 먼저 감사드립니다. 금일 다룬 Policy based의 학습방법론이 실제로도 잘 수렴가능함을  로봇관련 논문을 통한 학술자료등와 같이 발표시간에 제시한점이 매우 좋았습니다. 아직  이론적으로 뒷바침 되지는 않았지만 이는 딥러닝의 RBM의 Gibbss sampling을  Contrastive Divergence가 처음에는 이론적으로 증명은 되지 않지만 추후 증명이 되어 그 무게를 얻은 것처럼 , 본 세미나에서 다룬 Policy Gradient Method의 수렴여부와 local optimal문제는 추후에 이론적 백그라운드가 제시된다면 좀더 무게를 지닐수 있을거라 생각된다. 발표준비하느라 한주동안 수고한 발표자에게 감사를 전한다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161206204843]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[527]]></uid>
		<content_uid><![CDATA[225]]></content_uid>
		<parent_uid><![CDATA[517]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[예시에 대한 코멘트가 많아 두번째 세미나에서 보완을 하고자 했는데 미흡한 부분이 있었던 것 같습니다. Model-free의 예시는 sampling만 다를 뿐 DP와 그 형식이 같고 값을 넣어 계산해도 과정이 동일합니다. 주어진 세미나 시간에 이전 세미나에서 다룬 내용을 모두 다룰 수 없어 DP와 같은 과정을 반복하는 것을 일부 생략하였습니다. 또한 model-free는 sampling을 통해 시간에 따른 state를 학습하기 때문에 random walk example과 같이 어느 정도 수렴하는데까지 많은 sample이 필요하여, 실제 값을 넣어 계산해보는 것은 이해를 돕기 위한 단순한 예시로써 부적절하다고 판단했습니다. 그리고 greedy exploration의 증명과 iteration은 DP에서, MC와 TD의 차이는 model-free prediction에서 그 예시 및 학습 방법의 차이 등이 이미 다뤄졌습니다. Q-learning은 on-policy TD에서 target policy가 greedy로 바뀐 off-policy TD이며, policy distribution의 차이만 있기 때문에 반복되는 과정은 줄이고 설명을 추가하였습니다. 세미나가 거듭되면서 이전 내용을 토대로 진행되어 내용이 갈수록 복잡해져서 더 어려운 점이 있었던 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161207074811]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[528]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[앞의 세미나를 공부하면서 전부 discrete한 문제를 다루고 있어서 continuous 문제를 어떻게 다룰 것인지에 대해 줄곧 궁금증이 있었는데 이번 세미나의 주제 value function approximation이 그 해결책이 되는 주제였기 때문에 개인적으로 매우 흥미 있었던 세미나였습니다. 처음 value function이란 개념을 접했을 때 굳이 함수라는 표현을 사용하는지가 의문이었는데 아마 parameter를 이용한 함수를 통해 결국에는 value를 approximation하기 때문에 이러한 이름이 나오지 않았나 생각합니다. 또한 가장 널리 사용되는 DQN에 대해서도 다루어 지금까지 가장 관심있게 들었던 세미나가 된 것 같습니다. ]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161207075507]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[529]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[학습을 무엇으로 할것인가? 에 대한 큰 양대산맥으로 value based 와 policy based중 후자의 방식에 대하여 세미나를 가졌습니다. policy based update의 테크니컬한 가장큰 문제점은 아마 초기값과 초기 iterlation에서의 신뢰성이라 생각합니다. 예시를 든 로봇 연구에서도 해당 문제점이 있기 때문에 발표한것과 같이 행하였다고 생각합니다. 
개인적으로 로봇이 뛰는것을 예시로 든것같이 강화학습은 직접적인 actuator가 있는 실험환경에서 발전가능성이 높다고 생각을 항상 하고 있습니다. 따라서, 실제 데이터에 대한 습득혹은 작은 actuator를 사서 방학동안에 행해볼 생각이며, 실제적으로 policy based model과 value based model의 적절한 적용 판단에 대하여 생각해봐야 할 문제라고 생각하고 직접 부딫쳐볼 생각입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161207220554]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[530]]></uid>
		<content_uid><![CDATA[234]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[내용이 갈수록 어려워지고 있는데, 이전 학습 내용들을 요약 정리해준 부분이 좋았습니다. value function을 어떻게 추정하는지 배우는 시간이었습니다. 발표자의 강의 내용 이해도가 높아서 발표를 듣는 데 수월했습니다. 다만 반복적인 내용들은 조금 컴팩트하게 다뤘다면 집중력 있는 발표가 됐을 것 같습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161208152945]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[531]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[첫 발표인데도 무척 차분하게 진행한 점에 높은 점수를 드리고 싶습니다. 관련 논문을 찾아서 소개한 점도 인상 깊었습니다. 특히 AIBO 로봇 사례가 재미있었는데요. 마치 강아지, 갓난아기가 걷거나 일어서려는 시도를 하는 것처럼 강화학습을 통해 점차 유연하게 움직일 수 있게 됐다는 점이 흥미로웠습니다. 충실한 발표 잘 들었습니다. 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161208153624]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[532]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[첫 발표임에도 불구하고 많이 준비했다는 것이 눈으로 보이는 좋은 발표였습니다. 발표관련해서는 매우 흥미로운 예시로 로봇강아지의 예시를 들어서 좋았습니다. 해당 예시에서 복잡한 policy gradient 방법론이 아닌 naive한 policy gradient 방식으로 로봇강아지를 학습시킨 것을 확인할 수 있었는데 Occam's razor가 떠오르는 좋은 예시였습니다. 개인적으로 actor-critic 알고리즘이 EM과 비슷하다는 생각이 들어서 차이점을 상세히 파악해야 될 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161208161327]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[534]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[지난 function approximation 세미나를 준비하면서 공부를 했을 때, 상태에 대해서는 Continuous한 공간으로부터 Training하는 뚜렷한 예시가 있지만, Action에 대하여 명시적이지 않아 궁금증이 남아있었습니다. 이번 세미나의 로봇 강아지 예시를 통해 '다리를 얼마만큼 움직일 것인가'라는 continuous action을 적용하는 강화학습에 직관적이고 실용적으로 이해할 수 있었습니다. 지금까지 토대로 봤을 때, 강화학습에 적용되는 아이디어는 대부분 유사한 것 같습니다. 대부분 Monte Carlo, Temporal difference, Q-learning의 아이디어를 기반으로 확장을 해 나가는 것 같습니다. 기존의 아이디어들을 기반으로 새로운 아이디어를 결합하는 것이 연구에 중요하다는 생각을 갖게 되었습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161208170442]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[535]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Silver 교수의 강의에는 없는 내용을 따로 논문을 찾아 보고 랩원들에게 알려준 점이 아주 인상 깊었습니다. 하지만 강의 내용과 개인적으로 찾아 본 논문의 각 알고리즘의 기본 아이디어와 목적은 잘 설명해 주었으나 구체적인 학습방법과 과정에 대한 설명이 부족하지 않았나 하는 아쉬움이 듭니다. 이번 세미나 발표에선 주로 Policy Gradient Methods의 장점을 집중적으로 봤다고 생각합니다. Policy Gradient Methods의 단점은 어떤것들이 있을지 궁금합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161212120711]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[536]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[첫 발표 임에도 무척 준비를 많이 했다는 생각이 들었습니다. 지금까지 저희가 다루었던 방법들은 "Value-based" 강화학습이고 이 방법의 문제점으로 Stochastic 한 문제에 적용이 어렵다는 점을 알아 보았습니다. Policy-based RL은 Policy자체를 approximate해서 function approximator에서 policy를 구하게 되며, 이를 위해서 objective function이 필요함을 알아보았습니다. 학습을 위해 Objective Function의 Gradient를 구하는 방법은 Finite Difference Policy Gradient, Monte-Carlo Policy Gradient, Actor-Critic Policy Gradient 등 세 가지 방법이 있음을 살펴 보았습니다. 강의를 듣다가 막히는 부분을 여러 논문을 찾아가면서 파악해보자 한 점이 좋았던 것 같습니다. 
AIBO의 예제에서 이해하기 쉽게 설명했던 점도 도움이 많이 되었습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161212151116]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[537]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[저도 마찬가지로 value function approximation 세미나에서 continuous state space에 대하여 주로 다뤄서 continuous action space에 대한 좀더 자세한 내용이 궁금했는데  policy-based를 다루면서 이에 대한 좀더 general한 방법론을 배울 수 있었습니다.  그리고 지금까지 여러 specific case에 대한 단점을 보완하면서 기존의 방법론으로부터 발전시킨 방법론에 도달할 수 있음을 알 수 있었습니다. 또한 agent와 model의 주어진 정보 따라 적절한 강화학습 방법론을 사용할 수 있음을 알게 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161212161316]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[538]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[현실에서의 로봇, 헬리콥터, 드론 같은 기계의 동작에 대한 강화학습이나 알파고 같은 경우에는 continuous state space이기 때문에 value-based가 아닌 policy를 직접 학습하는 방법이 필요합니다.  이번 세미나를 통해 policy gradient에 대해서 알 수 있었고 발표자가 잘 설명해주어서 쉽게 이해할 수 있었습니다. 발표시간이 짧았는데 이 점만 보완하면 괜찮을 것이라는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161216115349]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[539]]></uid>
		<content_uid><![CDATA[240]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[어려운 내용을 차분하고 임팩트 있게 설명한 발표였습니다. 특히 AIBO 논문을 예시로 가져와 설명하는 등 주어진 내용에만 국한되지 않고 주제와 관련된 여러 세부내용에 대한 논문들을 소개한 점이 매우 좋았습니다. 하지만 압축된 발표로 조금 일찍 끝난 만큼 논문들의 세부내용을 좀 더 구체적으로 설명해주었다면 더 좋았을 것 같습니다. 첫 발표임에도 불구하고 많은 노력을 한 흔적이 보입니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161219123217]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[540]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 sample-based planning의 아이디어인 real experience에서 model(mdp)구성하고 해당 mdp로부터 simulated experience를 sample링하여 planning 방법을 적용하는 아이디어는 직관적으로 좋은 아이디어라고 생각하였습니다. 다만 본 세미나에서 살짝 아쉬운 점은 후에 tree search 파트에서 단순 monte-carlo evaluation과 mcts(monte carlo tree search)의 차이를 명확하게 설명하지않았다는 점입니다.(개인적 지식부족으로 이해를 못한 것일수도 있습니다.) 현재로서는 monte-carlo evaluation방법 같은 경우는 real experience로부터 mdp를 구성한 후, 모든 action을 고려하여 q function을 update하고 mcts의 경우는 real experience로부터 mdp를 구성한 후, 내가 실제로 행한 action에 대해서 q-function을 update하여 space를 줄이고 현재 simulated 할 때 쓰인 정책에서의 해당 action을 update하는 것으로 이해하고 있는 데, 좀 더 찾아봐야할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161225105140]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[541]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[발표 초반부에 지금까지 배운 내용을 개괄하여 준 점이 좋았습니다. 세미나의 끝무렵까지 와있고, 지금까지 배운내용이 많았기에 머릿속에 뒤죽박죽 있던 내용이 정리되는 느낌이었습니다. 이번 세미나 및 Silver의 강의는 지금까지 배운 내용을 종합하여 실질적으로 어떻게 강화 학습을 구현하는가 하는 내용이 아니였나 생각합니다. 그러한 면에서 Planning과 Learning이 어떻게 통합 되는지, 그 과정이 발표에서 좀 더 부각되었으면 더욱 알찬 발표가 되지 않았을까 하는 아쉬움이 남습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161225231625]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[542]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[이전에 배웠던 내용을 잘 정리해준 점이 도움이 되었습니다. 그 외에 새로운 Dyna-Q 라는 알고리즘을 소개하고 간단한 예제를 살펴보았습니다. 그 외에 MCTS(Monte Carlo Tree Search) 와 TD Search에 대해 설명해줬는데 이전에 알파고 논문에서 MCTS를 사용했다는 점을 들어서 흥미있게 들었습니다. 개인적인 생각으로는 강의가 뒷부분으로 갈수록 간단한 예제라도 실제 구현을 해보고 활용해 보면 좋을 것 같았습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161228104641]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[543]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[점점 새로 배우는 내용이 많아질수록 앞 부분과 연결이 안되고 머리 속에서 헷갈렸는데 발표자가세미나 시작할 때 그동안 했던 내용을 깔끔하게 표로 정리하여 이해하는데 많은 도움이 되었습니다. 하지만 개인적으로는 AB example이 잘 이해되지 않아 명쾌한 설명을 기대했는데 빨리 지나간 듯하여 아쉬웠습니다. 새로운 개념인 Dyna에 대해 알게 되었는데 아직까진 기존의 learning, planning과의 차이점이 명확하게 와 닿지 않아 추가 학습이 필요할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161228123555]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[544]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[Planning과 learning의 조합이라는 주제 하에서 세미나를 진행했는데, 주된 idea는 direct RL 파트였던것 같습니다. 뒷부분에서 바둑 관련 예제를 통해 이해를 시도한 부분이 좋았는데, 다만 조금 더 여러 방면으로 바둑을 바라보는 시각을 설명해 주던지, 조금 더 경우의 수가 적은 예제를 통해 이해를 도왔으면 하는 아쉬움이 남습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161228124121]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[545]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[지난 PGM 발표 때에 비해 발표 내용을 완전히 숙지하고 강의 내용을 차근차근 설명해 준 점이 좋았습니다. 발표 초반에 지금까지 배운 내용을 요약해 준 점도 인상 깊었습니다. 다만 예시를 좀 더 제시해줬다면 더 좋은 발표가 됐을 것 같습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161228124812]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[546]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[Dyna-Q와 Monte-Carlo Tree Search에 대한 개념을 접할 수 있었습니다. 올해 이슈였던 알파고에서도 MCTS의 기법이 사용되었었고 이를 설명해주는 부분을 매우 흥미롭게 들었습니다. 새로운 개념이 등장할 때마다 이전에 공부했던 개념들이 정확하게 이해했던 것인지 매우 의문이 들어 처음부터 다시 보곤 했습니다. Learning과 Planing의 차이를 아직 정확히 모르고 있는것 같습니다. 뒤로 돌아가 이 개념을 짚고 다시 이 챕터로 돌아와야 할 것 같습니다.  발표자분 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161228124916]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[547]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[11]]></user_uid>
		<user_display><![CDATA[류 나현]]></user_display>
		<content><![CDATA[planning과 learning을 적용한 model-based learning에 대하여 배웠습니다. 이전의 내용들과 많이 연결되어 있어서 이번 세미나를 이해하는데에 어려움은 없었지만 세미나가 좀 짧아 아쉬웠습니다.  planning의 경우 모델과 state를 아는데 이것을 model-based learning에서 어떻게 모델을 학습하는지 개인적으로 아직 헛갈리는 부분이 있어 추가 공부를 할 계획입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161228125254]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[548]]></uid>
		<content_uid><![CDATA[251]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[오늘 세미나의 첫번째 시간에서는 형석이가 Exploration &amp; Exploitation에 대하여 설명하였습니다.
먼저 들으면서 생각이 들었던 점은 교수님께서 언급하셨듯이 새로운 개념이 아니라는 것입니다.  MABP(Multi-armed Bandit problem)을 예시로 쭉 설명을 하여 주었는데 직관적으로 이해하기 쉬웠습니다.  첫번째로 여러가지 방법론을 설명해 주었지만 각 arm에 연결된 머신들이 각각의 parameter를 가진  poisson 분포를 따른다고 가정할때 (jackpot이 터지는!) 각 머신의 정확한 파라미터 추정을 위해서 현실세계에서 초기에 어느정도의 데이터를 이용하여야 하는가? 라는 문제에 빠질수 있다고 생각하였습니다. 두번째로는 Value(star)를 이용할때의 실제적으로 최적화시에는 상당히 큰 값으로 할당 하여야 하는데 이 값에 대한 문제도 발생할 수 있다고 생각하였습니다. 실제적으로 본인이 해당 방법론을 이용하여 본적이 없어서 이 부분에 관하여서는 직접 부딪쳐 봐야 되겠다고 생각하였습니다.  그리고 뒤쪽의 greedy 방법론의 variant들을 실험적인 결과를 비교하였습니다. 그리고 Bayesian bandit with upper confidence bounds에 관한 발상은 확실히 실전에서 사용이 가능하다고 느꼈습니다. 이때까지의 모든 방법론들은 Value에 대한 정의가 말끔해야 한다는 가장큰 허들이 있습니다. 이번방학때의 생각하는 아이디어를 피팅하여 보기 위한 framework를 한번 잘 생각해 보아서 이때까지 배웠던 방법론을 직접 부딪쳐 봐야 되겠다는 생각을 하였습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161229000536]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[550]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[금일 세미나 시간의 두번째 발표는 보섭이의 보드 게임안에서의 RL 적용방법과 그 역사를 언급하였고 연구의 과거시점에서 발생하는 문제점과 그 문제점을 어떻게 극복하였는지 경우의 수가 적은 경우에서는 과거 방법론이 좋았지만 그 이상에서는 잘 적용이 되지 않아 어떻게 극복하였는지 하나하나 차근차근 설명해 주어서 좋았습니다. 그 기저에서 사람의 1:1 경기시 Zero-sum game이라는것,  그리고 상대와 내가 turn 방식의 제도에서는 상대방의 수를 예측하는것보다는 실제 수를 둔 뒤에 value search algorithm을 해야한다는 가장 기본적인 개념부터 시작하였습니다. 실제 발표 뒤에 궁금점이 남는것은 3목을 예시로 들었는데 승패의 경우처럼 무승부의 경우도 상당히 높은 확률로 발생한다는 것입니다. 이 경우 policy를 승패에 비하여 어떻게 update하는지가 궁금해져서 찾아 보려고 합니다. 그리고 언급하였듯이 처음에 random initialization을 하였을때 사람의 기본적인 발상에서 게임의 룰을 따르는 policy를 만들려면 오래 걸릴수도 있다는것입니다. 여기서 생각이 든것은 물론 optimization을 하는것이 당연하고 어느정도 수렴한다는것이지만 이런 문제에서는 초기에 사람손을 거친 정보가 꼭 필요할수 있겠다는 생각이 들었습니다. 3목시 초수가 2행2열에 두게 되면 3줄이 일직선상에 있을수 있는 경우의 수가 상대방 기준으로 8가지중 4가지를 잃게 되기 때문입니다. 이 경우 random policy를 이용하게 되면 이 경우를 얼마나 빨리 인식하느냐가 수렴속도의 중요한 요인이 될 수도 있다고 생각하였습니다. 그리고 Simple TD에서는 명확한 문제점이 발생하기 때문에 이를 위해서 TD Root, TD Leaf, Treestrap에 관하여 언급하였습니다. 상식적인 발상으로 Treestrap이 경우의수를 훨씬 많이 포괄할 수 있기 때문에 나머지 두개의 방식의 단점을 보완할 수 있습니다. 그리고 마지막으로 Simulation based에 관하여 언급하였는데 오늘 재선이가 발표할 Alpha-GO의 경우가 현 기보에서의 데이터의 한개점을 극복하기 위해 Policy를 통한 simulation 정보를 기반으로 다시 학습 하는경우인데 이는 다음번의 재선이의 발표가 기대됩니다. 그리고 이런 강화학습 연구가 게임에서 활발하게 연구되고 있는이유중 가장 큰 이유로 Value과 Environment의 명확함이라고 생각합니다. 현실의 많은 상황은 이를 규정하기가 쉽지 않기 때문입니다. 그리하여 오늘 교수님께서 말씀해 주셨듯이 Actuator가 있는 로봇의 경우 바로 value에 대한 값을 인식할 수 있기 때문에 이런 부분에서 연구를 진행 할 수 있는 상황이 주어진다면 아주 재미있을것 같습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161229001800]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[551]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[550]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[좋은 코멘트 감사드립니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20161229105646]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[553]]></uid>
		<content_uid><![CDATA[246]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[시뮬레이션 기반의 방법론에 대하여 배울 수 있었습니다. 여기에서 나온 MCTS는 이후 알파고를 공부하는 데에 도움이 되었습니다. 다만, 연구실 내 사람들과 함께 하는 자리이니만큼 또렷하게 들릴 수 있도록 목소리가 컸으면 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170101142920]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[554]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[현실적으로, 물리엔진이 크게 발달하고 대중화되기 전까지는 RL을 시뮬레이션이 쉬운 게임에 국한될 것이라고 생각합니다. 이에 따른 과거 클래식 보드게임에 적용된 기법들을 알기 쉽게 예를 들어가며 발표했습니다. 에이전트가 여럿 있는 상황에서 셀프 플레이를 통한 approximation과 improve 방법을 game theory의 기초 개념과 함께 설명하여 좋았습니다. 알수록 RL은 흥미로운 분야라고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170101143629]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[555]]></uid>
		<content_uid><![CDATA[251]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[exploritation은 기존에 학습결과가 좋았던 알던 길을 반복해서 가는 것을, exploration은 전혀 몰랐던 새로운 길을 가는 것이라고 이해하고 있습니다. 첫 세미나에서 다루었던 두 개념의 간략한 설명이 오늘 세미나에서 자세하게 다루어 졌습니다. 이전의 강화학습 세미나에서 다루었던 mc, td의 개념들이 exploitation이 되고 epsilon gready의 개념이 exploration이었다고 정리했습니다. 더욱 많은 개념을 소개해 주셨는데 이 부분을 소화하기 위해 좀 더 학습이 필요하다고 느낀 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170103024443]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[556]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[기존의 강화학습 세미나가 나를 기준으로하는 policy에 대한 내용만을 다루었던 것에 비해 나와 상대방의 policy가 시간에 따라 서로 영향을 받는다는 점이 매우 흥미로웠습니다. 체스와 바둑처럼 규칙이 명확하고 한 수 한 수를 주고받는 게임은 강화학습에서 중요하게 여기는 value와 enviroment를 규정하기 좋다고 생각합니다. 세미나 마지막에 동영상에서 데미스 하사비스가 언급했듯이 게임은 삶의 일정 부분을 모사한 소우주이기에 흥미로우며 일정부분 단순화화 제약이 걸려있다고 했습니다. 오늘 우리가 다루었던 게임들은 한 수 한 수를 주고받는 discrete한 상황에서 이루어지는 게임이었지만 실제 세상은 continuous하며 스타크래프트처럼 유저의 policy가 동시에 이루어지는 환경에서 RL이 작동하는 방식에 대한 연구가 더욱 현실상황에 맞는 연구방향이 되지 않을까 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170103030030]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[557]]></uid>
		<content_uid><![CDATA[251]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이미 위에서 다른 연구원이 언급했던 것처럼 이번 세미나에서 인상 깊었던 점은 Exploration과 Exploitation이 이미 우리에게 친숙한 개념이라는 것입니다. 일반적으로 어떤 해결해야 할 문제와 마주쳤을때 취할 수 있는 기본적인 문제 풀이 전략이 어떻게 구체적으로 알고리즘의 형태로 구현되는지 고민해 보는 것도 지식의 이해를 깊이 하는데 도움이 되었습니다. 다음으로 흥미로웠던 점은 직접적으로 문제를 풀기 어려울때 Bound를 사용하는 방법입니다. Variational method와 일맥상통하는 방법인 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170103131529]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[558]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Game Theory의 개념을 게임 문제에 차용하는 방법이 흥미로웠습니다. 예제를 통해 그 과정을 설명해 주어 이해하기가 쉬운 점도 있었습니다. 모든 경우의 수를 파악할 수 있는 간단한 게임에서 부터 그 수가 계산하기 어려운 바둑과 같은 게임까지 다루면서, 각 경우에 어떻게 문제를 해결하는가 하는 방법을 비교하면서 듣기 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170103131940]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[559]]></uid>
		<content_uid><![CDATA[251]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[Exploitation &amp; Exploration이라는 개념에 대해서 직관적으로 이해할 수 있는 세미나였고 해당 방법론은 비단 OR뿐만아니라 여러 기계학습에서 많이 차용되고 있는 개념이라는 것을 이해할 수 있었습니다. 앞선 세미나에서 발표하였던 epsilon-greedy 방법론 뿐만아니라 여러 variation들에 대해서 세미나에서 설명하였는데 인상 깊었던 방법론은 UCB로서 신뢰구간을 이용하는 방법론이었습니다. 해당 방법론을 적용할 수 있는 여러 다른 기계학습 알고리즘을 생각해보아야할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170104131010]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[560]]></uid>
		<content_uid><![CDATA[255]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[experience replay와 fixed Q-target을 직관적으로 설명하여 좋은 세미나였습니다. 개인적으로 fixed-Q target은 신경망의  weight 학습방법에도 활용할 수 있을 것 같다는 생각이 들었습니다. (성능이 좋을 지는 잘 모르겠지만) 또한 AlphaGo 파트에서 실제 Policy를 학습하는 과정과 더불어 실제로  AlphaGo가 게임을 두는 과정인 MCTS에 기반한  variation을 잘 설명하여 큰 framework를 이해하는 데에 도움이 되었습니다. 형석이의 세미나 파트에서 나왔던 exploration 개념이 AlphaGo에서 Rollout policy를 통해 방문횟수에 기반한 방법으로 구현되는 것이 인상적이었고 개인적으로 가장 인상깊었던 것은 결국 최대의 Q값을 가지는 action을 취하는 것이아니라 가장 많이 방문한 트리로 간다는 점이었습니다. 이는 결국 AlphaGo의 목적이 사람과 바둑을 두어 이기는 것이 목적이기때문에 상대방이 최적이 아님을 가정하고 두는 것과 일맥상통한 방법이라고 생각됩니다. 좋은 세미나 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170104134715]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[561]]></uid>
		<content_uid><![CDATA[251]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[늘 긍정하자는 말로 시작한 긍정적인 발표였습니다. 사실 Exploitation과 Exploration을 하는 것을 긍정적으로만 바라보며 접근하면 시간상의 손실이 있을 수 밖에 없지만, 거기에 투자하는 시간 조차도 긍정적으로 피가되고 살이되는 개념으로 바라보자 라는 뜻이 아니었을까 생각했습니다. 호프딩의 바운드 개념이 중간에 나와서 반가웠는데, 제가 발표했던 내용과 매우 유사하지만 약간은 다른 개념이었습니다. 제가 MCMC때 소개드렸던 개념은 양쪽 바운드가 다 있는 것이었는데, 본 세미나에서는 한쪽의 바운드만을 가지는 상황이었습니다. 처음 세미나를 들을 때로부터 1년이 지났는데, 그간 강의로부터 배운 점이라던지, 세미나를 진행하며 혹은 연구실 생활을 하며 배운 것들이 점점 쌓여간다는 느낌이 왠지 모르게 느껴지는 세미나였습니다. 다음학기에는 최적화 관련 강의를 찾아 들어봐야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170104142637]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[562]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[여러 게임에 대해 공부하고 준비한 것이 느껴지는 세미나였습니다. 특히 틱택토(?)라는 예시를 통해 쉽게 설명해준 것이 시퀀스를 이해하는데 좋았던 것 같습니다. 틱택토는 너무 쉬운 게임이라 거의 대부분의 경우에서는 비기게 될 것 같긴 하다만, 그래도 이해에는 충분한 도움이 되었습니다. RL로 바둑까지 점령한 이 시점에 스타크래프트까지도 점령하려 하는데, action이 정말 많은 그 게임을 정말 정복할 수 있을지 기대가 되는 점이기도 합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170104143019]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[563]]></uid>
		<content_uid><![CDATA[255]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[DNN과 RL의 기본 가정 자체의 괴리에 의해 둘의 조합이 정말 어려운 것이었는데, 이를 어찌 해결하려 노력했는지로 본 세미나가 시작되었습니다. true value Q를 알지 못하는 부분에서 Q를 고정해둔 채로 학습을 진행하는 개념의 fixed Q-target이라는 아이디어가 처음 도입부에서 매우 신선했습니다. 또한 저장공간에 넣어두고 학습하는 방법은 DNN에서 batch 단위의 update같은 느낌이 들었습니다. 알파고는 4개의 정책을 고려하는 DQN 방법론을 가지는 방법론으로 이해했습니다. 그 중에서 자기 자신과 대국을 펼칠 때 다른 worker들은 해당 대국을 하지 않도록 하기 위해 약간의 수치적 수정작용을 해주는 rollout update idea는 특이한 발상이지만 테크니컬하게 배울점이 있다고 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170104144525]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[564]]></uid>
		<content_uid><![CDATA[255]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[먼저 Atari game에 관하여 어떻게 학습 하였는지에 관하여 상세하게 설명하였습니다. 
첫째로 재미있는점은 Action Node가 몇개 없다는점입니다. 실제 많은 상황속에서 Action node가 joystick과 같이 명확한 종류를 생각해 본다는 많이 떠오르지 않습니다. 허나 재선이가 언급한 것과 같이 물리적 action을 고정시켜놓았지만 하나의 framework으로 전체의 성능을 끌어올렸다는것은 큰 성과가 아닐수 없습니다. 34 page에서 double dunk의 성능이 좋지 않고 linear model에서 상당히 좋은것으로 나타나고 있는데 개인적으로 double dunk의 게임을 해본적이 없어서 한번 보았습니다. 
refernece : https://www.google.co.kr/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=double%20dunk%20atari
보게 되면 그 이유를 어느정도 이해할수 있는 부분이 생기는것 같습니다. 왜냐하면 Value function 자체를 score로 인식하였기 때문에 예시를 든 어뢰나 벽돌깨기 같은것은 상하좌우로 움직이는 형태가 아니라 비교적 고정이 되어 있는 형태 입니다. 하지만 농구의 스포츠 같은 경우 시작 점에서 슛을 쏘지 않고 좌우로 움직이는 것이 과연 value과 연관성이 있는가?라는 것을 생각해 보았을때 학습이 잘 안되는 이유를 직관적으로 이해할 수 있다고 생각합니다.
현실상황에서 본 연구에서 쓰이는 프레임워크를 적용한다고 생각하였을때는 명확한 환경 안에서 action space가 적은 공간안에서 행할 수 있다고 생각하였으며 발상 자체가 correlation을 없애려고 했다는점,실제 상황에서 60hz의 이미지를 어떻게 처리할것인지에 관하여 생각했다는점 등이 인상적으로 다가왔습니다.
다음으로는 AlphaGO에 관하여 발표 하였습니다. 개인적으로 본 연구를 깊게 공부해본적이 없기 때문에 전체를 이해하지는 못하였습니다. 허나 확실하게 와닿은 점은 우리가 구할수 있는 데이터는 한정되어 있고 그 데이터를 통하여 policy를 만들게 됩니다. 그 policy는 어찌보면 기계학습에서 항상 이야기하는 bias-variance problem에서의 large D(전체 데이터)를 구하지 못하기 때문에 편향된 policy를 만들수 있습니다. 하지만, simulation을 통한 policy를 구축하는 self-training 방식은 상당히 직관적이면서 데이터를 확장 시킬수 있다는 생각을 하는점에서 크게 와 닿았습니다. 그리고 교수님께서 말씀하신 연구에서 제시한 optimal hyper parameter를 찾는 tricky method가 있었으면 좋겠다는 생각을 하였습니다. 요즘 DNN을 train하면서도 하면 할 수록 체감적으로 얻는 노하우가 분명 생기기 때문에 좋은 하드웨어로도 10일 이상이 걸리는 본 연구에서 그러한 것을 알 수 있는 실험 환경안에서 몸소 체감해 보고 싶어졌습니다. 개인적으로 여러가지 constraint에 따른 optimal path를 구하는것을 강화학습으로 풀고 싶어서 MCTS에 대하여 알고자 하였는데 어느정도 예시를 통하여 풀어주어서 좋았습니다. Atari와 마찬가지로 경우의수가 엄청나게 많은 GO이지만, 상대방과 내가 풀어나가는 environment가 고정되어 있는 (시간에 따라서 환경이 바뀌지는 않는 상황이니) 문제에서 다음과 같이 풀었다는점에서 재미있었으며 시간에 따라서 환경이 바뀌는 상황에서는 꼭 강화학습으로 풀려고 시도하면 오히려 문제가 어려워질수 있겠다고 생각한 시간이었습니다.
감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170104202416]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[565]]></uid>
		<content_uid><![CDATA[251]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[큰 틀에서의 Exploration과 Exploitation의 개념을 잡아갈 수 있었습니다. 세미나 시간에 나눴던 이야기처럼, 이 개념은 강화학습 뿐만이 아니라 Gradient Decent나 Genetic algorithm 등에도 사용되는 철학이라는 것을 알 수 있었습니다. 이처럼 우리가 공부하는 머신러닝이나 다른 학문들도 마찬가지로 공유하는 아이디어가 있다는 생각이 듭니다. 깊게 파기 위해서는 넓게 파야겠다는 생각이 들었습니다.
UCT는 알파고에서도 사용되는 아이디어이며, 이를 확률론적으로도 풀어내어 깊게 알 수 있었습니다. 열심히 준비해주셔서 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170105042203]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[566]]></uid>
		<content_uid><![CDATA[255]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[알파고를 설명하기 위해 Deep Q-Network를 본인이 할당받지 않은 내용임에도 불구하고 세미나에서 다루어준 점을 칭찬하고 싶습니다. 발표 도입부에서 강화학습과 딥러닝이 서로 섞이기 힘든 알고리즘인데 Deep Q-Net과 알파고가 이를 잘 조합한 첫 사례라고 설명 해주었습니다. 하지만 이 부분이 잘 와닿지 않았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[1]]></unlike>
		<vote><![CDATA[-1]]></vote>
		<created><![CDATA[20170106192208]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[567]]></uid>
		<content_uid><![CDATA[255]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[먼저 2016년 후반기에 진행한 Reinforcement Learning강좌의 마지막 Seminar로서 유종의미를 느끼게 해준 발표자 박재선 군에게 칭찬합니다. 무엇보다 기존에 블로그에서 쉽게 접할수 있는 알파고와 아타리 게임 강화학습 내용정도의 shallow 한 영역이 아닌 논문을 기반으로 깊이 있는 발표 내용들은 지금까지 저희가 공부해온 내용들이 어떻게 적용하였는지를  detail하게 다루고 있어 청중으로 하여금 기존의 학습내용들과 잘 bridge를 연결 가능하였습니다.  너무 감사합니다. 한가지 아쉬운점은 이론적인 디테일뿐만 아니라 하드웨어나 소프트웨어적인, 즉 실험 환경에 대한 설명이 없어 조금은 아쉬움이 남습니다. 하지만 발표자가 내정되니 않은 상황에서 단시간내에 자발적으로 하여 발표한 상황을 고려한다면, 나무랄 것이 없는 발표라고 판단됩니다. 이러한 적극성과 준비자세는 박사과정인 저로 하여금 많은 동기부여를 주는 세미나 시간이었다고 생각합니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170108191622]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[568]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[게임을 통한 인간과 인공지능의 대결은 예전부터 많은 관심을 받은 주제입니다. 이러한 대결의 역사를 예전부터 잘 정리한 발표자룔를 통해서 큰 흐름을 읽을수 있었습니다. 또한 준비한 내용에 대해서 본인이 100% 이해하고 설명을 하고 있음이 잘 드러나는 세미나 발표였다고 생각합니다. 단지 아쉬운 부분은 슬라이드를 통해 내용을 진행함에있어 앞선 슬라이드가 다시 필요한 부분에 대해서 슬라이드 연결이 부자연스럽게 내용이 진행되어 앞으로는 이전 슬라이드를 다시 활용할 경우 다시금 슬라이드를 복사하여 진행을 한다면 좀더 자연스러운 발표가 되지않을까 생각합니다. 현재  Starcraft2가 Deep mind의 다음 topic으로 선정된 가운데 저또한 시간이 난다면 Starcraft1을 대상으로 공개된 API를 통해서 강화학습을 구현해보는 것도 재미있는 시도라고 생각됩니다. 관심있으신분 연락주시기 바랍니다!!! 마지막으로 발표를 잘 이끈 김보섭군 칭찬합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170108193125]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[569]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[형석이의 Deep learning summer school의 도입부분에 대하여 발표하였습니다.
본 발표에서는 어떤 이론이나 설명을 위하여 발표하는것이 아닌 제목그대로 전체 적인 theoretical motivation을 언급하였고 실제로 개인적으로 학습해보지 못한 부분도 몇가지 알려주어서 재미있었다. 중간에 발표한 shallow computer learning부분은 실제 컴퓨터 입장의 계산에서 꼭 필요하고 알고 있어야 하는 개념이다. 왜냐하면 미분으로 학습하는것보다 논리회로로 학습하는것을 이해해야지 코드로 구현하는법과 어디에서 연산이 많이 뭉쳐지는지를 알 수 있기 때문이다. 개인적으로 backpropagation을 할때의 Dropout과 Relu를 같이 쓰게 되면 Iterlation에서 Relu에서 Kill gradient가 되는 node들의 역전파가 모두 0이 되기 때문에 이 부분을 고려하고 dropout를 하는지가 궁금하였다. 왜냐하면 그것에 대한 Index를 제외하고 learning하는것이 빠른지 Indexing을 하는 시간과 메모리가 오히려 더 역효과인지 모르기 때문이다. 현재 구현되어 있는 코드들은 dropout rate를 각 layer에 할당하고 마지막 test propagation때 한번에 뭉치는 역할을 하는 코드기준으로는 고려하지 않기 때문이다. 실제로 relu를 통해서 나온 weight전체에 대한 0 weight들이 꽤 나오기 때문에 궁금하였다. 
  개인적으로 Optimization에 대하여서는 수식적으로가 아닌 성능적으로 거의 다다르지 않았나 생각이 든다. Full-batch를 사용할때는 Learning rate도 필요하지 않은 2개도 함수로써 전체를 밝히는방법론이 있고(실제로 거의 사용이 불가하지만) RMSProp, Adam 방법과 같은 2줄3줄이상의 activation function에서도 실제적으로 이제 PC기반으로도 연산이 가능하고(PC기반의 task에서는) 각 layer의 weight를 batch normalization하는것도 충분히 연산이 가능한 경지에 왔기 때문에 크게 성능변화가 있지 않다면 mini-batch방식의 hyperparameter가 필요한 방법도 충분하다고 생각이 든다. (물론 grid search 도 아닌, 2012년의 bengio 교수님의 hyperparameter의 random search를 통한 결과들이 모두 다르게 나타나기는 하지만 현실적으로 우리 입장에서는 쓸수 밖에 없기 때문이다
아직 Deep learning에 대하여 많이 학습하지 못하여서 이번 방학때 공부를 함에 있어서 전체적인 이정표를 나타내여 줄수 있는 발표여서 재미있었다. 특히 classification다음 부분부터 차근차근공부해서 최근의 논문까지 다다를수 있도록 노력해야지 이런 전체적인 내용도 한번에 보면서 이해할수 있겠다는 생각을 하였다.
이번주부터 시작되는 cs231n에서 하나하나씩 공부하면서 재미있는 방학이 되었으면 좋겠다.
감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170108200155]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[570]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[본 발표는 수현이가 기계학습에 대한 전반적인 내용을 담은 강의를 기반으로 발표 하였습니다.
가장 기초적인 부분인 {Supervised, Unsupervised,  Semi-supervised, reinforcement} learning에 관하여 개념적으로 간략히 소개를 하였으며 뒤에 Supervised loss function 에 관하여 일반적으로 사용하는 간단한 Error variant loss function 부터 SVM에서 사용하는 hinge loss와 여타 다른 loss function에 관하여 정리 하였고 간략하게 
처음에 말하고는 이제 많이 들어서 식상한 더이상 독립적인 분야로 기계학습이 있지 않다는것이었습니다. 그리고 기본적인 Expected risk 와 Empirical risk에 관하여 이야기 하였고 요즘에는 Structual risk가 SVM뿐안 아니라 CNN에서도 사용됨을 알 수 있으므로 전체적인 방향을 알고 이것을 적용하는것도 재미있는연구가 됨을 알 수 있습니다.   [reference] : Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization andStructured Prediction. Zhang, Sohn, Villegas, Pan, and Lee, CVPR 2015
그리고 마지막으로 bias variance decomposition에서 각각을 이해하며 해결점을 어떻게 해결할것인지에 관하여 개괄적으로 설명해 주었습니다. 전체적인 방향에 대하여 발표하는 시간이었습니다. 
감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170108230720]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[571]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[기계학습의 전반적인 내용을 담은 세미나였습니다. 특히 empirical risk와 expected risk 사이의 관계에 대해서 설명하는 부분이 매우 생각해볼 사항이었습니다. 보통 training error와 test error로서 모델을 훈련, 테스트하는 것이 익숙한 개념이었습니다. 이것을 유사한 용어인 empirical과 expected로 매칭시켜 생각해보면서 기존에 알고 있다고 생각했던 개념도 다시 한 번 짚어볼 수 있었습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170108233303]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[572]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[머신러닝의 전반적인 내용에 대해 소개하는 시간이었습니다. 여러 학문 분야에 걸쳐져 있다는 특징에 대해 언급하였고, 머신 러닝에는 Supervised Learning, Unsupervised Learning, Reinforcement Learning이 있음을 설명하였습니다. Supervised Learning은 예측을 위한 Predictive Model, Unsupervised Leaning은 Descriptive Model이다라는 것도 설명해 주셨습니다. 머신 러닝의 목적은 Generalization이 목적이며 모델의 평가를 위해 Loss function에 대한 이론에 대해 언급하였습니다. 발표 중에 교수님의 설명을 통해 Empirical Risk와 Expected Risk의 차이점을 자세히 알 수 있었고, 그 외 BA 시간에 배웠던 앙상블 개념에 대해 다시 정리하는 시간을 가졌습니다. 끝으로  시계열 데이터 등에서 학습용 데이터와 검증용 데이터와의 생성 분포 차이로 인해 발생하는 문제점에 대해 다시 생각해 볼 수 있는 좋은 시간이었습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109002444]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[573]]></uid>
		<content_uid><![CDATA[255]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[발표를 듣기 전에 얇게만 Monte Carlo Tree Search(MCTS)와 Policy Network, Value Network로 이루어진다 정도로만 알고 있었는데 그 세부 내용을 들여다 볼 수 있는 좋은 기회였습니다. 우선 동적확률계획법 수업을 들으면서 같이 살펴 보았던 DQN에 대해 자세히 설명해 주셨습니다. Atari 게임의 화면을 CNN을 통해 처리하여 레이블을 Atari 게임에서 할 수 있는 행동 (상하좌우버튼조합 등등)으로 나타낸다는 점이 인상적이었습니다. Rollout Policy이라는 새로운 개념 설명도 많은 도움이 되었습니다. 발표 외적으로 다른 과제나 준비해야 할 것도 많은 데에도 불구하고 자발적으로 발표 준비를 하는 점 등 항상 배울 점이 많은 것 같습니다. 발표 준비하시느라 정말 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109005557]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[574]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[우선 발표에 앞서 우리만의 Winter School이라는 센스 있는 애니메이션이 인상적이었습니다. Automating Feature Discovery 와 Image Recognition, NLP 등에서 많이 활용되고 있는 이론적 배경에 대해서 설명해주셨습니다. 딥러닝의 추상화 과정을 통해 다양한 문제를 풀 수 있고 특징으로 데이터가 많아야 하고 Generalization 성능을 보장해야 하며, Curse of Dimensionality 문제를 극복해야 한다는 점을 설명해 주셨습니다. 그 외 Multiple Layer representation, Distributed Representation, Connectionism 등에 대해 배웠습니다. 딥러닝이 공격을 받는 문제였던 로컬 옵티멈에 빠지는 문제에서, Global Optimum외에 Critical Point, Saddle Point에서의 성능도 충분히 활용할만한 가치가 있다는 점을 배웠습니다. 원 강의 예제 보다 실제 상황(고양이, 동화씨, 수지 등)을 통해서 설명해주신 점이 이해가 훨씬 쉬었으며 자연스러운 발표는 항상 배울 점이 많다고 생각합니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109011432]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[575]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[해당 발표에서 mirage of convexity 주제가 굉장히 흥미로웠습니다. 고차원 데이터에서는 대부분의 critical point가 saddle point이고 어떤 critical point가 local minima 중 하나에 해당하는 경우 global minimum과 큰차이가 안난다는 것이 골자인데 비록 수식적으로 증명된 것은 아니나 논문에서 실험적인 증거를 보여줬다는 것이 인상깊었습니다. 또한 step size parameter를 활용하지않기위해서 hessian을 활용하는 것이 필수적인데 hessian을 계산하기에는 complexity가 너무 높으므로 hessian matrix의 eigen value를 이용한 hessian free 방법 또한 굉장히 인상 깊었습니다. 비록 아직까지는 논문의 결과물들을 활용하는 응용연구를 하고있지만 언젠가는 optimize 방법을 개선하는 것과 같은 기초연구를 해보고 싶은 마음이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109110839]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[576]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 해당 세미나에서 가장 중요했던 개념은 function family와 risk라는 개념이라고 생각합니다. function family를 선택하고 hyper-parameter를 조정함으로써 function family의 범위를 크게하여  capacity가 높은 model을 선택할 수 도 있기때문입니다.(물론 overfitting의 문제도 발생할 수는 있지만) 또한 해당 세미나에서 어떤 model을 validation함에 있어서 여러 실용적으로 기계학습 model을 활용할 때 주의해야할 점을 상기시켜주어 좋았던 세미나였다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109111242]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[577]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[Intro에서 늘 나오는 여러 분야의 융합을 통한 machine learning을 얘기하며 세미나가 시작되었습니다. generalization error 측면에서 데이터를 분할하여 봐야함을 다시 상기하는 시간이 되었습니다. 사실 분석시에 전체적으로 탐색하며 validation set을 잘 이용하지 않고, test set으로 많이 접근하는 사례들이 있었는데, 다시한번 일반화 오류를 상기할 수 있기에 값진 시간이 되지 않았나 생각합니다. 이후에는 f를 추정할 때 필연적으로 bias, variance가 생김을 봤는데, 증명을 멋지게 한방에 써내려갔다면 더 보람찬 세미나가 되지 않았을까 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109132743]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[578]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[본 세미나에서는 deep learning이 좋은 이유에 대해서 경험적으로 나열하는 방식이었습니다. 그리고 이제까지 공격받았던 opt. 관련된 질문들을 되짚어보는 시간이었는데, local min에 빠지는 위험보다 saddle point에 안착해버리는 상황이 더 위험하다는 사실을 알게 되었습니다. saddle point에 빠질 위험이 있는 opt. 과정중에 이를 빠져나오기 위한 여러 수학적 방법론들을 시각적으로 확인하였습니다. 여러 방법론들이 그러한데, 최적의 parameter를 찾기 위해 노력하는 와중에 greedy하게 찾기도 하지만, 점진적으로 찾아가는 경우에는 opt. 방법론들이 쓰이게 되므로 이를 공부해 둔다면 앞으로 접할 여러 방법론을 이해하는데 도움이 될 것이라 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109133304]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[579]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[본 세미나에서 가장 중요한 부분은 모든 머신러닝분야에 해당하는 Generalization부분이라고 생각합니다.  Function family와 risk로 인해 모델의  capacity를 최대화 하면서 Overfitting을 야기하지 않도록 하기 위해서는 Validation을 통해서 이를 결정해야합니다. 이과정에서는 bias, varianc의 Trade-off를 통해서 최적의 일반화 성능을 이끌기 위해서는 모델을 학습함에 있어  Validation은 간과해서는 안될 부분이라고 생각합니다. 앞서 서덕성 학생이 언급한거처럼 Loss값의 decompostion을 수식 계산을 통해서 Bias, Variance를 설명하는 부분은 계속해서 언급되는 만큼 Swag있게 보여주지 못한부분은 다소 아쉽다고 생각합니다. 이상입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109133803]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[580]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 발표에서는 딥러닝의 theoretical motivation, 장점과 최근에 각광받는 이유에 대해 배울 수 있었습니다. 여러가지 장점들이 있지만, 그 중 딥러닝이 항상 공격 받았던 local minima에 빠지는 문제가 사실 saddle point이고 실험적으로 문제가 되지 않기 때문에 global optimum의 대안으로 사용할 수 있다는 점이 가장 인상 깊었습니다. 매번 느끼지만 개념을 설명할 때 적절한 시각 자료를 준비해 주셔서 이해에 많은 도움이 되었습니다. 이번에도 layer가 많아질수록 분류할 수 있는 영역이 많아지는 부분을 설명해주는 적절한 짤이 있어서 단순히 개념만으로 이해하지 않고 더 잘 와 닿았던 것 같습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109135247]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[581]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[기초적인 내용이라고 다 알고 있다고 착각하여 넘어가기가 쉬운 것 같습니다. 본 세미나가 이런 부분을 다루었습니다. 제가 인상깊게 본 것은 추상적인 개념으로 알았던 것들을 수식으로 구체화 한 점입니다. 이 수식은 앞으로도 자주 인용하게 될 것 같습니다. 
특히, Loss-function과 Generalization을 이해하기 쉽도록 기술했습니다. 일반적인 개념으로서의 loss-function과 training 관점에서 사용하는 surrogate loss의 차이를 생각하는 계기를 가졌습니다. 또한 train-validation-test data를 나누는 과정의 논리를 구체적으로 생각하는 기회가 되었습니다. 
이 내용은 모든 모델링 과정에서 필수적이고 기초적입니다. 하지만 간과하기 쉬우며 구체적으로 어떤 이유로 진행하는지 깊게 생각하지 않고 넘어가는 일이 잦습니다.
이번 세미나를 통해 머신러닝 공부를 밑바닥부터 시작하는 마음을 가지게 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109161934]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[582]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[수현이가 맡은 lecture도 마찬가지이고, 이 lecture 또한 제가 쉽게 그렇다고 가정하고 바로 모델링으로 넘어가는 부분을 콕 집어주었습니다. 
먼저 Yoshua	Bengio 교수의 말 중, "Deep learning은 feature representation을 학습하는 것이다"라는 말이 인상적이 였습니다. 이 말은 신선한 충격으로 다가왔고, 생각해보면 너무나도 잘 맞으며 당연할 수도 있다는 생각이 들었습니다. CNN을 간략하게나마 공부한  사람으로서, 이 말이 이제와서 인상적이었다는 것은 제가 공부를 허투루 하고 있다는 것이 아닌가 싶었습니다. 지엽적인 수식에만 집착하여 숲을 보지 못하고 있는 것 같다고 생각했습니다.
그리고 이 lecture의 theme은 Local optimum이 Global optimum에 준하는 성능을 보이기 때문에, Deep learning이 좋다는 것이라고 생각합니다. 물론 수식적으로 증명되지 않았기에 아쉬웠지만, 딥러닝의 정당성을 주장하는 것은 흥미로웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170109232346]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[583]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[딥러닝이 왜 좋은지 왜 우리가 공부해야 하는지 알수 있는 세미나가 되었습니다. 일반적으로 넌 파라미터 방식은 example 의 갯수만큼 region을 할수 잇고 파마미터의 갯수 만큼 region을 표현할 수 있는데 ,예를 들어서 KNN, 하지만 이러한 딥러닝의 차원 재표현은 지수적으로 파라미터와 지역을 증가 시킬수 있어 혁신적인 알고리즘이라고 할 수 있습니다. 우리는 이러한 재표현방식에서 유의미한 특징들을 추출할 수 있고 이 의미는즉 , examples에 근거한 prior에 의해서 패밀리 function 들은 선택되어지는데  히든노드와 히든레이어의 이용한 딥러닝은 powerful prior 을 가질 수 있다는 것이 큰 장점 이라고 할 수 있다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170110105650]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[584]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[기본적인 notion 을 잡기에 굉장히 좋은 세미나 였었습니다.  우리는 일반적인 오류를 최소화 하길 원하지만 우리는 그 참 예측함수를 실질적으로 구 할수 없기 때문에 실증적인 오류를 최소화 시켜 이를 예측하고자하는 방향이 머신러닝의 목적이라고 생각합니다.  이 강의에서는 앙상블 개념도 다루고 있으며 bagging-variance 를 줄이는 방향으로 , boosting-bias 를 줄이는 방향으로  일반화를 시킬 수 있다는 것을 이해하였습니다. 그리고 또한 기존에 있는 머신러닝 기법  - regularization은 딥러닝에서 decay, nosie injection 은 딥러닝에서 drop out 으로 활용된다는 점을 알게 되었고, 역시 기초가 중요한 것이라고 다시 생각해보는 계기가 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170110112441]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[585]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[강의자료에서 제시한 딥러닝의 이론적 기초는 두 가지로 나눌 수 있을 것 같습니다. 첫째, 딥러닝의 특징인 Distributed representation을 소개하고 이 방식이 가지는 장점이 딥러닝의 우수한 성능의 주요한 동력임을 제안했습니다. 둘째, 딥러닝이 공격을 받는 지점인 로컬 옵티마 문제를 방어하는 논거로 안장점을 소개했습니다. 발표에서는 두 번째 내용을 중점으로 다루었다고 생각합니다. 흥미로운 내용이었고 얻어간 아이디어도 많았지만 아쉬운 점이 남는다면 첫 번째 내용에 대해서 좀 더 구체적으로 소개해 주었으면 하는 것입니다. Distributed representation이란 개념은 이전 딥러닝 세미나에도 등장 했었지만 아직 구체적으로 어떤것인지 잘 와닿지 않습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170110124617]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[586]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[기계 학습의 기본 개념들이 알차게 소개되었던 강의와 발표였습니다. 하지만 다소 아쉬운 점이 남습니다. 세미나에 참여하는 인원은 이미 기계 학습의 주요 개념들에 대해 이미 알고 있기 때문에 이번 발표는 기존에 알던것을 다시 한 번 되짚어본 정도의 의미만 남습니다. 저는 이번 발표에서 기존에 알던 개념들 간의 관계에 집중해서 설명해 주기를 바랬었는데 개념 자체의 설명에 집중한 것 같아 아쉬움이 듭니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170110124911]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[587]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[이미지 데이터에 대해서는 처음 접하는 부분이라 이번 세미나를 더 흥미롭게 진행할 수 있지 않을까 생각합니다. 처음에 FeiFei 교수님께서 Computer Vision의 역사에 대해 소개하며, 지금의 수준, 그리고 현재 부족한 부분이 무엇인지 소개합니다. 여러 분야가 있다만 개인적으로 관심있는 분야는 image captioning인데, 이 부분을 공부하기 위해 CNN, RNN을 모두 공부해야 되는 것이므로 이번 세미나로 기반을 다지고 실제로 직접 접목해보기를 기대하고 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170111132152]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[588]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[경현이의 cs231n의 introduction부분의 발표였습니다. 내용적으로는 기존의 세미나 시간에 수차례 다려왔던 내용이라, 큰 지식의 습득은 없었습니다.  결국 Edge의 중요함을 시작하여 HoG같은 방법론으로 진행하다가 feature를 최적화 하는 방향으로 진행되어 왔다는것을 알 수 있었습니다.
언젠가는 마지막 페이지에 있는 오바마의 장난스런 모습도 텍스트로 나오게 된다면 정말 재미있을것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170111132237]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[589]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[딥러닝의 생물학적 Inspiration을 집중적으로 살펴볼 수 있어 좋았습니다. Hubel의 고양이 실험은 Hugo가 본인의 딥러닝 강의에서 생물학적 동기로 소개해 준 적이 있습니다. 그 당시 저도 Hugo의 소개를 보긴 했었지만 Hubel의 실험이 가지는 의의를 제대로 파악하지 못했었는데, 이번 세미나 발표를 통해 이해할 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170111132529]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[590]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[이번 강의는 Vision분야가 얼마나 중요한지, vision에서 deep learning을 사용함이 얼마나 타당한가를 다루어 앞으로 진행할 강의에 대한 자부심을 나타냈다고 생각합니다. 
생물의 시각적 능력의 발생은 "Evolution's Bigbang"이라고 칭할 만큼 굉장한 사건이었으며, 진화론적 관점에서 Vision의 유무는 생존에 굉장히 유리했음을 이야기했습니다. 
Hubel &amp; Wiesel의 실험과 비교하였을 때, convolutional neural network는 생물의 시각 메커니즘을 높은 수준에서 모사한 것이라는 생각이 들며 놀라웠습니다. 고양이 또한 edge를 인식하고 이를 추상화하여 세상을 바라본다는 것으로 받아들였으며, Larry Roberts의 block world로 저 또한 edge만으로 단순한 형태를 인지한 뒤 구체적인 특징들은 색깔, 명암 등으로 뒤늦게 인지한다는 생각이 들었고, 현대 computer vision은 제가 시각적 세상을 인지하는 기작을 추상적으로나마 알수 있게 도와준다는 생각에 흥미롭고 앞으로의 강의가 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170111140856]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[591]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[실제로 neural network를 공부하게되면 여러 강의에서 마찬가지로 두뇌를 어떻게 모방했는 지에 관해 설명을 해주는 데,이번 세미나에서는 Vision 분야의 예시로써 두뇌의 작동을 설명하여 매우 깊게 관련 내용을 이해할 수가 있었습니다. 결국 vision분야의 cnn이든 기존의 일반적인 deep neural network든 simple한 것에서 시작하여 high level abstraction으로 계층적으로 나아가게 되는데 이를 vision 도메인에서 edge로부터 시작하여 어떠한 모양체까지 넘어가는 부분에서 전체적인 neural network가 인간의 뇌를 모방한 것이라는 것을 이해할 수가 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170111142022]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[592]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[이번 겨울 방학때 꼭 들어야 겠다라고 목표를 세운 cs231n을 스터디로 진행하게 되어 기대가 큽니다. 여러 응용 분야 소개와 역사가 주된 내용이라 기술적인 내용은 많지 않았습니다. 컴퓨터 vision도 Hubel &amp; Wiesel의 실험에서와 같이 edge detection에서 시작한다는 점이 인상적이었습니다. 다음 세미나 부터 구현에 관련된 내용이 나와 실제 연구를 진행하는 데에 도움이 많이 될 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170111231715]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[593]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[그동안 이미지에 큰 관심이 없어서 vision의 역사가 이렇게 깊었다는 것을 깨닫지 못했습니다. 생물에게 눈이 생긴것에서 부터 이미지 segmentation까지 긴 역사를 배울수 있는 세미나였습니다. 이후로의 강의가 기대되고 방학동안 많이 공부할 수 있기를 기대합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170112110146]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[594]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[작년 딥러닝 세미나에서 computer vision 분야에 대해 발표하면서 vision분야에 관심을 가지게 되었고 개인적으로 외부 수업도 들어보면서 흥미를 가지게 되었습니다. cs231n강의를 들으면서 vision분야에 대한 지식이 넓혀질것이라고 기대하고 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170112123837]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[595]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[cs231n에서 가장 기본적인 k-nn classifier를 통하여 CIFAR10 예제와, Vision에서의 외생적 상황 background clutter, deformation, intra-class variation등을 소개 하였습니다.
k-NN의 가장 큰 문제점은 비전 분야에서뿐만 아니라 기존 다른 panel data에서도 test시간의 고려가 너무나도 크다는 것입니다. 그리고 본 예제에서의 이미지를 그대로 input으로 넣고 l1,l2등을 이용한 loss등을 사용하였을때 문제점을 이야기 하였고 결국 convolution이 왜 필요한지에 대한 기저를 잘 설명해 주었습니다. 그리고 강의 시간에 사용한 rawcode를 사용하여 설명해 주어서 좋았습니다. ]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170112173623]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[596]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[사람이 직관적으로 고양이 사진을 구분해 낼 수 있는것에 비해 컴퓨터 비전에서 고양이를 인식하기에 많은 장애물이 있음을 확인할 수 있었습니다. 가장 기본적인 이미지 데이터의 구조를 살펴볼 수 있었고 여러 classifier의 조합으로 w를 구성한다는 점을 잘 소개해 주었습니다. knn은 직관적이고 이해하기 쉬운 알고리즘이지만 이미지의 loss를 구성하는 부분에서 문제점이 있음을 보았고 향후 더 좋은 알고리즘을 학습하리라 기대합니다. 추가적으로 코딩을 설명해 준 부분이 좋았습니다. 감사합니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170112181658]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[597]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[image domain에서 classification 하는 데 문제가되는 여러 요소들을 알 수 있었으며 knn-classifier의 성능과 기타 여러 예시들로 image의 각 채널별 pixel의 밝기값의 차이를 이용한 방법론들은 image domain에서 크게 의미가 없음을 보여주어 우리가 왜 cnn을 학습해야하는 가에 대한 motivation을 자극시키는 좋은 세미나였다고 생각합니다.  또한 linear classifier어의 예제를 통해서도 각 label 별 어떠한 template을 얻을 수 있지만 실제로 해당 label의 어떤 특징을 뽑아내기보다는 채널 별 pixel 값에 많이 의존함(gray scale에서는 어떠한 효과도 얻을 수 없음)을 확인하였습니다. 향후 세미나가 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[2]]></unlike>
		<vote><![CDATA[-2]]></vote>
		<created><![CDATA[20170112183452]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[598]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[vision의 역사에 이어서 image recognition의 개념과 challenges에 대하여 알 수 있었습니다. kNN같은 경우 pixel dependency가 높아서 performance가 좋지 않음을 보았고, 앞으로 배울 neural net과 CNN, RNN에서는 이를 어떻게 극복하는지 기대가 되었습니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170112194710]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[599]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[Computer vision의 첫번째 목표는 사람의 시각적 능력을 모사하는 것이라고 생각합니다. 이에 제기된 문제가 Illumination, Deformation, Occlusion, Background clutter, Intraclass variation입니다. 앞에 언급한 조건이 크게 변하지 않는 이상 사람은 물체를 쉽게 구별하고 인식하지만, KNN, Linear classifier 등 기존의 computer vision algorithm은 조건이 조금만 바뀌어도 오작동을 하였습니다. 
이번 강의시간에는 기초적인 알고리즘과 데이터 분리를 다루었습니다. 앞으로의 강의가 더 기대됩니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170112221701]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[600]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[이미지 분류 문제에서 모든 클래스에 대한 분류기를 하드 코딩할 수 없기 때문에 데이터를 기반으로 한 접근법이 있음을 설명하였고 그 예시로 K-NN과 Linear Classifier에 대해서 다뤘습니다.  K-NN에 대한 알고리즘 구조상 모든 데이터를 저장해 놓고 예측할 시점에 모든 경우의 수를 다 비교해야 하는 문제점으로 사용하지 않는다는 점을 복습했고 그 외 MNIST를 Numpy를 이용하여 다루는 방법을 친절하게 설명해주신 점이 많은 도움이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170116152855]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[601]]></uid>
		<content_uid><![CDATA[251]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[세미나가 끝나고 우리끼리 편하게 이야기하는 자리에서도 Exploration과 Exploitation 개념을 활용한 농담을 할 정도로 직관적인 설명이 인상 깊었습니다. 앞서 많은 학우들이 언급했던 것처럼 다양한 학문 분야에서 활용되는 아이디어라는 점도 흥미로운 점이었습니다. 저는 아직까지 강의에 대한 이해가 부족하지만 좀 더 정진해서 많은 것들을 얻어가도록 노력하겠습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117125137]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[602]]></uid>
		<content_uid><![CDATA[252]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[강화학습의 응용 분야로 게임만큼 흥미로운 주제가 없을 거라 생각합니다. 사실 강화학습이 특히 국내에서 주목받게 된 계기도 이세돌 9단과 알파고의 바둑 경기였으니까요. 많은 예시들을 통해 강화학습이 게임에 어떻게 적용되는지 쉽게 설명해준 점이 좋았습니다. 김형석 학우가 이야기한 스타크래프트도 매우 즐거운 주제 같고요, 아직은 배움이 부족해 강화학습을 어떻게 활용해야 할지 감이 잘 오지 않지만, 저 같은 경우 강화학습을 활용해 챗봇을 만들어보고 싶은 욕구가 있습니다. 세미나에 그치지 말고 함께 공부하고 고민했던 주제들을 실제 결과물로도 만들면 좋지 않을까 합니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117125831]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[603]]></uid>
		<content_uid><![CDATA[255]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[상세하고 성실한 발표 잘 들었습니다. 그런데 제가 학습 이해도가 부족해서 그런지 강화학습 세미나 전체 내용 가운데 가장 난이도가 높게 느껴졌습니다. 하지만 강화학습과 딥러닝을 결합한 DQN의 얼개에 대해서 다소나마 접근할 수 있게 된 계기가 된 것 같습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117135546]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[604]]></uid>
		<content_uid><![CDATA[261]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[센스있는 발표 도입부가 인상적이었습니다. 딥러닝의 이론적 배경에 대해 설명하는 내용이었는데, 많은 데이터로부터 고도로 추상화된 wisdom을 추출한다는 메세지가 제겐 특히 와 닿았습니다. 사실 인문학 분야에서 지식들이 인간의 직관으로 탄생하는 경우가 많은데, 이 직관이라는 것은 그 사람이 경험해온 하루하루의 일상, 그로부터 추상화된 철학이나 세계관과 맞닿아 있기 때문입니다. 딥러닝이 인간의 뉴런을 모방한 뉴럴네트워크로부터 시작했다고 들었는데 앞으로 머신러닝/AI 방법론이 인간 사고나 지식의 어떤 점을 모방하고, 어떤 관계를 지녀야할지 깊이 생각해보게 하는 계기가 됐습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117140503]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[605]]></uid>
		<content_uid><![CDATA[262]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[기계학습 기본 개념을 짚어주는 발표였습니다. Generalization에 대한 개념을 좀 더 명확히 이해할 수 있게 돼 큰 수확이었습니다. 기초공사가 부실한 건물이 무너지기도 쉽듯, 기본 개념들을 확실히 해두지 않으면 나중에 애먹을 것 같다는 생각을 했습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117140844]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[606]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[비전의 역사 강의를 들으면서 인간 지성의 지평과 그 무한한 상상력에 다시 한번 감탄했습니다. computer vision의 눈부신 발전도 생물학, 심리학, 통계학 등 인접 학문의 도움 없이는 불가능했을거라 생각합니다. 제 능력의 한계로 모든 걸 섭렵하긴 어렵겠지만 가급적 넓은 시야에서 세상을 바라보고 문제를 해결해야겠다는 다짐을 하게 됐습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117141435]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[607]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[성급한 일반화 오류가 아닐까 하지만, 제 생각엔 머신러닝의 첫걸음이자 절반은 해결해야할 문제가 무엇이고 인풋과 아웃풋이 뭔지 정하는게 아닌가 합니다. 이런 관점에서 이번 강의는 인풋과 아웃풋, 문제 정의를 했다는 점에서 computer vision의 본질에 다가설 수 있는 계기가 된 것 같습니다. 코딩 등 성실한 준비가 돋보이는 발표였습니다. 준비하시느라 고생많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117141808]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[608]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[딥러닝이 만들어진 동기는 뉴런네트워크였고, 뉴런네트워크는 사람의 두뇌에서 발생하는 신경전달물질을 묘사해사 방법론으로 도출헸다는 사실이 인상깊었습니다. 한편으로는 유전알고리즘 또한 세대를 거듭하면서 부모와 자손의 교배와 전이로 최적해를 찾는 방법을 생각해보면 자연현상에서  의미 있고 새로운 방법론을 충분히 발견할 수 있지 않을 까 라는 생각이 듭니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117142346]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[609]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[이미지는  픽셀과 채널로 이루워져 있기 때문에 2가지 관점으로  접근할 수 있습니다. 예들들어 zero centering 을 할때 픽셀정보 마다 정규화를 시킬 수 가 있고, 채널별로 정규화를 시킬 수 있습니다. 후자일 경우 아웃풋이 3개의 평균이 각각 나오고 채널별로 centering  하기 때문에  계산하기 용이하다고 합니다. 즉, 이미지 분석에서 있어서 픽셀, 채널을 어떠한 관점을 보고 모델링하냐에 따라서 다양한 방법론이 될 수 있을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117143727]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[610]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 세미나에서는 CIFAR 10 data와 그 축소 버전 예시를 통해 linear classifier를 배울 수 있었습니다. 예시를 통해 Classifier로 KNN을 썼을 때 L1, L2 distance를 사용하여 구분하는 경우에 대해 배웠고 KNN이 classifier로서 적합하지 않은 이유에 대해서도 알 수 있는 세미나였습니다. 마지막으로 발표자가 비록 강의에서 사용하는 data는 아니었지만 MNIST를 파이선으로 구현하여 간략하게나마 설명해준 점이 좋았습니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117160047]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[611]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이미지 분류에서 풀어야 할 도전과제들을 흥미롭게 들었습니다. 모든 과제가  이미지의 Invariants를 찾는 것이었습니다. 각 Invariants를 찾기 위해 딥러닝이 컴퓨터 비전을 평정하기 이전에는 어떤 방식으로 이 문제를 다루어왔는지 간략하게 소개해 주었으면 더 알찬 발표가 아니었을까 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170117163652]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[612]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[해당 강의에서 놓칠 수 있었던 여러가지 질문들로 굉장히 원론적인 것부터 이해할 수 있는 세미나여서 좋았습니다. multiclass svm loss와 cross entropy loss를 잘 비교한 것이 좋았습니다. 개인적으로 이해가 조금 미진했던 부분은 regularizer 부분인데 해당 부분을 좀 더 공부해야겠다는 필요성을 느꼈습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170119080810]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[613]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[loss function은 잘 정의해서 사용하면 되겠지만, 널리 사용중인 대표적인 것들을 살펴봤습니다. 그리고 후반부에 CNN의 장점이 소개되었습니다. 기존의 이미지 분류 연구에서는 그 feature를 누가 의미있게 잘 선택했고, 학습을 잘 시켰냐가 관건이었다면, 2012년 Alexnet 이후로는 feature에 상관없이 대량의 데이터를 기반으로 누가 Network를 잘 구성하느냐의 문제로 바뀜을 들었습니다. 이미지 분류에서 CNN방식의 network 구축 방법론이 아닌, 이미지를 바라보는 참신한 아이디어가 떠오르길 바래봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170119152845]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[614]]></uid>
		<content_uid><![CDATA[263]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[현재많은 각광을 받고있는 Deep learning의 현재까지의 연구역사를 알수있는 재미있는 시간이었습니다. 예전부터 많은 강좌와 학습 자료를 통해서 반복해서 들어온 부분이지만 발표자가 지루하지않게 다양한 예시를 통해서 세미나를 진행해준점이 좋았습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170119153235]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[615]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[딥러닝이 현재 왜 주목받은지를 단적인 예(KNN)을 통해서 잘 보여주는 세미나 시간이었다고 생각합니다. 또한 스스로 학습내용을 이해고 코딩을 통해서 구현하여 알찬 세미나 시간이었다고 생각합니다. 저또한 현재 진행되는 전밙적인 수업 내용에 대해서 이를 실제로 구현이 가능할 정도로 이해하고 넘어갈수 있도록 노력하겠습니다. 좋은 발표 감사드립니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170119153550]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[616]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[딥러닝에서 현재 많이 사용되고 있는 Stochastic Gradient Descent(SGD) 이전에는 어떠한 방식으로 네트워크의 weight parameter들을 학습했는지 알수 있었던 시간이었습니. SGD뿐만아니라 다양한 variations들은 현재 네트워크 학습을 효율적으로 잘 학습할수 있도록 하여 지금까지도 딥러닝의 발전을 이어지게 해온것 같습니다. 어떻게 하면 좀더 개선될까 하고 끊임없이 생각하고 연구하는 자세를 가지고 앞으로 연구를 할수 있는 대학원생일 될수있도록 노력하겠습니다. 이상입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170119154132]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[617]]></uid>
		<content_uid><![CDATA[271]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[cs231d 자체가 computational하다는 부분을 명확히 짚었다는게 어떻게 보면 의미가 있다고 생각합니다. 이해할 때 컴퓨터같이 생각하면서 코딩했을 경우 어떻게 될까를 통해 이해하는 사람과 의미 자체를 이해하는 사람이 있는데, 강의에서 다루지 않는 의미자체, 수리적이고 기하학적인 접근으로 이해를 시도했다는 점이 높이 평가될 부분이라고 생각합니다. 본 강의는 back propagation 파트이지만 사실상 테크니컬한 부분은 많이 나오지 않았습니다. 앞으로 나올 부분은 많이 깊을것 같은데, 개인적으로 공부가 필요할 것 같다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170120010642]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[618]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[예시를 통해 loss function을 계산하는 방식에 대해 알아볼 수 있었던 세미나였습니다. loss function들 중 svm과 softmax에 숫자를 하나하나 대입하면서 loss를 구하여 더 이해하기 쉬웠던 것 같습니다. 원 강의에서는 말이 빨라 쉽게 놓칠수 있는 정보들을 발표자가 잘 정리하여 전달하여 유익하였습니다. 감사합니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170120104950]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[619]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[이번 세미나에서는 창엽형님이 Loss function과 Hinge loss와 Cross entropy의 cs231n의 예시를 들어가며 설명하여 주셨습니다.
가장 명확한 차이점은 SVM margin기반의 loss는 margin의 positive effect을 줄 수 없다는 것입니다. 따라서 결과가 다르게 나타날수 있습니다. 그리고 강의에서 나오지는 않았지만 CV에서는 tiny value로 보정을 하여 줍니다. 그리고 같은 Loss 값이라고 W가 여러가지가 나온다는것은 상당히 많은 생각을 할 수 있습니다. 따라서 Accuracy정보는 의미가 상당히 적어진다는 의미가 됩니다.
강의에서 왜 Negative Log likelihood인가에 대한 답변으로는 log를 쓰는것이 math metric에서 좀더 좋은 결과를 산출할수 있기 때문이라고 말하였습니다. 실제 구현의 쓰이는 코드를 보여준것도 좋았습니다. 감사합니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170120145034]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[620]]></uid>
		<content_uid><![CDATA[271]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[해동이가 Backprogagtion을 cs231n에서 gate기반으로 설명한것에 대하여 자신이 직접 기하학적으로 설명한 부분이 재미있었습니다. 실제 cs231n의 예제에서는 backprogation gate기준 multiple indegree example이 없습니다. 이 부분에 대하여 자신이 직접 예제를 짠것이 상당히 좋았습니다. 실제 우리가 생성하는 Network는 multiple일 확률이 상당히 높고 특히 Cross-Entropy부분의 gate기반 back propagation은 단일로는 생각하기 한계점이 있기 때문입니다. 그리고 Fully-connected layer에서 Jacobian matrix때문에 결국 bottleneck이 걸린다는점이 4강에서 내용이었습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170120145641]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[621]]></uid>
		<content_uid><![CDATA[271]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[항상 해동씨가 발표할 때마다 느끼는 것이지만 매끄러운 발표 스킬과 여유에 배울 점이 많다고 느꼈습니다. 그 외 예제를 직접 만들어서 강의에서 나온 것 보다 더 복잡한 구조에서의 설명도 많은 도움이 되었습니다. Gate 형태로 네트워크를 구성한 뒤 Chain-ruled으로 Backpropagation 하는 과정을 통해 이론적으로 쉽게 이해할 수 있었습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170121013712]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[622]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[강의의 내용을 충실히 반영하였고, 거기에 더해 본인의 이해를 반영하여 종합적으로 매우 좋은 발표였습니다. 특히 본인의 특기를 잘 살려, 코드 설명에서 유용한 팁을 알려준 점이 좋았습니다. Max-margin loss와 Softmax loss의 차이점을 알려준 점도 유용하였습니다. Max-margin이 조그만 파라미터의 변화에도 변동이 없다는 점은 이 함수가 강건하다는 의미로 받아들였습니다. 그렇다면 이러한 특성 때문에 Max-margin이 가지는 단점은 없는지 궁금합니다. 찾아보고 알게 된다면 연구실 구성원들에게 공유하도록 하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170121160819]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[623]]></uid>
		<content_uid><![CDATA[271]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[gradient descent를 원론적으로 왜 하는지를 설명하여 수학적 방법론에 대한 이해가 깊어졌습니다. 더불어 강좌상에서 코드로만 설명하던 것을 그래프로 도식화하여 예제를 보강한 점은 해당 세미나에 있어서 매우 칭찬할만한 점입니다. 또한 해동이만의 딥러닝에 대한 시각을 확인할 수 있어서 좋았습니다. 해동이의 시각에 따르면 딥러닝은 SIMD와 MIMD의 중간단계를 구현한 것과 같은 것으로  MIMD의 효과가 딥러닝의 계층적 구조로부터 얻어졌다고 생각할 수 있을 것 같습니다.. 알고리즘의 개선방향을 지엽적인 것에 두기보다 큰 개념의 변화으로 개선방향을 생각하는 것도 나쁘지않은 것 같습니다. 해당 방향에 대해서 깊이 생각해보아야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170122234533]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[624]]></uid>
		<content_uid><![CDATA[264]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[이미지를 linear classifier 하는 기초적인 방법을 배웠습니다. k-NN 기반과 선형 이미지 분류기를 사용하였는데 원본 그림에서 shifted 되거나 일부분이 가려지거나 색상이 변화게 되면 픽셀 값이 많이 달라지기 떄문에 이미지 분류에서 적용하기 힘들다는것을 느꼈습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170124124430]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[625]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[loss function, optimization, 이미지의 특징을 추출하는 HoG/ SIFT/Bag of Words 방법등에 대해서 발표를 들었습니다.  동영상 강의에서 발표자와 수업을 듣는 학생들의 질문과 답변을 정리하여서 세미나 발표를 하셔서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170124124837]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[626]]></uid>
		<content_uid><![CDATA[271]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[backpropagation과 neural network를 설계하는 방법에 대한 세미나를 들었습니다. backpropagation의 수학적인 방법에 대해서 설명하였는데 코딩할 때 직접 계산을 하지는 않지만 어떠한 원리로 딥러닝이 학습이 되는지 복습 할 수 있어서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170124130956]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[627]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[제가 처음 멀티 클래스에 대한 분류문제를 어떻게 해결해야할까라는 의문점을 들었을 때 SVM에서 이에 대한 해결책을 다루었고 그 개념이 다시 이 세미나에 등장해서 리마인드하는 계기가 되었습니다. 일반적으로 멀티클래스 분류문제를 2가지로 분류하고자하면 각클래스에 대한 특정 확률값을 도출해 가장 손실이 최소가되는 값을 선택하는 방식(softmax)이 있고 각 클래스마다 분류기를 설정하여 멀티 클래스의 해를 찾아가는 방식(leave one out)있다고 생각합니다. 이 중에 어떤 상황에서 어떤것이 좋고 나쁜지는 앞으로 공부해가면서 좀더 생각해 보겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170124132219]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[628]]></uid>
		<content_uid><![CDATA[271]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[딥러닝에서 최적화를 효율적으로 손실이 최소화하기 위해서 backpropagation 방식을 쓴다라는 것이 이번 세미나의 주제였습니다. 뒤에서 Training 방법에 대해서 구체적으로 다루는 데 활성함수 초기화 값 learning rate 등 여러 하아퍼파라미터가 있지만 본질적으로 손실함수를 낮추는것이고 이를 낮추기 위해서는 backpropagation 방식이 잘 이루어져야 한다고 생각합니다. backpropagation방식을 효율적으로 하기 위해 학습과정에서 정규화를 하게 되면 그 전달값자체에 변이가 되서 학습이 잘 안될 수 있다는 것을 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170124132806]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[629]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[금일 세미나에서는 Training Neural Networks Part 1을 다뤘습니다. 지난 시간까지 back propagation에 대해 gate 단위로 구해지는 것을 살펴 봤고, 이번 시간부터는 initialize, normalize, update등에 대해서 살펴봤습니다. 특히 mini batch normalize에 대해서는 세미나 중간에 깊은 얘기가 나왔는데, 왜 그런지에 대해서는 깊게 고민해 볼 문제라고 생각합니다. hat_x를 구하는 부분에서 normalize가 된 후에 BN이 진행되는데, 이것이 scaling과 shift의 역할을 합니다. relu func.을 activation func.으로 고정한 상태에서 pre-activation 상태의 값을 어떻게 최적화 해줄 수 있느냐의 문제로 마지막에 이해를 했는데, 약간은 더 생각해 볼 내용이 있지 않을까 싶습니다. 감마와 베타 파트에 대해 각 hidden layer에서 hidden unit마다 학습을 한다는 것이 너무 computational time이 오래걸릴 것 같아서 뭔가 다른것이 아닐까 싶기도 한데... 확실히 이해하지 못해서 섣불리 얘기하지 못한 부분이 있었던 것 같습니다. 다음 세미나인 목요일 발표때 명쾌한 해답을 기대해 봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170124165900]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[630]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[cs231d 해당 lecture에 있는 내용 뿐만아니라 연구실내의 image 분야의 모델링 유경험자로서 많은 노하우를 들을 수 있는 시간이어서 좋았습니다. 개인적으로 오늘 세미나의 흐름은 local gradient를 잘전파하기위해서  activation function을 적절히 변형, 초기  weight initialization에 neural net의 학습이 굉장히 dependent하므로 초기 initialize를 잘하는 법, pre-actviation의 distribution 형태에 따라 학습의 속도가 달라지기때문에 pre-activation을 적절히 변형하는 batch normalization에 관한 주제들이었습니다. 개인적으로 batch normalization이 학습속도를 증대시킬 수 있으나 decorrelation까지 해낼 수 있는가에 대한 의문이 있어 관련 논문을 읽어야겠습니다. 또한 relu가 pre-activation이 음수값이 나오는 데이터에 대한 gradient를 kill하는 내용을 개인적으로 생각해본 바 그러한 data point자체가 별로 데이터 자체에 분류기를 생성하는 데에 있어서 큰 의미를 가지지 않는 노이즈 정도로 생각할 수 있지않을까하는 생각입니다. 정확한 매칭은 아니지만 마치 svm에서 분류기 생성에 기여하는 support vector가 아닌 다른 data point들이 실제 Deep learning에서  relu에의해 무시되는 gradient를 생성하는 data가 아닐까 짧게 생각해봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170124180906]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[631]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[대체로 두루뭉술하게 알고있던 개념을 단단히 잡아가는 것 같습니다. chain rule에 의한 gradient의 backpropagation을 implement 관점에서 어떻게 계산되는지 구체화되었습니다. 정확한 기작은 모르더라도, non-linearity 함수의 역사와 성능에 대하여 생각을 할 수 있었던 것도 재미있는 일이였습니다. 웨이트 이니셜라이제이션 역시 신기하면서도 경험에 기반한 노하우를 전수받는 느낌이였습니다. 추상적으로 생각하기에는 xavier와 he의 weight initialization이 output값의 분산을 적절히 조절하는 것 같습니다. 이에 대한 적절한 논증이 있다고 하니, 논문을 구체적으로 읽어봐야겠다는 생각이 들었습니다.
Batch normalization 또한 재미있는 토론이였습니다. 이 또한 추상적으로 생각한다면, 제가 세미나에서 말했던 것처럼 gamma는 자극의 크기를 조절하고 beta는 자극의 역치를 최적화하는 것 같습니다. 이 역시 출현하게된 배경과 영향을 파악하기 위해서 원문을 보고싶어졌습니다. 
이 강의는 현대 Vision의 A to Z이며, 정말 잘 짜여진 강의라는 생각이 듭니다. Vision의 역사부터, 현대의 발전과 최신 이론까지 모두 넓게 다루고 있습니다. 깊이 또한 얕지 않으며, 더 공부하고 싶다는 의욕을 만들어내는 강의라고 생각합니다. 준비하느라 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170125013640]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[632]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[주먹으로 꼭꼭 말아서 만든 김밥같이 알찬 발표였습니다.  교수님께서 일전에 예견하셨던 것처럼 강의가 점점 어려워지고 있는데, 이번 강의가 그 분기점이 아닌가 생각합니다. 그만큼 내용이 많았고, 그에 비례해서 이해가 가지 않았던 사항도 많았습니다. 특히 Initialization에서 잘 이해하지 못 했던 부분이 많았었는데, 발표자의 친절한 설명으로 궁금했던 사항들을 쉽게 해소하였습니다. 세미나 이후에도 질문한 사항에 대해서 친절히 답변해준신 점에 대해서 감사드립니다. 특기할 만한 사항으로 이번 발표에서 Batch Normalization에 대해서 열띤 토론이 있었습니다. 학습 자료인 강의를 들을때는 깊게 생각해 보지 않고 쉽게 넘겼던 부분이었는데, 다시 생각해 보니 고민해 봐야할 거리가 많았습니다. 이런 기회를 통해 학우들과 의견을 교류하고 지식의 폭과 깊이를 넓힐 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170125032049]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[633]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[전반적으로 딥러닝 뉴런네트워크 학습이 어떻게 되는지 알수 있는 유익학 세미나 시간이었습니다. 베치 노멀라이제션 이나 여러 개념들을 알게 되었고 프리 트레이닝 , 러닝 레이트, 초기값 부여 등 다양한 하이퍼 파라미터가 존재하고 모델의 정확도를 높이기 위해서 하나 씩 테스트 해가면서 해답을 찾아 가야한다는 것이 한편으로는 일관성없는 해결책을 내는 듯하지만 한편으로는 잘 조절해 가면 우리가 원하는 특정 값을 찾을 수 있다는 생각이 들었습니다. 또한 뒤에 이어지는 드랍아웃이나 레굴러라이제이션을 마저 공부한 후 딥러닝 모델 구축 시 확인해야할 to do list 를 정리해 두면 유익할 것같다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170125181558]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[634]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[activation function에 대해서 상세히 설명해 주셔서 해당 내용을 이해하는데 도움이 많이 되었습니다. batch normalization이라는 개념을 통해 딥러닝 성능이 많이 개선되었다는 사실을 알고 있었는데 근본적인 작동원리나 구조에 대해서는 잘 모르는 상태였습니다. 이번 세미나에서 batch normalization에 대한 토론을 통해 좀 더 관심을 가지게 되었고 목요일 세미나가 기대 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170125191157]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[635]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[간단히 노드와 레이어의 조합으로 이루어진 딥러닝이 사실 그 내부에서는 깊은 이론의 조합으로 이루어져있음을 다시 느끼는 세미나였습니다. 'forward, backward연산으로 학습을 시킨다'는 일견 듣기에는 단순한 개념이지만 왜 그것이 가능한지 그리고 어떻게하면 더 효율적이고 정확하게 학습이 가능한지를 계속 고민해 보아야 함을 느꼈습니다. 마이너스 가중치가 제거되는 부분은 추상적으로는 우리의 뇌가 인지한 정보에서도 쓸모없는 정보는 점차 뉴런전파가 약해져 기억하지 못하는 부분과 비슷한 부분이 있는것이 아닌가하는 생각도 해보았습니다. 또한 Batch normalization에서 activiation function이 이미 고정된 함수의 형태를 지니고 있어 또다시 정규화를 해야하는 데이터의 특성이 있다는 부분이 아직 잘 이해가 되지 않습니다. 차분히 다시 고민해봐야 할 문제라고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170125211857]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[636]]></uid>
		<content_uid><![CDATA[270]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[Hinge loss와 Sofrmax loss function에 대하여 자세히 알 수 있었습니다. 세미나 발표 구성에 질문들이 있어 이해하는데 도움이 많이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170125233724]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[637]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[Image 분야에 경험이 많은 준홍씨의 노하우가 돋보인 발표였습니다. 강의를 보면서도 Batch Normalization에 대한 이해가 되지 않고 Activation Function에 잘 대응되도록 퍼뜨린다 정도로만 이해하고 있었는 데, 연구실원들과 토론하며 좀 더 배울 수 있었습니다. 그 외 머신 러닝의 여러개념 Regularization, Dropout, Batch Normalization이 서로 비슷한 기능을 하여 각 상황에 맞게 적용해야 한다는 점을 배웠고 최근에는 Batch Normalization이 필수적이라는 것을 배울 수 있었습니다. 그 외 ReLU Activation Function에서 마이너스로 오는 입력값은 다 날려버리는 데, 이 부분이 성능에 영향을 미치지 않는 이유에 대한 토론이 있었는 데 아직은 잘 감이 잡히지 않아 개인적으로 많은 공부가 필요할 것 같습니다. 5강 부터 갑자기 어려워진다라는 느낌을 많이 받았지만 여러모로 유익한 세미나였습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170126002749]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[638]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[Cs231n강의의 흐름을 따라가되 발표자가 생각했을 때 batch normalization과 같은 더 중요한 부분을 집중적으로 설명해주어 좋았습니다. 특히, 강의에서 빠르게 언급하고 지나가는 논문들은 하나하나 파악하려면 오랜 시간이 걸리는데 발표자가 습득한 후 소개해주어 간단하게나마 이해할 수 있었습니다, 하지만 batch normalization과 관련된 논문의 수식들은 아직 잘 이해가 되지 않아 차분히 앉아 추가 학습이 필요할 듯 합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170126121737]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[639]]></uid>
		<content_uid><![CDATA[271]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[Backpropagation과 관련하여 해동 오빠의 유쾌한 강의를 들었습니다. PPT를 구성하는 이미지 하나하나의 의미를 설명하면서 청중들의 관심을 잘 유도한 것 같습니다. 주로 computational한 측면을 강조하는 cs231n의 한계를 보완하기 위해 발표자 개인이 따로 이론적인 설명을 준비하여 균형 잡히게 배울 수 있던 세미나가 아니었나 하는 생각이 듭니다. Switch와 같은 언어를 본인만의 언어인 swapper로 바꾸는 등 발표자가 내용을 제대로 소화하고 넘어가는 모습이 인상적이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170126122452]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[640]]></uid>
		<content_uid><![CDATA[274]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[요즘 갓파씨님이 짜놓은 LSTM 코드를 커스터마이징해서 돌려보고 있는데 backpropagation 개념이 확실하지 않으면 이해가 어렵습니다. 특히 mul gate, add gate 등에 대한 카파시의 설명이 탁월한데 하나하나 그래프를 그려가보면서 따라가고 있습니다. 이번 준홍 씨 발표 역시 내용은 무척 어렵지만 네트워크를 만들고 적용해보는 데 있어 가장 중요한 실전팁들 아니었나 싶습니다. 활성함수는 뭘써야 하는지, 웨이트 매트릭스 초기값은 어떻게 줘야 하는지, 데이터 인풋은 미니배치로 할지, 또 그 배치를 normailize 해야 하는지 등등은 우리가 실제 뉴럴넷을 구현하는 데 있어 늘 맞딱뜨리는 문제이지 않나 합니다. 준홍 씨가 평소 공부하거나 프로젝트를 하면서 어려웠던 점을 스스로 고민해보고 찾아보고 공부한 결과를 이번 강의와 연관지어 설명해주신 점이 무척 좋았습니다. 다음 발표도 기대하겠습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170126123925]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[641]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 neural net에서 활용하는 optimization method 방법론들과 drop out이 이번 세미나의 핵심이었다고 생각하며 여러가지 방법론들을 잘 비교 설명하여 좋았습니다. learning rate를 다루는데에 있어서 또가지 방법론의 범주가 갈리는 것 같은데 요약하자면, 모델생성시 시간에 따른 decay등 여러가지 방법으로 사용자가 조절하는 방법들이 있고 그 learning rate를 data에 dependent한 방식으로 줄이는 방법론들이 있음을 확인하였습니다. 후술하니 learning rate보다는 학습의 정도 (gradient descent 값의 정도)로 후술하는 게 맞는 것 같네요. 또한 Drop out에 대한 논의도 매우 좋았습니다. 개인적으로는 Ensemble의 관점에서 이해하고 있었으나 다른 관점도 눈여겨봐야할 듯 합니다. 좋은 세미나 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170126213301]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[642]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[세미나 시간에 이야기 나누었듯이, Batch normalization은 아직 제게는 불명확한 것이 많은 것 같습니다. 논문을 읽고 더 깊이 공부해야겠다고 느꼈습니다. 
그리고 SGD의 다양한 방법론을 다루었는데, 시각적으로 명확하게 표현되어 어떤 일이 일어날 수 있는지 어느정도 가늠할 수 있었습니다. CS231n의 여러 장점 중, 큰 비중을 차지하는 것이 Karpathy가 경험했던 Training tip을 공유하는 것이라고 생각합니다. 이에 맞게 Learning rate 설정법, 모델 앙상블에 대한 trick, 이론의 implement  등을 익혔습니다.
초반의 강의에 비해서 난이도가 상승한다는 것이 체감되는데, 앞으로 더 열심히 공부해야겠다 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170127011203]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[643]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[많은 optimization method들을 접할 수 있는 세미나였습니다. 하나하나가 등장한 배경은 결국 수렴을 더욱 효율적으로 할 수 있도록 하는것에 초점을 맞춘 것 같습니다. 딥러닝의 학습에 있어 효율적인 학습이란 빠르게 수렴하는 것과 고른 가중치를 갖는 것이라 생각합니다. 가장 기본적인 SGD 방법론으로부터 learning rate의 변화, momentum 등 다양하게 발전해온 과정을 볼 수 있었습니다. dropout은 제가 느끼기에 딥러닝의 규모가 커질수록 무의미해지는 노드가 있을 것이라고 생각합니다. 그렇기에 일정비율 노드를 끊어준다는 것은 일견 느끼기에도 합당하다 생각합니다. 마지막으로 무엇보다도 준홍이형의 세미나는 본인이 실험을 하며 경험해온 것을 소개해주시는 부분이 있어 굉장히 집중도도 높고 노하우가 공유되는 느낌을 항상 받습니다. 이번에도 좋은 세미나 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170128231257]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[644]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[Batch Normalization개념을 다시 정리해 주셨지만 아직 제가 완전히 이해하지 못한것 같습니다. 논문과 다양한 사례들을 실험해보면서 개념을 정리해야 할 것 같습니다. 그 외, 여러 Optimization method들을 하나하나 코드를 확인하면서 설명해주신 점이 많은 도움이 되었습니다. 수식과 시각화 한 그림을 보면 각각의 메소드가 상황에 따라 다르게 동작하여 나중에 데이터에 맞게 전부다 적용해서 결과를 비교해 봐야할 것 같았습니다. 그 외 Learning Rate에 대한 개념과 앙상블 개념에 대한 설명도 많은 도움이 되었습니다. 한 주 내내 세미나 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170129125612]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[645]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[개인적인 내공이 부족한 탓에 Batch normalization에 관해 세미나 시간만으로는 이해가 온전히 되지 않았습니다. 세미나를 마친 후 논문을 살펴보고, 보섭이형과 얘기하며 개념을 좀 정리해 봤는데, 이해한 부분과 의문이 남는 부분을 남겨봅니다. 어떤 unit의 input으로 어떤 처리 과정을 진행하게 되면 더 성능이 좋아지며, 수렴이 더욱 효율적이라는 것인데, 해결할 처리 과정으로는 1. 0-centering, 2. same scaling, 3. de-correlation 이 되지 않을까 생각합니다. 처음 input layer에 대해서 전처리를 진행는것과 유사하게, 각 layer를 input으로 생각한다면 타당한 접근이 될 것이라 생각합니다. 여기에서 1번과 2번은 -mu와 /std 을 이용하면 되는데, de-corr.부분은 *gamma와 +beta를 이용해 접근한 것이 아닐까 생각중입니다. layer 내의 unit들이 de-corr.되기를 바라는데, gamma와 beta를 학습시킬 대상으로 포함시켜서 진행한다면 이것이 de-corr.의 효과를 가져오면서 성능향상을 볼 수 있는 것과 가까워지지 않을까 생각합니다. 물론, 원복이라던지 평행이동, scaling등의 효과의 관점으로도 볼 수 있을 것입니다. 3주 뒤 있을 재선, 경현의 실험 결과물로 궁금증이 해결되기를 바라고 있습니다. CS231n의 Karpathy 교수님의 여러 노하우와 준홍이형의 개인적인 노하우들이 집약된 멋진 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170130125709]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[646]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[무엇보다 지금껏 세미나 후기글중에 가장 간단 명료한 후기이네요. 지금까지의 cs231n 의 도입부 및 이론적인 background를 잘 정리하였고, 게다가 강의에서는 큰 비중을 차지하지는 않지만 많이들 사용되고 있는 batch normalization에 대해서 비중있게 다루어서 좋았습니다. 점차 딥러닝 학습에서 이러한 tricky한 학습 방법들이 많이 나오고 논문화 되고 있는 시점에서 현재까지의 방법론들을 어서 빨리 소화하여 좀더 개선하고 나은 방법론을 제안하기 까지 많은 노력을 통해 더 나아가고 싶습니다. 좋은 세미나 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170131130806]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[647]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[지난 시간의 batch normalization의 보충설명에 이어서 parameter update방식에 대해 배웠습니다. 항상 느끼는 점이지만 cs231n 강의는 여러 방식들을 소개하고 실제로 이것들을 사용할 때 computation 측면에서 어떤 장단점이 있는지 소개해 주어서 명쾌한듯 합니다. 강의자료에 나온 saddle point 그림에 발표자가 방향을 더 명확히 표시하여 이해하기 쉬웠습니다./]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170131131750]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[648]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[parameter update에 대한 다양한 방법들에 대해서 배웠습니다.  그림으로만 보던 내용을 자세한 수식과 프로그래밍 코드에 대해서 설명해주셔서 좋았습니다.  Dropout에 대해서도 배웠는데 세미나 시간에 batch normalization을 하면 dropout이 필요없다는 이야기가 있어서 이부분에 대해서 좀더 공부해봐야 될 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170131131838]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[649]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[CNN 구조, CNN역사, CNN에서의 convolution filter, activiation map, zero pad, deconv 등을 다루었고 Fully connected의 단점과 Inception, Skip connection에 대하여 다루었습니다. 
cs231n에서 지금까지가 기초적인 벽돌을 쌓는 과정이었다면 다음주부터 많은 variant들이 나오기 시작하는데 앞으로의 세미나가 기대되며 Inception + skip connection 이후의 새로운 혁명적인 구조가 나오기를 소망합니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170131170009]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[650]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[발표 앞 부분에서 CNN의 구조에 대해 CS231n 강의에서 다루지 않는 여러 자료를 소개해 주셔서 많은 도움이 되었습니다. Conv Layer, Pooling Layer, Norm Layer, FC-Layer와 AlexNet, ZFNet, Google-Net,VGGNET, ResNet등 역사에 대한 친절한 설명과 중간 중간에 센스있는 그림(고요속의 외침) 덕분에 이해하기 수월했던 것 같습니다. 슬라이드 장수가 말해 주듯 정리할 내용과 설명해야 할 내용도 많았었는 데 긴 발표 준비하시느라 고생 많으셨습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170131172230]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[651]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[본 강의에서는 다루지 않았던 기본적인 CNN구조에 대한 설명이 있어 좋았습니다. 해당 설명들을 통해서 CNN이 기존의 neural net에 비하여 어떠한 강점을 가지는지 알 수 있었습니다. 또한 ConvNet을 설명하면서 발표자가 이해한 순서와 이해에 어려움을 겪었던 부분들에 대하여 먼저 말을 하고 설명을 해주어 처음 공부를 하는 입장에서 공감하고 받아들이기에도 보다 용이했습니다. 2012년도에 AlexNet이 혁신적인 퍼포먼스를 보여주며 연구가 활발히 진행되었고, 2015년 ResNet을 기점으로 혁신적인 방법론 보다는 나와있는 연구들을 앙상블하는 형태로 변해가는 것 같다는 개인적인 느낌이 있는데 또 다시 재미있는 연구가 등장하기를 기대하는 바입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170131204611]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[652]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[cs231n의 메인디쉬 CNN이 나와 설렘 가득한 세미나였습니다. 앞으로는 CNN의 학습, 적용 사례, 빠르게 수렴시키는 법 등과 RNN과의 조합까지 다뤄지겠는데, 너무 기대가 됩니다. 특히 오늘 좋았던 점으로는, 기본 용어를 명확히 전달했다는 것과, CNN의 발전에서 여러 Net들의 흐름과 그 형태까지 다뤘다는 것이었습니다. 추세가 작은 filter로 깊게 쌓는 것이라는 것을 본 세미나를 통해 알 수 있었는데, 파라미터 수를 비약적으로 줄이면서 성능이 좋은 구조가 우리 연구실에서 생겨나길 기대해봅니다. 추가적으로 deconvnet이라는 것이 이런 개념이라는 것을 간략히 알 수 있었는데, 따로 공부할 때 도움이 많이 될 것이라 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170131224156]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[653]]></uid>
		<content_uid><![CDATA[286]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[저번 시간에 못 다한 Batch normalization에 대해 다시 한 번 연구실원들과 토론해 보는 좋은 시간을 가졌습니다. 혼란스러웠던 부분이 어느정도는 정리되었지만 아직 이 방법론에 대해서 완벽히 이해했다고 생각되지는 않습니다. 개인적으로 공부하고 보완해야 할 사항이 있다고 느꼈습니다. 일 차 미분을 활용한 최적화 방법론들을 최근 트렌드와 함께 설명해 주어서 실전에 어떤 것을 어떻게 적용해야 할지 정리를 하면서 발표를 들을 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170201110948]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[654]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Convolutional neural network (CNN) 의 기본 구조가 가장 일반적인 신경망인 Feedforward neural network과 다르기 때문에 이전 딥러닝 세미나에서 CNN을 공부했을 때 헛갈리는 부분이 많았다. Hugo의 강의와 현재 학습하고 있는 강의의 내용을 적절히 조합하여 기본 개념을 차근차근 쌓아서 이번에 확실히 CNN의 구조를 이해하는데 도움을 많이 받았다. 앞으로 어떤 방식으로 CNN을 학습할지, 그 내용이 기대된다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170201111816]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[655]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[Convolution의 forward 연산이 어떻게 진행되는지 알 수 있었습니다. 기초에 머물지 않고 강의에서 최근 모델 아키텍쳐의 트렌드가 어떤지 이야기하는 것이 좋았습니다. 강의에서는 기초적인 묘사만 하고 지나가는 각 case study의 모델을 자세하게 설명하려 하였습니다. 다만 Backpropagation에 대한 구체적인 정보가 없어 아쉬웠습니다.
하지만 앞으로의 강의는 CNN을 활용한 다양한 모델이 등장할 것입니다. 이를 위한 기초를 잘 다졌다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170201171520]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[656]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[Convolutional neural network을 익히는 데 있어 중요한 개념들을 다루는 시간이었습니다. 무엇보다 제가 오늘 발표자에게 감명을 받았던 것은 트렌드를 설명하려 노력한 점입니다. 개별적인 개념과 각 CNN 모델들이 있고 그들이 시간이 지나면서 어떻게 발전해왔는지 고민하며 학습을 해야겠다는 다짐을 하게되었습니다. 좋은 세미나 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170202115648]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[657]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[이번 세미나에서는 CNN의 구조에 대한 설명과 LeNet부터 ResNet까지 이어지는 CNN의 발전과정에 대한 발표를 들었습니다. 다음 발표를 준비하는데 많은 도움이 되었고, 활발한 토론이 있어서 좋은 세미나 였다고 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170202193823]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[658]]></uid>
		<content_uid><![CDATA[293]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[localization와 object detection task와 관련하여 생소한 용어들을 정리하여 좋았습니다. 또한 강의에서 다루지 않았던 SPPNet을 다룬 점도 좋았고 어려운 모델들을 쉽게 설명하려고 한 시도가 좋았습니다. 다만 아쉬웠던 점은 cs231n  lecture에서는 이 모델이 왜 나왔는 지에 대한 발단? 같은 것을 다루었는데 오늘 세미나에서는 그 점이 빠졌던 것 같아 조금 아쉬웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170202220520]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[659]]></uid>
		<content_uid><![CDATA[293]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[localization, detection task에 대해서 전반적으로 훑은 시간이었습니다. 개인적으로는 detection이 조금 더 재미있었는데, 시작이라고 볼 수 있는 R-CNN과 그 문제점(crop, warp로 인한 데이터 왜곡, 긴 run time 등)을 해결하고자  추후 모델들이 개발되어 가는 흐름을 알 수 있었습니다. 가장 좋았던 점으로는 제일 마지막에 간단한 형태로 각 detection model들을 도식화한 것이었는데, 이를 각 model마다 넣어서 정리하면서 넘어갔다면 각 model간 차이를 명확히 파악하여 소개할 수 있었을 것이며, 질문에 좀 더 적극적인 자세로 대응할 수 있게 되어 세미나를 좀 더 매끄럽게 이끌어갈 수 있지 않았을까 하는 아쉬움이 남습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170202230252]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[660]]></uid>
		<content_uid><![CDATA[293]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이전까지 모르던 컴퓨터 비전의 새로운 테스크를 알게 되어서 흥미롭게 들었던 발표입니다. 강의에서는 해당 수업의 목적이 기말 프로젝트 제안을 돕기 위해 분류 문제 이외의 테스크들을 소개해 주는 것이었기 때문에 알고리즘 설명이 부족한 부분이 있다고 느꼈습니다. 그런 아쉬움을 세미나 발표와 연구실원들과의 토론을 통해 보충할 수 있었습니다. 알고리즘 측면에서 흥미롭게 느꼈던 부분은 CNN을 하나의 구성요소로 사용했던 것입니다. 이전까지 분류 문제에서 본 CNN은 그 자체로 완결되는 구조였습니다. 이것을 R-CNN에서 전체 구조의 일부분, 추상적인 특성을 찾아주는 역할을 하는 요소로 활용하고 있는 점이 인상적이었습니다. 기존에 없던 새로운 구조를 구상해 내는 것도 중요한 작업이지만, 이미 있는 것들을 적절히 조합하고 개선하는 작업으로도 아주 유의미한 결과를 얻을 수 있다는 생각을 얻었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170204122039]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[661]]></uid>
		<content_uid><![CDATA[293]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[컴퓨터 비전에서 분류 문제 외에 Localization, Object Detection, Instance Segmentation 개념에 대해서 설명하였습니다. 그 외 강의에서 R-CNN, Fast R-CNN, Faster R-CNN, YOLO 에 대한 알고리즘 설명이 있었는 데, 강의 내용 만으로는 자세한 알고리즘에 대해서 이해하기에 어려움이 있었습니다. 해당 논문에 대해서 세미나 발표자가 읽어 본 뒤에 공유한다면 다른 연구원 들에게 더 큰 도움이 될 것 같습니다. 그 외에 CS231n에서 다루지 않은 SPPNet과 외부 강의 자료를 많이 인용한 점은 좋았던 것 같습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170207013838]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[662]]></uid>
		<content_uid><![CDATA[292]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[Convolution neural network의 기본 구조를 상세하게 설명해준 발표였습니다. 특히 요즘 연구트렌드를 설명해주신 점이 좋았습니다. 아직 CNN에 대해선 좀 생소한 개념들이 많은데, 남은 방학 중에 직접 구현해보면서 배워나갈 예정입니다. 한국어 문장 분류나 감성분석 등에 적용해볼 생각인데 이때 보섭씨 발표가 매우 큰 도움이 될 것 같습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170207125935]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[663]]></uid>
		<content_uid><![CDATA[293]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[Localization, Object Detection이 워낙 어려운 개념이다보니 청중도 발표자도 이해하기 쉽지 않았던 것 같습니다. 그럼에도 생소한 용어들을 차근차근 설명하려고 한 점이 돋보이는 발표였습니다. 다만 발표할 때 발음에 신경을 좀 더 써주신다면 훨씬 좋은 발표가 될 것 같습니다. 설명할 때 조급해하지 말고 여유를 두고 또박또박 말씀해주신다면 전달력이 높아질 것 같습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170207130453]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[664]]></uid>
		<content_uid><![CDATA[294]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[금일 세미나는 cs231n lecture에서 다룬 내용을 매우 세세하게 파고들어간 점이 좋았습니다. 개인적으로 해당 lecture에서 backprop를 할 때 model의 weight를 고정한 채로 data에 gradient를 전파하는 내용이 개인적으로 사고를 많이 넓힐 수 있었습니다. 강의를 듣기전에는 한번도 생각해보지 못한 부분이기 때문입니다.  또한 개인적으로 Neural style 같은 경우 model에 대한 이해가 아직 부족하여 개인적으로 논문을 읽어보아야하겠습니다. 구현체를 활용하기보다는 개인적으로 이미지를 모아서 시도해보는 것도 재미있을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170207183319]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[665]]></uid>
		<content_uid><![CDATA[294]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[CNN 내부의 feature map으로 있는 부분의 시각화를 어떻게 하면 표현할 수 있는지에 대해 알아보는 시간이었습니다. 처음에 KNN based image classification 부분을 발표할 때, weight를 시각화 하여 판넬을 만들수 있다고 강의에서 소개되었는데 이해하지 못했었습니다. 그런데 이번 기회를 통해 시각화 하는 방법론이 여러가지가 있고, 이 각각 개념이 정말 다양함을 확인할 수 있었습니다. 너무 어려워서 완전히 이해하기 위해서는 논문을 직접 봐야할 것이라고 생각했습니다. 마지막에 adversarial example에 대해서 다루었는데, 이것이 지난번에 잠시 얘기했던 GAN과 유사한 부분이 있지 않을까도 생각해 봤습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170210101806]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[666]]></uid>
		<content_uid><![CDATA[293]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[Localization의 접근 방식이 재미있었습니다. 이전에 Bengio 교수가 했던 말 중, 딥러닝은 Representation을 학습하는 것이라는 것을 다시 느꼈습니다. 한편, Object Detection 같은 경우는, 너무 두루뭉술해서 이해할 수 없었습니다. Region proposal의 구체적인 기작을 모르니 뒷단의 구조에 집중이 안되고 이해하기 어려웠습니다. 발표자가 그 부분을 다루어 주었다면 더 좋았을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170210175907]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[667]]></uid>
		<content_uid><![CDATA[294]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[오늘은 Visualization방법론에 대하여 발표하였습니다. 해당 부분은 Visualization으로 끝나는것이 아니라 다양한 app들을 개발할수 있는 기저의 학습내용이여서 재미있게 들었습니다. 특히 Guided-backpropagation에서의 결과가 논문에서는 더 좋게 나오는데 해당 부분은 직접 실험해볼 계획입니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170210180003]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[668]]></uid>
		<content_uid><![CDATA[296]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[RNN에 대한 structure에 이를 backprogation gate를 사용하여서 듣는사람입장에서 상당히 수월하게 해주신점 감사드립니다. 그리고 직접 한글로 구현된 (numpy를 이용한)것을 보여주셨는데, Tensor로 실행할경우 생각보다는 시간이 안걸린다는 점에서 신기하였고, 앞으로 학습할시 많은 도움이 될것 같습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170210180245]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[669]]></uid>
		<content_uid><![CDATA[294]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[CNN Task의 전반적인 시각화를 모두 다루었습니다. 가장 쉽게 접근하는 방법은 Activation map을 그대로 시각화 하는 것, 그리고 Filter의 weight 값을 그대로 시각화 하는 것 입니다. 하지만 이에 그치지 않고 Deep learning의 인과관계를 약간이라도 파악하기 위하여 어떤 분류 문제에 대하여 왜 이렇게 분류하였는 가를 판단하는 visualization을 하였습니다. 이는 Black box model의 단점을 약간이라도 해석하기 위한 시도라고 생각합니다. 
특히, Backpropagation을 input 이미지에 적용하였다는 것에 정말 감탄했습니다. 어떤 모양이 이미지의 클래스를 만드는 지 대략적으로 추측해볼 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170210185114]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[670]]></uid>
		<content_uid><![CDATA[296]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[CS231n 강의 초반부와는 달리, 난이도가 점점 상승하면서 강의 자체로는 해결되지 않는 부분이 많아지고 있습니다. 이번 강의 또한 마찬가지였는데요, Vision을 주제로 하는 Deep learning 강의인 만큼 이번 강의의 주제는 Image Captioning이였습니다. 하지만 선술한대로 해당 논문을 자세하게 읽지 않았기에 이 주제를 이해하기는 어려웠습니다. 
하지만 직접 Computational graph를 작성하여 보여주어 RNN과 LSTM의 구조는 쉽게 이해할 수 있었습니다. 그리고 이를 직접 소설 '무정'으로 학습하여 보여준 결과 또한 재미있었습니다. 감사드립니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170210185655]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[671]]></uid>
		<content_uid><![CDATA[296]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[기창형의 발전이 돋보이는 세미나였습니다. computational graph를 직접그리고 간단하게나마 RNN과 LSTM을 구현하여 코드로 실행하여서 이해가 굉장히 쉬웠습니다.  또한 char-rnn 예제도 설명을 잘해주어서 가장 어렵다고 생각했던 rnn을 조금이나마 이해할 수 있었습니다. 이제 직접 구현해보는 일만 남았네요 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170210210232]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[672]]></uid>
		<content_uid><![CDATA[296]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[세미나시간에 교수님께서 극찬하셨듯이, RNN, LSTM을 graph로 표현해 직관적으로 이해를 돕고자 한 부분이 정말 너무 좋았습니다. 관련 분야 커뮤니티에 올리면 다른 사람들도 공부하는데 충분히 도움이 될 것이라 생각합니다. 또한 배운것을 이용해 직접 한글 소설과 영문 소설에 적용해서 실험을 진행해본 부분도 좋았는데, 학습이 멋지게 되지 않은 부분은 왜 그런지 생각해볼 문제일 것 같습니다. 다음번 기창형의 세미나 발표가 벌써 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170212234908]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[673]]></uid>
		<content_uid><![CDATA[294]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[ZFNet 복습을 통해 CNN에서 feature map들을 시각화 하는 방법을 복습하게 되었습니다.
이전에 Neural Style 논문 내용을 공부하였을때 이해하기 쉽지 않았었는데, 발표자께서 깊이 있게 설명해주셔서 많은 도움이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170213100423]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[674]]></uid>
		<content_uid><![CDATA[296]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[RNN, LSTM의 작동원리에 대해 상세히 설명해주셔서 많은 도움이 되었습니다. 강의에서 주어진 코드를 수정하면서 연구를 진행하시는 모습에 감명받았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170213100600]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[675]]></uid>
		<content_uid><![CDATA[294]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[여러개의 논문에서 발췌한 내용을 담고 있던 강의라 준비해야 할 분량이 많았음에도 불구하고 발표자가 충실히 발표를 해주었다. 질문에 막힘 없이 대답하는 모습에서 그러한 점이 더욱 느껴졌다. 딥러닝 모델을 내부 작동방식을 알 수 없다는 뜻에서 블랙박스 모형이라고 흔히 불리운다. 이번 발표에서는 아직 미지의 세계라 할 수 있는 딥러닝의 작동방식의 재미있는 부분을 여러개 관측할 수 있었다. 가장 인상 깊었던 점은 최적화를 역으로 적용하여 인풋 이미지를 만들어 내는 방법이었다. 과히 발생의 전화이라 할 수 있다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170226001007]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[676]]></uid>
		<content_uid><![CDATA[296]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[RNN과 LSTM을 Computational graph로 표현한 점이 돋보였다. 두 방법론의 Forward progagation과 Back propagation의 작동과정을 아주 쉽게 파악할 수 있었다. 자신이 관심 있어하는 분야에 열정을 가지고 공부하고 있는 기창 형님의 앞으로의 무궁한 발전을 기원합니다. 발표 마지막에 보여준 예제를 보고 한글 텍스트 분석에 대해 이런저런 생각이 들었다. cs231n 강의에서 추상 대수학 교재로 학습하고 생성한 임의의 텍스트와 같은 예시에서는 상당히 그럴싸한 문장이 만들어 졌다. 하지만 한글의 소설로 학습한 결과물은 강의에서 보여준 예시에 비해 좋지 않아 보인다. 비슷한 모델로 학습했는데, 한글이 영어보다 텍스트 분석이 어렵운 것이 아닌가, 하는 생각이 들었다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170226001855]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[677]]></uid>
		<content_uid><![CDATA[299]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[CNN을 실전에서 학습할 때 유용한 팁과 트릭을 강으외 세미나 발표를 통해 배웠습니다. 특히 흥미롭게 봤던 것이 데이터가 부족한 경우에 쓸 수 있는 방법이었습니다. 우리가 현실 문제를 풀 때 가장 쉽게 마주치는 문제 중 하나가 아마 부족한 양질의 데이터가 아닐까 생각합니다. 데이터가 많더라도 레이블이 없다든가 해서 당장 사용할 수 없는 경우가 많습니다. 이번에 배운 Data augmentation과 Transfer learning이 CNN과 이미지 데이터에서 뿐만 아니라 다른 분야에서도 비슷한 방법을 응용할 여지가 충분히 많다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170226003215]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[678]]></uid>
		<content_uid><![CDATA[299]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[VGGNet을 보면 한번에 19개 layer를 학습시키기에 적당한 파라미터 값을 찾기 어려워 적은 layer로 pre-training을 시키고 그 weight를 바탕으로 더 많은 레이어를 쌓아 학습하는 방법을 사용했다고 합니다. 이렇게 pre-training을 통해서 문제를 접근하는 방법은 다른 도메인이나 상황에서 딥러닝을 적용하는데 앞으로 꼭 다루고 이해해야 할 내용이 아닌가 싶습니다. 많은 layer을 쌓으면 성능이 좋아질 것 같고, 그리고 어떻게하면 training을 빠르게할 수 있을지는 지속적으로 고민하고 풀어내야 할 일이라고 생각합니다. 하드웨어 성능이 좋아짐으로써 이는 해결되어갈 문제이겠지만 더 효율적이고 좋은 알고리즘이 주어진 상황에서 가장 효율적인 방법을 제시해줄 것입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170226164240]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[679]]></uid>
		<content_uid><![CDATA[300]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[우선 프로젝트 최종보고 이후, 몸도 마음도 지친 상태임에도 불구하고, CS231의 후반부로 갈수록 깊어지는 강의 내용과 많은 내용의 논문들을 殺身成仁의 마음으로 최선을 다하여 준비한 자세에 먼저 박수를 보냅니다. 특히 많은 내용을 다루면서도 주도면밀하게 위와 같은 내용들을 하나하나 이해하고 이를 세미나 청중들에게 잘 전달한것 같습니다. 

 특히, 세미나 내용중에서 (1) segmentation 의 연구중 ' Recurrent convolutional Networks for scene labeling' 과 내용후반부의 'Show, Attent, and Tell' 이라는 연구가 필자로 하여금 인상깊었습니다.  Recurrent convolutional Networks for scene labeling에서는 재귀적으로 CONV Net의 성질을 통해서 Image scale에서 자유로운 Segmentation이 가능하도록 하며, 이과정에서 CON layer의 중복적인 연산을 최소화 하는 트릭은 실제로 다른 방법론에서도 사용이 가능하리라 생각됩니다. 둘째로   'Show, Attent, and Tell' 이라는 논문에 대한 소개는 평소에 막연하게 생각되던 영상과 사진의 Captioning에 대하여 정확하진 않지만 어느정도 실마리를 제공해주는 시간이 되었습니다. 특히 Hard & Soft attention 단계에서  Hard case의 경우 MCMC를 통해서 이를 추정한다고 하는데, 이는 개인적으로 논문을 통해서 알아보고 싶습니다. 또한 실제 예제나 코딩을 통해서 구현하고 실험해 보고 싶은 분야이기도 합니다. 

내일이 지나면 새학기가 시작됩니다. 이미 지나
가버린 지난학기의 아쉬운
일들을 잊고 새로운 마음가짐으로 각자의 연구 및 지식, 경험
등 여러곳에서 磨斧作針하는 DSBA 구성원
이 되도록 노력했으면 좋겠습니다.
다 같이 화이팅!]]></content>
		<like><![CDATA[3]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[3]]></vote>
		<created><![CDATA[20170226230008]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[680]]></uid>
		<content_uid><![CDATA[299]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[기존 연구들을 보면 특별한 task가 아닌 이상에 초기 weight를 Imagenet 데이터로 대체하는 일종의 transfer learning을 하게 됩니다. 이를 실제로 Tensorflow의 경우는 weight를 object로 가져오고 그 부분을 학습이 되지 않게 하는 일종의 손이 어느 정도 가게 되는 작업입니다. 그리고 Cropping이나 Input의 variant를 주는 방법론들은 실제 classification의 단점을 어느 정도 보완하자는 방법론입니다. classificiation은 class대비 mutually exclusive한 feature를 찾는 회적화식이기 때문에 class가 적고 여집합이 존재하는 경우 일반화 성능이 낮아질수 있기 때문입니다. 이부분에 대하여서 여러가지 방법론을 열거하고 사용하는 방법에 대하여 언급 하였습니다. 그리고 본 taks시 최대한 잘게 class를 나누어서 하는 방식이 기존 실험 계획법에서 언급하는 것과 같이 최대한 task를 잘게 쪼개는것이 좋다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170227152956]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[681]]></uid>
		<content_uid><![CDATA[300]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[해당 세미나는 cs231n의 중추적인 척추와 같은 주제입니다. Deconv를 구현하게 되면 항상 걸리는것이 buffer save문제이고 이것에 대하여 생각해 보았을때 1*1 conv 2stride를 생각하였으나 이는 이미 여러사람에게서 잘 쓰지 않는 방법론입니다. 해당 방법론들으로 연구주제를 생각해 보았을때 나올수 있는 많은 연구 아이디어가 쏟아지므로 해당 방법론은 이번학기에 실천하여 연구할 생각입니다. 특히 weakly supervised learning등은 데이터를 구하는데에 있어서 class가 잘 쪼개져 있다면 상당한 이용가치를 얻게 됩니다. 이를 통해 SSL통한 어플리케이션을 한단씩 쌓아나간다면 엄청나게 귀중한 연구 결과물을 얻을 수 있을것이라고 생각합니다. 그리고 MNIST데이터 셋을 통해 transform한 image를 원복하는 결과물을 보여줬었는데, 이는 이전에 vision 분야에서 회전 왜곡 현상에 대한 3d 원복 연구분야와 상당히 비슷하며 상당히 난제로 알고 있습니다. 따라서 이것의 기저가 과연 많이 연구된 얼굴 분야 이외에 어떤 분야에서 사용될수 있을지 생각중입니다. 해당 주에서는 상당히 많은 양이 있었고 준비하는데에 상당한 시간이 들었을것이라고 생각합니다. 하나하나 곱씹어 가면서 cs231n의 진면목을 학습하는데 주력할 계획입니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170227153619]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[682]]></uid>
		<content_uid><![CDATA[299]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[CNN 학습 과정에서 데이터가 부족한 현상을 해결하기 위해 Data augmentation과 Transfer learning 방법으로 해결하는 방법을 알아보았습니다. Convolution layer를 정교하게 쌓기 위해서 작은 해상도의 convolution을 여러층 쌓아서 하는것이 계산적인 측면에서 효율적인것을 알 수 있었고, 푸리에 변환 내용에 대해서 궁금했었는데 세미나 시간을 통해 알게 되어서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170228130041]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[683]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[기초적인 NLP의 word vector와 One-hot과 embedding방식을 알아 보았습니다.
SVD를 실제 NLP task에서 이요하게 되면 sparse matrix가 dense한 Matrix로 바뀌고 float연산이 일어나기 때문에 상당히 비효율적입니다. 그리고 개인적으로 Skip gram에서 tensor에서 1:1방식으로 진행하는것에 대하여 찾아보고 알게 되었는데, 컴퓨터 관점에서는 당연히 이해되는점이며 window size가 크게 되면 plus gate에서 gradient가 아주 클수 있는 환경이 크게 되는데 이에 대한 대처 방안은 없는지가 궁금하였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170302153114]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[684]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[새로 시작한 224d (NLP) 강좌의 intro였습니다. NLP의 기본 배경과, 우리가 다루고자 하는 점, 그리고 왜 NLP가 어려운지에 대해 다루었습니다. 단어를 벡터화해서 표현하고자 하는 방법론으로 가장 유명하고, 성능 또한 어느정도 인정받은 Word2Vec을 살짝 맛보았는데, 다음주에 있을 학습 파트가 기대됩니다. Word2Vec의 기본 가정인 주변 문맥을 통해 단어를 설명(또는 그 반대)한다는 것을 뛰어넘는 새로운 아이디어를 우리 연구실에서 만들어서 Word2Vec을 넘는 방법론이 만들어지기를 기대해봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170302172734]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[685]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 word2vec의 알고리즘을 보면서 항상 느낀것은 objective function을 정말 잘 정의한 케이스라는 것입니다. objective function을 잘 정의하는 것이 기계학습의 왕도라고 느끼고 있습니다. 해당 세미나에서는 CBOW와 skip-gram 중 왜 skip-gram이 학습이 잘 되느냐라는 것에 대한 토론이 있었는 데, 개인적으로는 CBOW의 경우가 같은 corpus라도 objective function에서 조건부의 제약이 많아 실제 학습할 수 있는 data는 적기 때문이라고 생각하였으나 gradient의 전파 관점에서 이해하는 것이 조금 더 합리적인 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170302222246]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[686]]></uid>
		<content_uid><![CDATA[299]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[지금까지 CS231n에서 계속해서 공부하던 것은 이론적인 것들이었습니다. 하지만 실제로 구현을 위해서는 또 다른 트릭들을 알아야합니다. 일반적으로 CNN은 같은 크기의 이미지를 input으로 받으며, 훈련과 학습 데이터는 온갖 크기의 이미지가 있습니다. 또한 우리는 시간이 모자랄 수 있으며, 더 빠르게 작동하는 모델을 만드는 것, 혹은 다른 사람이 잘 만들어 놓은 모델을 사용할 수도 있습니다. 이러한 것들을 모두 다루었으며, 이런 pre-trained 모델을 남기는 흐름은 이미지 뿐만 아니라 다양한 방면에 적용할 여지가 있는 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170303162330]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[687]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[딥러닝을 이미지에 이어 자연어 처리에 적용하는 강의입니다. 도입부분이였으며, 기초를 다지는 파트였습니다. 이전에 Word2vec을 제대로 알지도 못하고 적용하였었지만, 이번 기회에 Embedding을 비롯하여 텍스트를 딥러닝을 이용하여 처리하는 데에 기초를 다지고 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170303162503]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[688]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Word2Vec 방법에 대하여 연구원들 간에 활발한 토론이 있어서 좋았던 세미나였습니다. 강의에서는 수식을 위주로 설명하였는데, 세미나에서 인공신경망의 구조를 제시하며 설명을 하였습니다. 그렇기 때문에 활발한 토론이 있었다고 생각합니다. 수식의 전개만 따라가다 보면 큰 흐름을 놓치는 경우가 있기 때문입니다. 덕분에 Word2Vec에 대해서 깊이 있게 생각해 볼 수 있었습니다. 다음 세미나 시간에서 Negative sampling을 통해 Word2Vec을 학습하는 과정이 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170306170059]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[689]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[NLP강의의 가장 기본적인 파트로 NLP의 응용부분, 어려운 점 등에 대해 배울 수 있었습니다. 또한, 컴퓨터가 받아들이기 쉽도록 자연어를 표현하는 각 방법들과 그 방법의 한계, 이를 해결하기 위해 고안된 방법들까지 확인하며 복습할 수 있었던 시간이었습니다. 많은 내용을 소개하려고 한 발표자의 노력이 보이나 너무 많은 정보를 한 슬라이드에 넣다보니 수식을 직관적으로 이해하는데 약간의 어려움이 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170306170356]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[690]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[word2vec과 관련하여 모두들 기본적인 Background가 있었던 만큼, 열띤 토론의 장이 열린 세미나 시간이었다고 생각합니다. 현재 연구실에서 많은 비중을 가지고 있는 과제들이 Text 분석인 상황에서, 본 CS224d 의 NLP 세미나중 떠올리는 다양한 idea들을 실제 프로젝트 및 연구주제로 연결하는 것이 좋을 것 같습니다. 저는 본 세미나를 학습하고, 이를 준비하는 것 뿐만 아니라, 세미나 중에 발생하는 아이디어나 생각들을 연구노트에 하나씩 적어가고자 합니다. 항상 아이디어를 떠올리지만 이를 그때 마다 잡아두지 않아 놓치게 되는 경우가 많은 것 같습니다. 여러분도 틈틈히 이런 아이디어를 그때마다 적어두면 향후 연구주제를 찾게 될때, 좋은 도구가 될것이라 생각됩니다. 이상입니다.
마지막으로, 세미나를 준비한 모경현 학생에게 감사를 전합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170306171201]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[691]]></uid>
		<content_uid><![CDATA[296]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[hidden state에 cell state 을 고려한 LSTM 방법론은 infinite context를 가진다는 점에서 큰 의미가 있는 것 같다. 이 말은 즉, 불연속적으로 연결되어있고 멀리 있는 정보들을 cell state에 의해서 주고 받을 수 있다는 것이다. 이러한 특징은 비디오 분석시, 떨어져 있는 프레임 단위간에 관계를 이어준다는 점에서 다양한 분야 방법론에 응용 될 수 있을 것 같다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170306191524]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[692]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[word2vec은 딥러닝의 distributed representation에서 착안된 아이디어로 획기적이면서도 다양하게 사용되 온것 같습니다. word2vec의 단점에 대해서 생각해보면은 비슷한 방향성을 가지는 것 끼리 같이 묶이면서 새로운 차원으로 표현되는데 특정 단어의 극성을 제대로 표현하는데 한계가 있는것 같습니다. 이 말은 즉, 축소된 차원들을 가지고 충분히 원하고하는  모델의 성능을 가질지는 의문점이 있는것같습니다. 분명한건 재표현된 벡터들을 제대로 극성에 따라 분류될 수 있다면 훨씬 더 좋은 방법론으로 자리 잡지 않을까라는 생각을 가지게 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170306192319]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[693]]></uid>
		<content_uid><![CDATA[300]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[세그먼테이션의 특징은 픽셀단위로 라벨을 달아 foreground의 형태의 특징을 뽑아 내고자 하는 것이라고 생각합니다. 형태를 뽑아낼 지라도 그 형태는 같은 클래스로 취급될수 있고, 더 나아가 비슷한 형태들을 각 개별 객체로 구분하는 instance segemation이  있었습니다. 이미지, 비디오 분석을 테스트 타임이 실시간으로 이루어져야 의미가 높은데 픽셀마다 레이블링을 한다는 점이 오랜 계산시간을 야기한다는 점에 학습속도나 과정을 어떻게 효율 적으로 해서 개선시킬까라는 점이 segmentation에 이슈 인것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170306192859]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[694]]></uid>
		<content_uid><![CDATA[299]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[지금까지 학습방식의 효율성과 하이퍼 파라미터등 기술적으로 개선해서 모델의 성능을 향상시켰다면 근본적으로 데이터 그자체로 돌아가 그 이미지를 자르고 확대하고 변형시켜 보다 많은 example들을 확보하고 그로 인해 모델의 성능을 향상 시킬  수 있다는 것을 알 수 있었습니다. 무조건 데이터를 transformation 하는 것보다 원 이미지자체의 특성을 이해하고 그것에 맞는 augmation아 이루어져야 하지 않을 까라는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170306193301]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[695]]></uid>
		<content_uid><![CDATA[294]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[많은 분량의 논문을 밤새 준비하는 모습에 깊은 감명을 받았습니다. 개인적으로는 이번 cs231강의를 통해 지역적인 구조 특징을 잡아내는 CNN의 강점에 대해 알게 된 것이 큰 수확이라 생각합니다. CNN의 feature 맵들을 시각화하는 이번 챕터는 이미지뿐 아니라 텍스트 처리에도 괜찮은 영감을 주는 것 같습니다. 조만간 관련된 연구를 진행해볼까 합니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170307082954]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[696]]></uid>
		<content_uid><![CDATA[299]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[발표 중간 소개했던 stochastic rounding이 가장 인상 깊었습니다. 32비트 정보를 이 기법을 통해 16비트로 바꾸어 연산해도 그 정보 손실에 비해 계산복잡성 개선으로 얻는 이득이 크다는 아이디어입니다. 비슷하게 텍스트에도 적용해볼 수 있지 않을까 고민해 보고 있습니다. 예컨대 텍스트 처리의 경우 코퍼스 단어수가 십만개를 넘는 케이스가 비일비재한데 네트워크 학습이 어렵습니다. 형태나 의미가 비슷한 단어를 stochastic하게 뽑아 한 단어로 보아 단어수를 줄이는 방법도 가능할 것으로 생각되는데 앞으로 실험을 통해 좀 더 고민해보겠습니다. 발표 준비하시느라 고생많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170307083434]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[697]]></uid>
		<content_uid><![CDATA[300]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[CS231 강의 후반부는 솔직히 내용을 잘 이해하지 못하고 있습니다. 계속 정진하겠습니다. 소화해야할 분량이 많은데 성실하게 커버한 점이 돋보인 발표였습니다. 제 옆자리에 있는 재선 학우는 발군의 코딩, 수학실력을 가지고 있음에도 오래 앉아 공부하고 있는데 앞으로가 정말 기대가 많이 됩니다. 마침 옆자리에 있으니 많이 물어볼게요, 많이 알려주셔요. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170307083843]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[698]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[가장 많이 쓰고 친숙해서 당연히 알고 있다고 착각했던 word2vec에 대해 알아본 시간이었습니다. 논문과 학습자료를 찾아보고 강의를 들으면서 그 이해의 수준을 높여가고 있습니다. 다만 sg에 쓰이는 소프트맥스 식(강의 별첨문서 맨 마지막 페이지)의 분모 분자에 각각 임베딩된 단어벡터의 내적으로 계산을 하는데 두 벡터 크기가 1이라면 내적이 코사인 유사도가 되기 때문(곧 내적 크기가 커지면 두 벡터가 유사)에 이런 식을 쓰겠지 라는 추측을 하고 있지만 정확히 어떤 의미인지 확 와닿지는 않는다는 생각이 여전히 듭니다. 후속 발표가 있는 만큼 계속 고민해보겠습니다. 발표 준비하느라 고생많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170307084433]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[699]]></uid>
		<content_uid><![CDATA[304]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[고난이도 많은 분량의 발표 준비하느라 고생 많으셨습니다. 강의 이해 자체가 잘 안돼서 논문이나 관련 자료를 찾아보며 추가 학습할 계획입니다. 프레임/픽셀 블럭 단위로 image gradient를 뽑아 피처로 사용하는데, 텍스트 분석의 경우에도 음절/어절/문장/담화 단위로 syntax/sementic gradient를 뽑아서 쓰면 어떨까 라는 생각이 들었습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170307085327]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[700]]></uid>
		<content_uid><![CDATA[300]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[이전에 컴퓨터 비전 책에서 segmentation 내용을 공부할 때는 hand craft한 방법으로 수행해서 성능 측면에서 많이 부족해 보였는데, 다양한 CNN 기법들을 적용하여서 segmentation을 수행한 내용들을 배울 수 있었습니다. image detection 방법을 차용하여 instance 단위로 segmentation을 할 수 있는 부분도 흥미로웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170307115901]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[701]]></uid>
		<content_uid><![CDATA[304]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[아주 많은 내용이 압축되어 있던 강의로 세미나를 준비하느라 고생한 흔적이 역력합니다. 시간축 비디오는 이미지에서 시간축 하나가 추가 되었을 뿐인데, 복잡성은 그에 반해 매우 크게 증가하였다. 이 문제를 해결하기 위해 기존 방법들이 제안하였던 아이디어들이 재미있게 느껴졌다. 하지만 지엽적인 테크닉과 엔지니어링이 너무 많이 가미되었다는 느낌 또한 받았다. 이미지 인식에서 딥러닝이 그러한 지엽성을 깨었던 것처럼 비디오 분야에서도 조만간 큰 도약이 있지 않을까 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170309164554]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[702]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[이번 학기의 비정형 데이터 분석 수업과 더불어 세미나도 텍스트 관련 주제를 다루게 되어 아주 기대가 됩니다.  Word2vec이 갖고 있는 문제점과 DSBA Winter Seminar에서 Distributed Representation 에 대해서 박은정 박사님이 발표해주신 내용을 다시 생각해 보면서 들을 수 있는 좋은 세미나 였던 것 같습니다. 세미나가 끝날 때 쯤 CBOW와 Skip-Gram의 업데이트에 대한 논의도 다시 생각해 보는 데 많은 도움이 되었습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170309233642]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[703]]></uid>
		<content_uid><![CDATA[304]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[우선 가장 어려운 부분을 맡아 긴 시간 동안 발표 준비를 해주신 발표자 분께 감사 드립니다. 이미지 분야에서 AlexNet이 나온뒤 기존에 주로 사용했던 HOG 필터 및 기타 분류 알고리즘 들이 줄어들었던 것 처럼 비디오 분야도 큰 성능 향상을 보여줄 수 있는 분야라는 생각이 들었습니다. 동영상에서 frame 들을 샘플링해서 사용한다는 점과 RGB를 grayscale로 변환 한 뒤에 필터를 적용하는 부분은 계산 복잡도를 줄일 수 있지만 너무 많은 정보가 손실되고 이 부분이 성능에 많은 영향을 줄 것 같다는 느낌이 들었습니다. 강의 시간에 언급한 여러 논문 들에 대해 개인적인 공부가 많이 필요할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170309234706]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[704]]></uid>
		<content_uid><![CDATA[304]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[cs231n의 마기작 종착역인 Video부분에 대하여 발표 하였습니다. 해당 분야는 아직 핸들링 해본 경험이 없어서 전체적인 프레임웍이 확 와닿지는 않았습니다. Interpolation variant에 대하여 다루었는데 실제로 CNN을 하더라도 class activation mapping등에서 항상 발생하는 이슈중 하나 입니다. 너무 작게 되면 spatial 한 정보를 잃게 되고 deconv net과 같이 앞쪽의 pooling부분을 같이 이용하는 부분 그리고 atrous conv를 이용하는 방법 PVANET에서와 같은 방법등이 있습니다. 재미있었던 부분 tracking에 대한 부분인데 단편적으로 볼때에는 상당한 computing power가 필요한것으로 보였습니다. 요즘에 상당히 큰 부분을 차지하는 적은 연산 저전력 모듈에서 결과 시연을 하는 부분이 있으니 이부분도 video에서 상당한 부분을 차지 하지 않을까? 라는 생각을 하였습니다. 요즘은 이미지에 빠져 있어서 비디오 부분을 공부를 미루고 있는데 재미있는 일이 생긴다면 꼭 해보고 싶은 분야입니다. 새로운 부분이라 준비하는데에 많은 시간과 노력을 쓴 흔적이 많이 보였습니다. 다음번에 비슷한 세미나 주제로 연구실에서 준비할때 많은 도움이 될거라 생각됩니다. 준비하느라 고생많았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170310012016]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[705]]></uid>
		<content_uid><![CDATA[304]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[평상시 Video 분석에 대해서는 단순히 이미지 처리의 연장선으로서 쉽게 생각해왔는데, 본 세미나시간을 통해서 시간축을 고려함에 있어서 그 복잡성을 인지 할수 있었습니다. 또한 HOG, HOF, 와  MBF와 같은 motion boundary descuptor들을 활용하여, 이미지 분석의 연장성이 아닌 Video 분야에서의 독자적인 다양한 방법론이 인상적이었습니다. 또한,  captioning 과정에서는 이미지와 텍스트를 동시에 다루는 task로써 한번즈음 도전해보고 싶은 분야라 관심을 가지고 발표를 듣게 되었습니다. 다른 이미지 처리와 달리 생소한 video분야를 성심성의껏 준비한 발표자 김동화학생에게 감사의 말씀 전합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170310173145]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[706]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[more word vector라는 이름답게 단어를 어떻게 벡터화해서 표현할 것인가에 대한 세미나가 진행되었습니다. 널리 알려져있고, 연구실에서도 많이 사용하고있는 word2vec 방법론부터, GloVe, FastText까지 다루었는데, 이번 기회를 통해 word2vec을 좀 더 자세히 공부한 것 같아 내심 뿌듯합니다. GloVe가 word2vec이랑 아이디어 밑단은 비슷하다는 점과, word2vec의 계산상의 비효율적인 부분을 개선하려 노력한 점을 확인했습니다. 가장 최근에 나온 FastText는 영문 버젼으로 charater n-gram의 접근으로 단어를 임베딩하는데, 한글로는 어떻게 생각할 수 있을지 고민해볼 문제일 것 같습니다. 강의에서도 다루고, 세미나에서도 나왔던 얘기인데, word embedding의 결과를 어떻게 평가할지도 큰 문제가 될 것 같습니다. 현재로서는 임베딩 결과물을 어떤 task에 적용해서 성능이 더 좋게나온다면 임베딩이 잘된것이다 라고 보는 관점인데, 이보다는 임베딩 자체를 평가하는 방법이 탄탄하게 있다면 더 좋겠다는 생각을 했습니다. word2vec의 임베딩 구조는 지도학습이지만, 실제 output을 평가하는 부분에서는 지도학습적인 어떤 지표를 대입할 수 없기에 고민해볼 문제가 될것이라 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170311144944]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[707]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 word2vec 계열의 embedding 방법론을 많이 활용해 본적이 없고 이론만 이해하고있는 상태였는데 오늘 세미나와 저번 모경현 학우의 세미나와 금일 세미나의 토론을 토대로 더욱더 많이 이해할 수 있었습니다. 해당 세미나에서는 word2vec의 loss function이 결국 cross-entropy loss로 정의도고 cross-entropy-loss로 정의하기전 확률로바꾸는 과정에서 softmax function을 거쳐야하는데 거기서 분모에 해당하는 부분을 계산하는 것이 computational issue가 있음을 알았습니다. word2vec에서는 negative sampling의 방법론으로 극복을 하는데 컴퓨터적인 사고가 부족한 탓인지 hierarchical softmax는 이해하는데 어려움이 있었습니다. 해당 부분은 저 스스로 좀 더 탐구해봐야할 부분인 것 같습니다.
 또한 gloVe embedding 방법론의 목적식에 항상 이해가 안되는 부분이 있었는데 오늘 토론을 통해서 왜 그런식으로 loss가 정의되는 지를 내적의 의미에 집중하여 이해할 수 있었다는 것이 큰 소득이였던 것 같습니다. 
 오늘 세미나에서 토론하며 서로 이해해가는 과정은 좋았습니다. 개인적으로 아쉬웠던 점은 한글로 슬라이드를 재구성하는 과정에서 여러한 오탈자가 있어서 이해하는데 방해가되었다는 점입니다. 원문을 정확한 의미가 남도록 번역하지 못한다면 차라리 영어원문을 그대로 슬라이드에 옮기는 것이 더 좋을 것 같습니다. ]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312110457]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[708]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[항상 자세한 메커니즘도 모른 채 Word2vec을 사용해왔습니다. 이번 기회에 자세히 알아가고 싶습니다. 이에 word2vec의 여러가지 트릭들에 대하여 준비한 것은 좋았습니다. 하지만 단순한 설명에 그치고 청자들이 갖는 질문을 해결해주지 못한 것을 보아, 발표자가 이 트릭들에 대해 깊이 고민하지 않았다는 생각이 들었습니다. 
한편, CS224d에서도 설명이 깊지 않은 것 같습니다. 직접 구현할 수 있을 정도로 깊게 이해하기 위해서는 해당 논문들을 모두 읽어야겠다는 생각을 했습니다. word embedding은 일반적으로 텍스트를 다룰 때, 대부분의 모델의 성능을 향상시킬 수 있는 방법일 것입니다. 이를 위한 초석으로서 embedding을 착실히 공부해야겠다는 생각을 했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312110520]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[709]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[실제 word2vec 논문을 읽게 되면 발표자가 말한 방식들과 결국 Noise contrastive estimation (NCE)를 통하여 loss를 구하게 됩니다. (https://arxiv.org/pdf/1410.8251.pdf) 실제 tensorflow에서도 NCE를 통하여 구한것을 확인 할 수 있습니다. 이 부분은 논문을 읽으면 바로 나오는 부분이라 설명이 없어서 아쉬웠습니다. 실제 freq words 방식이나 h-softmax방식은 거의 사용하지 않는 것도 확인 할 수 있습니다. Word2Vec과 Glove는 연구실의 반 이상이 알고 있는 알고리즘이니 수업자료 이외에 재미있는 정보가 없는 부분도 아쉬웠습니다.  개인적으로 궁금한 점은 Embedding의 loss와 classification의 loss는 상관 관계가 항상 있지는 않을것이고  sementic한 정보를 제외한다면 특히 진행하고 있는 연구의 loss가 작은것이 좋은 window size라고 생각합니다. Glove도 비슷한 방식으로 진행할 수 있을것입니다. 개인적으로 word embedding은 방법론적인 큰 틀보다 fast text등과 같은 input을 고치는 방향이 훨씬 성능 결과에 좌지우지 될것 같습니다. 마치 아무리 좋은 classifier를 크게 만들어도 제대로된 변수 아이디어 몇개가 몇십차원을 줄이는 것처럼 말입니다. 텍스트 분야에서 앞으로 강의에서 One hot과 embedding사이의 재미있는 트릭들이 많았으면 좋겠다는 생각을 하였습니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170312112949]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[714]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[지금까지 알고 있었던 텍스트 분석의 방법으로는 bag of words 방법 뿐이었습니다.(사실 제가 알고 있던 방법이 bag of words라는 이름이었다는 것도 최근에 알았습니다). 때문에 단어를 벡터화하여 표현함으로써 단어 사이의 의미의 유사성 등을 벡터로서 표현해 낼 수 있다는 word vector의 방법이 정말 신기했습니다. 하지만 이번 세미나에서 어떻게 단어를 벡터화 할 것인지에 대해 자세하게 설명했음에도 불구하고 아직 많은 부분에 대해서 명확한 이해를 하지 못했기 때문에 개인적인 공부를 통해 부족한 부분을 채워야 할 것 같습니다. 또한 word2vec에서 각 단어의 임베딩은 loss를 최소화 하는 지도학습임에도 불구하고 결과를 평가함에 있어서는 마치 비지도학습의 경우 처럼 결과를 객관적으로 판단할 지표가 없다는 점이 참 아쉬웠습니다. 
이에 대한 해결 방안을 고안하는 것도 재미있는 연구주제가 될 수 있을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312131000]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[711]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[학부생 때는 몰랐던 유명한 word2vec을 알아볼 수 있는 좋은 시간이었습니다. 복잡하지 않고 간단하지만 매우 효율적인 아이디어로 텍스트 분석 계산 효율량을 이렇게까지 높일 수 있다는 점이 매우 흥미로웠습니다. 발표에서 나왔던 Hierarchical sampling과 Negative sampling 부분은 제대로 이해하지 못하여서 개인적으로 조금 더 알아봐야 할 시간을 가져야 할 듯 싶습니다. 
 특히 cs224d에서 강의를 들었을 때는 예제를 통한 설명 없이 바로 넘어가버려 다른 설명을 찾아보면 완전히 이해할 수 있을 것 같습니다.
gloVe의 경우는 word2Vec과는 다른 접근 방식을 가지고 있다는게 흥미로웠습니다. word2Vec보다는 자세히 이해하지 못했지만 이 방법론 역시 조금 더 깊은 공부가 필요하다는 생각을 했습니다..]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312124632]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[712]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[word2vec은 의미관계를 보존하는 알고리즘이라 생각해 왔습니다. 그렇기에 NLP 분야에서 굉장히 의미가 있는 표현 방법이라 생각했으나 지금에 와서는 one-hot으로 표현할 수 있는 벡터를 작은 차원 수로 줄이는 알고리즘 이상의 역할이 있는가란 생각이 듭니다. 지난 저의 세미나와 이번 세미나를 통해 그런 생각이 더 확고히 드는 것 같습니다. 좀 더 NLP 관점으로 들어가기 위해서는 word2vec을 비롯한 벡터화 알고리즘뿐만 아닌, 좀 더 나은 input을 주기 위한 방법을 생각해야 할 듯합니다. 오늘 다루어진 알고리즘들은 벡터화를 위해 가장 선구적인 알고리즘인 만큼 이들을 이해하고 작동 방식을 이해하여 이를 어떻게 다른 문제에 적용할 수 있는지 고민해보아야겠습니다. 준비하신 발표자 분도 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312124952]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[713]]></uid>
		<content_uid><![CDATA[304]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[강의의 마지막 내용에 해당하는 비디오를 다뤘습니다. 비디오라서 어려운 이유도 있었겠지만,휴가 다녀오며 뒤가 이어지지 않는 느낌이 들어서 개인적으로 이해하기 조금 어렵다는 느낌을 받긴 했습니다. 이미지 분야의 분석을 넘어 시간축까지 가지고있는 비디오 데이터를 분석하는 것이 발표자의 말에 의하면 아직 갈길이 멀다고 했습니다. 그만큼 우리가 할일이 더 많지 않을까 생각했습니다. 또한 비디오를 처리하고자 여러가지로 아이디어를 내고 정의하고 접근한 노고가 눈에 띄었습니다. 비정형 데이터 분석에 대한 수요가 높아지는 만큼 비디오라는 도메인도 익혀둔다면 직접 다루는 것을 넘어 다른 도메인에 아이디어를 적용해볼 수 있을만한 여지가 있을것 같다는 생각이 듭니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312125945]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[715]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[이번 세미나에서는 텍스트분야에 쓰이는 최신식 알고리즘에 대해서 공부한것 같습니다. 계층적인 샘플링에 대해서는 lecture에 언급되지 않았는데 논문을 좀더 봐서 더 이해하도록 하겠습니다. 차원축소로 representation 한다는 것이 차원의 수가 많고 문맥상의 순서가 뭉개지는 bag-of-words의 단점들을 극복하는 좋은 방법론 인것 같습니다. 도출되는 식 자체에서 프로그래밍적으로 포뮬러가 형식화 되어 있는데 어떠한 아이디어가 있을때 수리적으로 그리고 기호적으로 표현 할수 있는 능력을 길러 나가야 할 것 같습니다. 또한 해싱이라는 방법론을 사용해서 기존에 계산적으로 고통을 많이 받았던것을 정보 검색,처리를 이용해서 쉽게 연산한다는 것이 다른 분야에서 주로 다루고 있는 부분도 새롭게 채득해야 할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312131157]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[716]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[NLP 분야에서 많이 사용되는 알고리즘에 내부를 들여다 볼 수 있어서 좋은 시간이었던 것 같습니다. 발표를 들으면서 여러가지 궁금한 내용들이 있었는 데, Hierarchical Softmax에서 Binary Tree 를 구성하여 leaf node까지 log 2 V로 접근할 수 있다는 점은 이해가 되는 데, 어떤 기준으로 노드에서 left, right 로 분기하는 지에 대해 궁금했고 Glove는 Co-occurance Table 한 번 구축한 다음에 이를 참조하는 방식으로 이해했는 데, 텍스트 분야에서 높은 dimension이 문제가 되는 데, co-occurance table을 만든다는 점이 쉽게 이해가 되지 않았습니다. Matrix 연산을 병렬로 구성하여 분산 처리가 가능한 형태로 구현을 한 것인지 Sparse 형태로만 표현한 것인지 확인을 해봐야 할 것 같습니다. 그 외 세미나 시간에 토론하면서 더 많이 배운 것 같습니다. 발표 준비하시느라 고생하신 박민식님과 칠판에 친절히 그림까지 그려주시며 설명해주신 서덕성님에게 감사 말씀 드립니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312135408]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[717]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[지난 시간에 이어 NLP에서 많은 연구가 진행 된 word representation의 두번째 시간으로 지난 시간의의 방법론에 이어서 Glove에서부터 최근  Fast text 까지 다루는 시간이었습니다. Corpus가 커질수록 huge하고 Sparse한 co-occurrence matrixt로부터 dens한 low dimension으로 임베딩하기 위한 목적을 위해서 다양한 방법론에 대해서 다루었습니다. 특히, 평소에 관심이 많은 분야로 본 세미나에는 몇가지 다소 아쉬운 점이 있었습니다. 
첫번째, 발표를 진행하는 박민식학생의 경우 평소 발표를 진행함에 있어, 그 pitch가 낮고, 악센트가  없어 청중으로 하여금 다소 지루한 발표가 되는 경우가 있습니다. 본 세미나에서도 그러한 부분이 드러나지 않았나 생각됩니다. 이 부분은 앞으로 계속적인 피드백을 통해서 개선될거라 생각됩니다. 두번째, 세미나를 진행하는 가운데 발표가 자연스럽게 연결되지 않고, 툭툭 끊기는 경향이 있다고 생각합니다. 이는 세미나를 준비하는 가운데 슬라이드를 보면서 머릿속에서 정리된 sequence에 기반하여 진행하는것이 아닌 슬라이드에 기반하여 즉흥적으로 진행되는 경향이 있어 이러한 문제점이 발생하지 않나 싶습니다. 마지막으로, 세미나를 준비함에 있어 너무 briefly하게 lecture영상만을 다루지 않았나 싶습니다. 본 세미나에서 다루고 있는 부분에 대한 Lecture note 및 논문과 같은 추가적인 정보를 다루고 있지 않아 다소 아쉬움이 남습니다. 물론 이 같은 부분은 세미나를 학습하면서, 개인이 스스로 다룰 필요도 있지만, 세미나시간에서도 이를 같이 다룬다면 세미나 청중들과의 원활한 세미나 시간이 되지 않을까 생각됩니다. 
 위와 같은 개선점을 고친다면 앞으로 대학원 생활을 진행하면서 swag있는 발표자가 될 거라 기대합니다. 이상입니다. 발표를 준비하느라 수고한 박민식 학생에게 감사의 말씀 전합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312153550]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[718]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[기존에 관심이 많았던 텍스트를 주제로 새로운 세미나가 진행되어 기대가 큽니다. 가장 유명하고 널리 쓰이는 word2vec 방법론 외에 Hierarchical Softmax 방법론과 negative sampling 방법론에 대한 설명이 부족한 것 같아 아쉬운 마음이 들었습니다. https://arxiv.org/pdf/1411.2738.pdf논문과  http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf 논문을 참조하여 개인적으로 아쉬운 부분을 충족 시킬 수 있었습니다. Fast text같은 경우에는 최근에 나온 방법론으로 논문부터 읽으며 차근차근 공부를 해야겠다는 생각을 하였습니다. 발표 내용은 물론이고, 세미나 중에 하셨던 질문들과 서덕성님이 알려주신 고차원에서 words들 간의 거리를 통한 유사도를 구하는 그림은 큰 도움이 되었습니다. 감사합니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312154257]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[719]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[708]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[Skip-gram과 CBOW의 성능에 일반적으로 Skip-gram이 좋다고 알려져있습니다.
문득, 이는 CBOW의 hidden layer에서 averaging을 하는 것에 기인할 수 있다는 생각이 들었습니다. Skip-gram은 input과 output이 직접적으로 연결되어 관계가 적절히 보존되는 반면, CBOW의 averaging이 input 단어와 output 단어의 관계를 왜곡하여 성능이 저하된다는 가정입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312165849]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[720]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이번 세미나에서는 지난 시간보다 word representation방법에 대해 좀 더 구체적으로 학습하였습니다. 개인적으로 강의를 들으며 negative sampling 부분에서 단어를 예측하는데 어떻게 특정단어와 관련이 있고 없고를 판단하여 샘플링을 하는지 이해가 잘 되지 않아 답답했었는데 세미나 시간에 토론을 통해 배웠습니다. 그럼에도 불구하고 직접 구현해보지 않아 그런지 아직까지도 단어끼리 관련이 없다는 기준을 어떻게 세우는지에 대해 명확하게 와 닿지 않아 좀 더 공부가 필요할 것 같습니다. Fast Text는 이번에 알게 된 방법이라 좀 더 자세히 배우고 싶었는데 발표자가 빠르게 지나가 아쉬웠습니다. 다음부터 좀더 notation에 신경 써주셨으면 좋겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312190457]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[721]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[단어를 벡터로 바꾸는 방법에 대해 학습하였습니다. 그런데 Word2Vec, GloVe, Fasttext 세 방법론이 크고 작은 차이점을 가지고 있지만 co-occurrence 정보를 보존한다는 점에서 기존 count 기반 방법론과 크게 다르지 않다는 점이 매우 흥미로웠습니다. 저도 hierarchical sampling과 negative sampling은 제대로 이해하지 못해서 앞으로 추가 학습할 계획입니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170312215700]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[722]]></uid>
		<content_uid><![CDATA[304]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[비디오 부분중에서 Dense trajectories and motion boundary descriptors에 대한 발표를 들었습니다. 이 부분은 ConvNet을 주로 활용하기 보다는 HoG, HoF등의 기존의 컴퓨터 비전에서 사용하던 기법들을 이용하여 영상에서의 dense trajectroies를 찾는 방법이었습니다.  cs231n강의에서는 깊게 설명을 안하고 넘어갔던 부분인데 상세히 설명을 해주셔서 많은 도움이 되었습니다. ConvNet이 좀 더 사용되는 Spatio와 Temporal 부분이 포함된 다음 세미나 발표가 기대 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170313103110]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[723]]></uid>
		<content_uid><![CDATA[301]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[이번 시간에서는 자연어처리에 대한 기본적인 내용과 SVD, Word2vec에 대한 세미나를 들었습니다. 수업시간에만 Document Representation에 대해서 배우고 그동안 많이 활용할 기회가 없어서 잘 몰랐었는데 이번시간을 통해서 다시 복습할 수 있었고,  Word2vec 목적함수에 대해서 상세한 증명을 해주어서 word2vec에 대해서 많이 배울 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170313103839]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[724]]></uid>
		<content_uid><![CDATA[306]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이미 연구실 내에서 흔히 사용되고 있는 Word2Vec 방법에 더해 Glove와 FastText라는 새로운 단어 표현법에 대해서 학습하였다. 이번 세미나에서 가장 유익했던 점은 연구원들과의 활발한 토론을 통해서 얻었다. Word2Vec과 Glove의 메커니즘에서 볼 수 있는 유사점을 통해 두 방법론을 비교분석하였다. 원 강의에서는 언급되지 않았던 Glove의 문제점, 아주 큰 행렬을 필요로 한다는, 은 이 방법론이 실제 문제에서 사용되기 어려울 것 같다는 생각이 들게 하였다. 방법론의 성능뿐만 아니라 사용의 편리성 또한 알고리즘을 평가하는데 중요한 요소임을 다시 한 번 느꼈다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170313130059]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[725]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[이미지 분석을 넘어 비디오를 분석하는 세미나였는데, 아직 초기단계로 여러 시도가 있지만 성능이 좋지 않은 상황이라고 알고있습니다. 도전해서 괄목할만한 성과를 낼 수 있는 분야로 받아들여도 되지 않을까 싶은 생각이 들었습니다. 이미지 분석시 filter를 정의해서 stride하며 다음 단계의 feature map을 만들었는데, 비디오는 시간축까지 있어서 프레임 몇개를 한번에 봐야하는 상황이므로 filter를 새로 정의하는 모습을 봤습니다. 3D filter를 이용하는 것이 파라미터가 더 적은데도 사용하지 않았는데, 사실 명확히 이해되지 않는 부분입니다. 이미지 분석을 먼저 공부해보며 이해하고, 나중에 비디오도 공부해볼 영역이라고 생각합니다. cs231n 강의가 마지막에 수준이 너무 올라가서 약간 버거운 감이 있었는데, 그래도 끝까지 오면서 이번기회를 통해 이미지를 접할 수 있어서 너무 좋은 시간이었던 것 같습니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170319002604]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[726]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[픽셀의 방향성을 간략하게 표현할 수 있는 새로운 좌표계가 있으면 문제가 쉬워질 것 같다는 생각이 들었다. 각 프레임의 이미지를 픽셀값을 활용해서 벡터로 표현하지만 이 벡터는 우리가 원하는 방향성을 잘 나타내고 있다는 생각이 들진 않는다. 수학에서 쓰이는 벡터 필드의 개념을 어찌 요리조리 잘 접목시켜봐도 재미있는 결과가 나오지 않을까? 이번 강의에서 제일 인상 깊었던 것은 RNN을 CNN을 구성하는 컴포넌트로 결합시킨 연구를 소개하는 부분이었다. 여러가지 인공신경망의 구조를 잘 조합하는 것 만으로도 성능 향상을 꾀할 수 있다는 점이 흥미로웠다. 나도 번뜩이는 아이디어가 있어 저러한 것을 하면 좋지 않을까, 생각합니다. 그러기 위해선 여러가지 구조를 배워야 겠죠, 논문을 열심히 읽어야 겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170319005332]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[727]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[비디오 분석은 아직 main stream이 없는 분야같습니다. 지난번 yolo9000 영상을 보면서 실시간 detection이 가능한 모습을 보았는데 비디오 분석은 목적하는 바가 무엇인지 아직 명확하지 않은 느낌이었습니다. 이전 frame과 현재 frame, 미래 frame을 연결, 시간축을 추가한다는 아이디어는 좋았으나 그렇게 추가한 축으로써 동작을 이해하는 것이 어떤 application으로 발전할 수 있을지 의문입니다. 동작을 이해함으로써 captioning을 더욱 정확히 할 수 있지 않을까 싶으나 현재의 captioning 성능과 어떤 차이가 있을지도 궁금합니다. cs231n을 보면서 딥러닝을 통한 이미지, 영상 분야를 접해볼 수 있었고 학습해야 할 것이 많음을 느꼈습니다. 마지막 강의를 여러주에 걸쳐 준비한 발표자분께도 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170319025015]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[728]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[지역적 탐색을 할 수 있는 방법으로 기존 단일 이미지를 분석하는 방법과 동일한 single frame 외에 late, early, slow fusion에 대하여 알 수 있었습니다. 또한 이론적으로 시간 축을 추가하여 3D convolution이 존재하지만, 직관적으로 이해하기도 어렵고 그에따라 사용하기도 어려워 이미지 전체를 학습시키는 task와 t-1 frame과 t frame에서 움직임이 있는 부분을 학습시키는 task를 병렬적으로 처리하는 방식을 사용한 것이 인상적이었습니다. 시간축을 보다 직관적으로 인지하고 사용할 방법은 고민해 볼법한 문제인 것 같습니다. 가장 흥미로웠던 점은 conv net -&gt; LSTM의 sequential한 방법이 아니라 conv net을 RNN구조로 실행시키는 구조적인 통합(?)이었습니다. 앞으로 이러한 아이디어는 여러 부분에서 유용하게 사용될 수 있을 것 같다는 생각을 해보았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170319045854]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[729]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[개인적으로 Video(움직임)에 관하여서는 분석 해본 경험이 없어서  명확히 어떤 데이터로 어떤 전처리를 하는지를 명확하게 알아야 되겠다고 생각하였습니다. 움직임에 관한 데이터는 프레임단위로 혹은 특정 단위로 구할수 있을것인데 무식하게 이것을 전부 입력되는 이미지 단위로 구하는 것이 아니라 이것조차 예측할수 있는 특성 물체는 예측하는것이 옳겠다고 생각 하였습니다. 그리고 질문 하였던것 처럼 특정 장면의 변화에 대한 전처리로 한번 알아 봐야 되겠습니다. 
많은 경우가 time axis가 있을 경우 RNN을 많이 쓰는 구조인데  개인적으로 연구해보고자 하는 경우도 이 경우에 대하여 2가지 관점에서 예측하는경우이기 때문에 연구해 보려고 합니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170319112257]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[730]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[이미지 인식은 계속 발전해나가고 있지만 비디오 분석은 아직 주요 연구 분야로 떠오르진 않는 것 같습니다. 기존 이미지 분석에는 여러 필터를 사용하여 feature map들을 사용했다면 비디오에서는 여러 프레임을 한번에 봐야하기 때문에 아예 새로운 filter를 정의한다는 것이 흥미로웠습니다.  초당 30프레임 이상, 러닝타임 2시간 이상의 동영상을 분석하려면 학습에도 다양한 종류의 많은 데이터가 필요할텐데, 굉장히 데이터 처리가 어려울 것이라는 생각이 들었습니다.  기존에 있던 신경망 구조를 통합시켜 효율적인 새로운 구조를 만들어내는 부분이 가장 흥미로웠습니다. CNN으로 각 프레임을 분석한 후 RNN으로 연결시키는 부분이 특히 기억에 남습니다. 
세미나를 들을수록 공부해야할 부분이 끝없이 쌓여나가는 느낌입니다.이미지 인식 분야 공부가 특히 많이 부족하다고 느끼고 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170319125048]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[731]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[지난 세미나에서 영화 '007'을 detection하는 동영상을 보았을 때, 비디오 분석도 상당한 수준의 발전이 이루어 졌다고 생각했었습니다. 하지만 이번 세미나를 통해 비디오 분석은 아직 초기단계에 머물러 있다는 것을 느꼈습니다. 아직 공부가 부족하여 명확하게 이해를 하지는 못했지만 이미지 분석을 할 때는 픽셀 값만을 고려하여 filter를 학습하는데 비해 비디오 분석은 시간까지 고려해야 하기 때문에 여러 프레임의 이미지를 한번에 볼수 있는 filter를 학습해야 하고, 때문에 이미지 픽셀 값 뿐만 아니라 방향성까지 고려해야 한다는 점이 흥미로웠습니다. 우선 이미지 분석에 대한 공부를 먼저 끝내고 비디오 분석을 공부하면서  시간축에 대한 문제를 어떻게 해결할 수 있을까에 대해 많은 논문을 찾아볼 계획입니다. 그러나 아직까지는 비디오 분석의 명확한 방향성이 확립되지는 않은 것 같아 보입니다. 따라서 방법론에 대한 개발과는 별개로 현업에 적용 할 시에는 목적에(ex : 물체 탐지) 맞게 이미지 분석의 기술을 적절히 응용하는 것이 아직까지는 더 바람직 하다고 생각됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170319141628]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[732]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[이미지 자체에 대해 분석하는 것도 까다롭고 어려운데 video를 시간에 따라 방대한 양의 이미지로 바꿔 분석하는 것도 굉장히 어려울 것 같다는 느낌을 받았습니다. Video를 분석할 때는 연속된 동작을 자르는 시간에 따라 분석하는 이미지가 바뀌니 어느 구간을 어느 정도의 interval로 분석하는 관건일 듯 합니다. 특히 long-time spatio에서 LSTM을 사용하여 한 구간을 분석하는데 한~참 앞 뒤의 event까지 고려할 수 있다는 것이 인상 깊었습니다. 그 원리가 아직 명확하게 이해되진 않지만 긴 시간적 인과관계를 포함한 스토리까지 반영하여 특정 장면을 탐지한다는 발상 자체는 굉장히 의미 있는 발상인 것 같습니다. 끝으로 방대한 양을 정리하여 마지막까지 수고한 발표자에게 박수를 보냅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170319215428]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[733]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[비디오 분석은 프레임의 모임이니까 각 프레임을 잘 분석하면 비디오 분석이 가능할 것이라고 생각했었는 데 세미나를 통해 완전히 잘못된 판단임을 알게 되었습니다. 비디오 분석에서의 성능은 시간을 어떻게 정의하느냐에 따라 많이 달라질 것 같습니다. late, early, slow fusion 등 시간 축 정보를 어떻게 반영하는 가에 대한 다양한 시도를 소개하였고, 공간 정보와 시간 정보를 별도로 학습하는 것에 대한 내용도 다뤘습니다. Convolutional Network가 Deep해질수록 원래 공간 정보를 잃어 버리는 게 되는 데 CNN으로 각 프레임을 분석한 뒤에 RNN 으로 연결시키는 부분이 정확하게 어떤 의미와 효과를 가지는 지에 대한 의문이 남습니다. CS231n 세미나 중 연구실 구성원 간의 토론으로 많이 배울 수 있었던 것 같습니다. 대단히 많은 내용임에도 마지막까지 성실히 준비해주신 발표자 분 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170320002350]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[734]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[시간적 가정을 고려한 video 분석방법에 대한 세미나를 들었습니다. 강의에서 소개된 논문들은 2014~2015에 출간된 논문이어서 이미지 분석에 비해서 좋은 성능을 나타내지는 못하였지만 CNN과 RNN 등의 다양한 딥러닝 기술을 적용시킨것이 흥미로웠습니다. 딥러닝 관련 연구결과물이 쏟아지는 많금 비디오 분야에서도 많은 발전이 진행된것으로 생각이 들고 이후의 논문들을 더 찾아봐야 겠다는 생각이 들었습니다. 지금까지 비디오 내용에서는 영상자체에 대해서만 분석하는 내용이었는데, 동영상에는 음성정보도 있으니 음성정보를 결합하여 해당 장면의 context를 분석하는데 도움이 될수 있다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170320141152]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[735]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 cs231n 세미나중 가장 어려웠던 세미나였던 것 같습니다. 기본적으로 개념이 너무 생소했기 때문에 그랬던 것도 있고 특히 우리가 실제로 CNN을 영상에 적용할 때, 영상 데이터 구성을 본 적이 없기 때문 인 것 같습니다. 처음에는 김창엽 박사과정처럼 영상은 이미지의 모음이기에 배치단위로 처리를 할 때, 이미지의 sequence를 고려하여 배치처리를 하면 될 것 같다는 단순한 생각이 틀렸음을 알았습니다. 개인적으로 해당 세미나에서 느낀 점은 비록 딥러닝의 대두로 비전관련 공부를 해본적이 없는 저도 비전분야의 최신 알고리즘을 이해할 수 있지만 알고리즘이 발전되어온 과정속에서 마지막에 딥러닝이 있는 것이기 때문에 "과거 어떠한 연구로 어떤 문제점을 해결하려고 했는 가?"를 파악하는 것이 중요하다는 것입니다. 때문에 Hog 또는 sift 같은 알고리즘도 알아야겠다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170320224801]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[736]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[최근 구글이 Fei-Fei Li와 함께 동영상 내의 객체 인식 기술을 개발했다고 합니다. 자세한 정보는 모르지만, 이런 선행 연구에 기반하여 새로운 모델을 만들었을 것입니다.
이미지는 분류와 객체 인식이 거의 끝장을 보다시피 하고 있습니다. 하지만 이번 강의를 통해, 동영상은 이미지 분류를 조금 붙여놓은 수준에 불과한 것 같다는 생각을 했습니다.
 이번 세미나를 통해 동영상 분석에는 2가지 핵심 요소가 있다는 생각을 했습니다. CNN으로 위치상의 Local connectivity에 시간상의 Local connectivity를 더하여 Feature를 추출하는 것, RNN으로 과거와 현재와 미래의 정보를 종합하는 것입니다.
 CNN으로부터 추출한 Feature가 작지 않기 때문에 RNN으로 연결할 시간이 길어진다면 계산상 구현이 불가능해질 수 있습니다. 그래서 현실적으로는 어느정도 성공한 사례들과 더불어 Fei-Fei Li가 이루어낸 동영상 검색 또한 의미 추출보다는 객체 인식정도인 것 같습니다.
하지만 현재 기술의 개선속도는 상상 이상이기에, 앞으로의 발전이 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170323143511]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[737]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[항상 가까이에 있었기에 소홀했던 Cross-entropy의 소중함을 일깨워준 발표였습니다. 제가 너무 Cross-entropy를 쉽게 사용하고 있었다는 생각이 들었습니다. 강의에서 부족했던 점을 스스로 발굴해서 보충한 발표자의 성실함에 칭찬을 하고 싶습니다. 다만 발표가 전반적으로 조금 빠르게 진행된다는 느낌도 받았습니다. 열심히 준비한 내용이 휙휙 지나가니 보는 저도 아깝다는 느낌이 들더군요. 강의 내용이 적어서 시간 부담이 덜했던 만큼 친절히 하나하나 짚어가며 발표를 진행했어도 괜찮았을 것 같다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170325003433]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[738]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[정말 당연하게 분류 문제에서는 softmax crossentropy를 사용했는데, 그 속의 내용이 역전파에서 gradient vanishing과 관련한 문제라는 것을 명확히 알 수 있는 발표였습니다. 하나하나의 이런 연구자들의 노고가 녹아있는것이 딥러닝이 아닐까하는 생각이 들었습니다. window 사이즈에 따른 효과도 확인하였는데, 짧을수록 구조적인 특징을, 길수록 의미적인 특징을 잡아낸다는 것을 순식간에 설명하고 넘어갔는데, 그 의미를 조금 더 곱씹어보면 word2vec같은 임베딩 방법론을 만들수 있는 기반이 되지 않을까 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170325004802]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[739]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[cross-entropy는 평가지표로서 다양하고 많이 사용하는 것 같습니다. Co-training의 레이블을 넘겨주는 보조적인 수단을 cross-entropy를 사용했던것이 기억이 납니다. 또한 정보이론이란 채널들에서 엔트로피란 용어들이 많이 사용되는 것으로 알고 있고 물리학에서도 항상 모든것은 자연스러운 쪽으로 흘러간다는 점에서 엔트로피는 항상 높은 쪽에서 낮은쪽으로 흐르는 것과 같습니다. 엄밀히 물리학에서 엔트로피를 에너지와 관계시키기 때문에 에너지가 일정해지면 엔트로피는 낮아지면서 자유도는 올라가고, 엔트로피가 일정하면 에너지는 높아져 자유도가 올라가는 관계를 가지고 있습니다. 이러한 부분은 저희가 배우는 쪽과 거리가 있지만 과학적이 사고방식은 항상 자연현상에서 찾으려는 경향이 있는 것 같습니다. 유전알고리즘이나, 인공신경망, 엔트로피, 텐서등 여러이론들이 자연현상을 빗대어 발견되는 것같습니다. 그렇기 때문에 저또 해외 여행등을 다니면서 뭔가 신비한 자연현상들을 탐색할 좋은 기회를 가지길 기대해 봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170325054942]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[740]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[왜 MSE대신에 Cross-Entropy를 사용하는지에 대하여 언급하였고 전반적으로 이전에 세미나에서 다뤘던 발표로 진행을 하였습니다. 1Norm과의 비교가 있었으면 더 재미있었겠다는 생각을하였습니다. 세미나 중간에 언급되었던 Word Embedding을 Input으로 놓고 local gradient자체를 Embedding공간의 word 데이터까지 하자는 것을 예전에 생각해본적이 있는데 해당 테스크를 사용한다는 관점에서는 좋을것 같습니다. 쉽게 예를들어 semi supervised learning을 한다고 가정한다면 word embedding공간을 전체로 만들고 fine tuning할 부분만 다시 update한다던지 기존의 Image에서 transfer leaning의 개념처럼 큰 corpus에서의 embedding 차원을 가져다가 쓰고 local gradient를 써서 fine tuning한다는 개념으로 접근한다면 특정 task에서 더 좋은 결과가 나올꺼라고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170325174359]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[741]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[NLP에서 extrinsic task (NER ) 등에서 앞선 챕터에서 word vector들을 어떤식으로 활용할 수 있는지 알 수 있는 시간이었습니다. 개인적으로 window 관점이 이미지에서의 convolution filter의 receptive filter 개념과 유사하게 느껴졌고 실제로 그런 유사점들 때문에 NLP에서 CNN 모델들을 많이 활용하는 것으로 생각했습니다. 해당 발표를 통해서 개인적으로 느꼈던 것은 사실 corpus data를 많이 확보해놓은 상태에서 해당 corpus에서 word vector를 word2vec, glove의 방식으로 얻어놓고 extrinsic task에서 word vector를 retraining 하는 것은 큰 문제가 안되지만 실제로 그러한 모델이 활용성이 있을 지 궁금한부분입니다. 왜냐하면 end 2 end로 learning이 불가능하기 때문입니다. 실제로 unseen word가 등장한다면 해당 word에 대해서 initial vector가 주어지지 않기때문입니다. end2end로 word2vector를 구할 수 있는 방법에 해서 찾아보아야하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170325182413]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[742]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[이번 시간에는 Word vector를 활용하여 다른 태스트를 수행하는 Extrinsic task와   MSE대신에 Cross Entropy를 사용하는 지에 대한 이유, Window Classification에 대해서 다루었습니다. 발표자의 설명 중에 Window 사이즈가 작아지면 구조적인 특징을 잘 반영하고 Window 사이즈가 커지면 문맥적인 의미를 잘 반영한다는 것에 대해 다시 생각해 볼 수 있는 기회였던 것 같습니다. 악성코드를 스트링 기반으로 분류할 때 어셈블러의 순서를 나열하여, 단순히 n-gram에서의 bigram, trigram이 Feature를 뽑기 위한 수단으로만 생각했었는데 중심 워드의 앞 뒤를 살펴 보는 Window size가  분류 모델 성능에 어떤 영향을 미칠 수 있는지에 대해 더 고민해봐야 겠다는 생각이 들었습니다. 발표 준비하시느라 고생많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170326142245]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[743]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[회귀 문제에서는 MSE를 분류문제에서 Cross Entropy를 쓰는 것은 너무 당연한 일이라고 여기고 있습니다. 그리고 gradient 전파에 있어서 둘이 어떤 차이가 있는지를 살펴보았습니다. window size가 작은것과 큰 것의 차이를 구조적인 특징과 의미적인 특징을 잡아낼 수 있는 것이라는 내용도 다루었습니다. 사실 word2vec은 결과를 보았을 때 단어간의 의미관계를 보존하는 것처럼 보이지만, 같은 공간에 임베딩 된 결과들을 보면 문법적으로 비슷한 위치에 있는 것들이 같은 공간에 있는 것을 확인할 수 있었습니다. 그렇기에 의미관계를 보존하는 것은 사실상 단어들의 구조관계가 더 명확히 보여질 수 있는 것이 word2vec이 아닌가 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170326151140]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[744]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[Word vector를 사용하여 Extrinsic task를 수행하는 방법, Cross Entropy의 장점, Window classification 그리고 Neural Network에 대해 다루었습니다.  그 중에서 가장 흥미로웠던 부분은 Word vector에서 작은 window size를 사용하면 구조적 의미를 가지고 큰 window size를 사용하면 문맥적의미를 더 확보할 수 있다는  것이었습니다. 실제 데이터에 window size를 변경하며 적용하였을 때 얼마나 모델 성능에 영향을 미치는지 조금 더 확인해보고 싶었습니다. 
세미나 발표 후 Cross Entropy라는 개념이 97년에 희소 사건의 확률을 구하기 위한 방법으로 처음 나왔는데, 지금은 머신러닝에서 널리 쓰인다는 것도 알게 되었습니다. 20년 전에 나왔던 개념이 지금도 유용하게 쓰이는 것을 보며 머신 러닝 분야 뿐만 아니라 조금이라도 연관 있는 분야에 대한 내공이 매우 중요하다는 것을 새삼 느낄 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170326155941]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[745]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[NLP task에서 retrain을 이용하여 good에 대한 가장 유사한 단어로 bad가 나오던 기존의 방식에서 nice, terrific과 같이 반대의 뜻을 가지는 단어를 제외할 수 있다는 논문을 읽은적이 있습니다. 이번 세미나에서 retrain에 대하여 다시 볼 수 있어서 좋았습니다. NN구조에서 cross entropy function가 MSE보다 왜 유리한지에 대하여 발표자의 수고 덕에 쉽게 알 수 있었습니다. 또한 윈도우 사이즈가 커질수록 semantic 추론이 syntactic추론 보다 유리하다는 것또한 흥미로웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170326192541]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[750]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[지난 Word2Vec과 Label propagation을 이용하여 Sentimental lexicon을 만드는 연구와 삼성전자의 Bin2Vec 연구에서 모두 Word embedding 기법을 이용하였습니다. 그런데 Embedding된 word가 얼마나 잘 embedding 되었는지 파악하는 일은 정말 어려웠습니다. 지난 시간의 Intrinsic과 이번 lecture의 Extrinsic evaluation은 모두 떠올리기 어렵지 않은 방법입니다. 하지만 이 Task들은 모두 한계가 있으며, 이를 해결하기 위한 좋은 Intrinsic evaluation 방법을 만드는 것도 좋은 연구[지난번에 준홍이형이 얘기했던 Loss를 기준으로 삼는 것 등]가 될 수 있겠다는 생각이 들었습니다.
Window classification은 최근에 CNN을 이용한 Text 분석과 같은 task에 자주 사용되고 있지 않나 싶습니다. 그만큼 단순하지만 Context를 고려하는 강력한 방법이라고 생각했습니다.
MSE와 Cross entropy의 장단을 강조한 것이 좋았습니다. 지난 CS231n의 Lecture note에 Regression Task도 Classification으로 바꾸어 진행할 수 있다면[예를 들어 1~5점의 별점 예측은 Regression보다는 각 점수를 classification으로 예측] Classification으로 진행하는 것이 좋다는 이야기를 했습니다. 이와 상통한 주제로 직관적으로 다가왔습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170326194102]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[751]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[741]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[최근 본 논문 CNN for sentence classification(Yoon Kim, 2014)에서는 형이 마지막에 제기한 내용을 다루기도 하는데요, wrod2vec등으로 미리 학습된 단어가 아닌 단어에 대해 randomly initialize한 뒤에 retrain을 전반적으로 하는 형태를 가졌고, 성능도 괜찮다고 소개하고 있었습니다. 이상 설명충 스피드덕 이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170326222013]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[752]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[지금까지 classification 문제에 있어서 아무 생각 없이 cross entropy를 사용했고 한번도 그 이유에 대해서 생각해 본 적이 없었는데 이번 세미나를 통해 cross entropy의 장점에 대해 알 수 있어서 좋았습니다. 무엇보다 강의에서 MSE보다 cross entropy를 더 많이 사용하는 이유에 대해서 두리뭉실하게 넘어갔는데 발표자께서 그 부분에 대해 자세하게 조사해오셔서 더 많이 알 수 있었습니다. 또한 window classification에서 window size가 커질수록 문맥적 의미를, window size가 작을수록 구조적 의미를 반영한다는 것이 매우 신기했습니다. 파이썬 코딩이 좀 더 익숙해지면 한번 직접 구현해보고 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170326230450]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[753]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Word embedding 된 결과를 활용하기 위한 extrinsic task, softmax classification, window classification에 대한 발표를 들었습니다.  Neural network에서 cost function으로 MSE, 활성함수로 Sigmoid 함수를 사용하는 경우 학습속도가 저하되는 문제가 발생하는데 정보이론에서 쓰이는 엔트로피 개념이 cost function 성질과 유사하여 cost fucntion으로 사용할 수 있고, 이를 이용하여 학습속도 저하 문제를 해결할 수 있습니다.  한 단어가 희귀하거나 다의어로 뜻이 애매한 경우에 window classification을 사용한다고 하였는데 직관적으로 이해가 되지 않아서 다시 공부를 해볼 생각입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170327134342]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[754]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[목적식에 내가 가지고 있는 데이터 뿐만 아니라 그것을 Corrupt 시켜서 산출한 에러를 추가한 아이디어가 재미있다고 생각했다. CS 231n 강의에서 소개해준 Data augmentation기법의 텍스트 버전인 것 같다. Richard가 이 부분에 대해서 개별적으로 추가적인 내용을 설멸해 주었으면 하는 아쉬움이 남는다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170331234317]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[755]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[역전파 알고리즘을 되새겨보는 세미나였습니다. 기존에 cs231n에서 했던 computation graph 방식으로 이해하는 것도 좋지만 computational graph로는 알 수 없는 것을 수식에서 알 수 있기에 이런 방식으로 재환기하게되어 좋은 세미나였다고 생각합니다. corrupt winodw를  통해서 text 분야에서도 Data augmentation issue가 중요하다는 것을 또한 알수가 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170402005924]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[756]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[목적함수(Loss Function)를 Max margin 형태로 정의하고 맞추고자 하는 워드 벡터의 스코어는 최대화,  Corrupt  워드에 대한 스코어는 최소화하는 것으로 정의한 것 외에는 역전파 알고리즘에 대한 내용이었습니다. CS231n 과 비교했을 때 확실히 수식 보다는 연산 그래프 형태로 보내는 게 편하다(..)라는 생각이 들었습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170402132012]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[757]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[역전파 알고리즘을 학부생 이후 오랜만에 제대로 다시 공부할 수 있는 좋은 시간이었습니다. 역전파에 대해 잘 알고 있다고 생각했는데 막상 공부를 하고보니 이런 저런 부분에서 여러 부족한 곳들이 보여 복습할 수 있었습니다. 
중간에 Corrupt window라는 개념을 도입하여 목적식을 수정하는 방법이 가장 흥미로웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170402174501]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[758]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[딥러닝에서 뺴놓을 수 없는 역전파에 대해 알아보는 시간이었습니다. 다시 한번 chain rule을 이용하여 back propagation하는 방법을 복습할 수 있는 시간이었고 한 가지 새로웠던 점은 기존과 다르게 wrong class에 대해 corrupt score를 부여하여 목적식에 추가한 것이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170402232853]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[759]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[이번 세미나는 역전파에 대해 보았습니다. cs231n에서 그래프 형태로 설명했을 때는 쉽게 이해가 됬었는데 강의에서 이를 수식적으로 보이는 것이 잘 이해가 되지 않았었는데 이번 세미나를 통해 부족한 부분을 보완할 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170403005235]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[760]]></uid>
		<content_uid><![CDATA[308]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[지난 세미나에 이어 매우 이해하기 어려운 강의를 최대한 쉽고 직관적으로 설명해주신 점이 인상 깊었습니다. 저 같은 경우에 텍스트 처리에 관심이 많은데 텍스트도 영상처럼 입력값의 방향성을 고려하면 좀 더 성능이 나아지지 않을까하는 느낌도 들었습니다. 현재는 우리 연구실이 텍스트와 이미지 연구를 하고 있지만 향후 연구 지평을 넓힌다는 점에서 의미있는 세미나 시간이었던 것 같습니다. 세미나 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170403093210]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[761]]></uid>
		<content_uid><![CDATA[310]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[지나치기 쉬운 주제이지만 기본에 충실한 발표였습니다. 저는 요즘에 빈도frequency가 의미semantics로 바뀌는 과정 자체에 큰 흥미를 느끼고 있는데요, window나 cross entropy 개념 모두 자주 같이 등장하는 단어들의 정보를 보존한다는 점에서 일맥상통하는 것 같습니다. 언어학에서는 형태소를 분석할 때 계열관계와 통합관계를 따지는데요, 이것은 같은 문맥 속에서 다른 단어로 대치될 수 있는가 혹은 다른 단어가 더 들어와도 되는가 등을 따지는 분석 방법입니다, 제가 보기엔 이런 분석 방법이 co-ouccerence와 직접적으로 닿아있다는 생각도 드는데요, 한낱 숫자(빈도)에 불과한 정보들이 모여 어떻게 의미로 변화할 수 있는지 좀 더 고민하고 연구해보도록 하겠습니다. 발표 준비하시느라 고생많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170403094456]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[762]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[역전파는 아무리 다시 봐도 쉽지 않은 내용인 것 같습니다. 네트워크 구조가 복잡해질수록 더 그런 것 같은데요. 하지만 향후 패키지나 라이브러리 도움 없이도 구현해야 하는 순간이 올지도 모르기(정말?) 때문에 늘 기본에 충실해야겠다는 다짐을 하게 됐습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170403094748]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[763]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[역전파에 대하여 발표하였습니다. 준비한 방식의 핵심은 델타룰로 묶인 체인룰이었습니다. 다음에 역전파를 다시하게 된다면 cnn기준에서도 역전파가 어떻게 상세히 일어나는지 해주면 더 좋을거 같습니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170405094954]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[764]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[back propagation은 neural network가 발전할 수 있게 해준 내용으로 그 과정이 어떻게 이루어지는지 이해하는 것이 중요하다고 생각합니다.  chain rule과 미분을 이용한 간단한 개념이지만 그 수식적 결합이 의미하는 바가 무엇인지 곱씹으면서 학습해야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170406145833]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[765]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[(회사에 다녀오는 관계로 참석하지 못했습니다. 녹화본이 올라오거든 듣고 올리겠습니다.)]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170409144237]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[766]]></uid>
		<content_uid><![CDATA[313]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[인공신경망에서 필수적인 Backpropagation을 다루었습니다. 미분 notation도 좋지만, 역시 직관적인 이해에 있어서는 CS231n의 computational graph이 혁명적이지 않았나 하는 생각도 들었습니다.
그리고 이번 세미나 시간에 중간에 헤매였던 곳은 notation이 헷갈렸던 것으로 보입니다. 이 부분은 PPT의 대소문자가 계속 달라졌던 원인도 있었던 것으로 보입니다.
그리고 이를 matrix notation으로 바꾸면 더 쉽게 이해할 수 있지 않을까 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170411204452]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[767]]></uid>
		<content_uid><![CDATA[318]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[민식이의 연구활동과 무엇을 공부 하고 있는지 발표하였습니다. 
공부한 논문을 이해한 전체적인 플로우를 보자면 
(1) Voice data를 Image data로 변환
(2) Image data를 통하여 Hypothesis 학습(based on GAN)
(3) Hypothesis를 통하여 Voice generate
(4) Generate한 결과를 통하여 기존 방법과 likert척도를 통한 비교를 하였습니다 .개인적으로 Likert척도에서 3점대가 나왔다는 것은 좋다고 할 수 없다고 알고 있습니다. 사람의 심리상태에서 5점척도의  likert에서 3점은 거의 보통이라고 집을수 있고 좋다면 상당히 양극단으로 치우친다고 알고 있습니다. 개인적으로 GAN의 결과가 상당히 image에서는 blurry한 결과가 나오는데 voice 분야에서 counter part방법론보다 좋다는 결론이 실제로 듣고 싶어서 찾아봤는데 잘 못찾겠습니다. 이에대한 영상이나 음성이 있다면 알려주면 고맙겠습니다.
그리고 deconv를 사용하여서 image를 뽑는다고 하였는데 deconv가 왜 필요한지에 대하여 아직 개념이 확실하게 잡히지 않았습니다.
결론적으로 전체적인 application 관점에서 보자면 '소리를 image로 만들수 있고 이것으로 무엇을 할것인가?' 라는 질문으로 세미나 시간에 의논한것 같이 style을 입힌다거나  retrieval관점에서 빠른 search가 가능하겠다 정도로 생각했습니다. 
제가 개인적으로 예전부터 음악에 관심이 많은데 application  관점에서 만약에 drum beat를 새로 만든 (snare가 음이 다른)것들을 sample로 올려놓고 파는 사이트가 많습니다. 음악 작곡 관점에서 보면 이런 음에 대한 기저가 되는 것들을 학습데이터를 통하여 무한정 생성 할 수 있을것 같다는 관점에서 재미있는 놀이감(?) 이 될 수 있을것 같습니다. 쉽게 이야기 해서 DJ들이 dj controllers로 하는것들은 정해진 상황에서 가지고 노는거지만 이것을 새로운 소리로 만들 수 있따는 관점에서는 재미있을것 같습니다. 개인적으로는 새로운 주제를 듣고 생각해본 시간이라 재미있었습니다.
앞으로 제가 알지 못하는 주제인 해당 분야에서 재미있는것을 많이 보았으면 좋겠습니다. 
화이팅 입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170415223148]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[768]]></uid>
		<content_uid><![CDATA[318]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[딥러닝 프레임워크가 주로 적용되는 이미지와 텍스트가 아닌 소리 데이터를 사용하는 연구라는 점이 매우 흥미롭다. 다른 영역에 비해 탐색이 덜 이뤄진 만큼 본인 스스로 겪어야할 시행착오가 많을 것이라 생각된다. 앞으로 좋은 결과가 있기를 바랍니다. 한 가지 의문이 드는 점은 음악 데이터를 Spectrogram이라는 이미지 형태로 밖에 사용할 수 없냐는 것이다. 음파라는 파동 데이터를 사전에 처리하는 작업이 많으면 많아질수록 비전공자인 연구자 본인에게는 불리한 점이 될 수도 있다고 생각한다. 딥러닝의 묘미를 살려 소리 데이터의 처리를 최소화 하고 특성을 발견할 수 있는 방식은 없을지 궁금합니다. ^.^]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170416232131]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[769]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[이전에 cs231n에서 했던 내용이 많이 겹쳐지는 세미나였습니다. 세미나에서 나온 내용같이 RNN의 LSTM strcuture자체가 gradient문제를 해결하는 것이라고 많이 알려져 있는데 다음 세미나 준비하시는분에 알려 주시면 감사하겠습니다. 그리고 실제 상황에서는 train/test가 많이 균등하게 성능이 차이가 나는 경우가 있는데 이 경우에 대하여서는 개인적으로 데이터가 모자라거나 sampling이 잘못되어있을경우라고 알고 있습니다. overfitting이라는 machine learning에서의 큰 틀자체가 우리가 배운 상황에서는
https://kr.mathworks.com/help/nnet/ug/improve-neural-network-generalization-and-avoid-overfitting.html?requestedDomain=www.mathworks.com
위와 같은 방법들이 있지만 이부분이 항상 안맞는 경우가 맞아서 사람들이 기존의 방법론으로 과연 neuralnet을 해석하는것이 맞는가?라는 질문을 많이 하는것을 볼 수 있습니다. 예전에 황상흠박사님이 언급하셨던 내용으로 기억합니다. 
Batch Normalization을 사용할때도 EMA parameter자체가 (hyper-parameter)아니고 처음에 상당히 큰 영향을 끼치기 때문에 이부분에 대하여 converge되어있는상황에서 training하는 경우가 많이 있습니다. 이러한 technical report형식의 내용을 많이 다루었으면 좋겠습니다. 그리고 adam의 경우도 이와 같은 상황에서 correction value자체로 decay를 하여야 한다는 주장도 많은데 앞으로 실 상황에서 실험을 많이 해봐야 되겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170417133509]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[770]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[RNN을 시작하기에 앞서 기본적인 신공망에 대하여 알아보는 시간이었습니다. cs231n에서부터 많이 다루었던 역전파에서의 연쇄 법칙(chain rule)기법과 비선형 함수들, weights에 초기값을 부여하는 방식, 활성화 함수에 따라 달라지는 파라미터 초기화 방식들, 지속적으로 발전해온 최적화 방식들에 대하여 다시 한 번 배울 수 있어서 좋은 시간이었습니다. 발표의 마지막 부분에 있는 과적합에 대해서 개인적으로는 신경망의 과적합 문제가 다른 machine learning 방법론에서의 과적합과 같은 의미를 지니는가에 대한 의문이 있으나 해결하지 못하고있습니다;; 혹시 알려주실 천사같은 분이 계시지 않을까요?]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170417135137]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[771]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[신경망 구조에서 이해해야 되는 부분을 전반적으로 다룬 세미나였습니다. 구조를 짤때, 미리 진행되어야 하는 사항들에 대해 list up 하고, 그 세부 내용들을 장단점 및 많이 쓰이는 것 등을 설명하였습니다. 비선형 함수의 종류와, weight 초기화 방법론, 최적화 방법론등을 훑어보는 시간이었습니다. 또한 중간에 LSTM에서는 왜 ReLU를 사용하지 않는가 라는 주제로 잠시 얘기가 나오기도 했는데, 이는 연구실 내에서도 의견이 분분하고, 인터넷 상에서도 여러 의견이 있는데, 모든 의견이 각자 타당한 점을 가지고 있다고 생각합니다. 논리적으로 타당하다는 이유와, gradient exploding 관련 이슈, 혹은 수렴성 등이 주된 내용입니다. 현재 tensorflow를 사용하게 되며 이런 기초적인 부분에 소홀해 지기 쉬운데, 앞으로도 틈틈히 리뷰를 통해 그 기저를 잊지 않도록 노력해야겠습니다. 다음 세미나부터는 RNN에 대한 설명이 이어지는데, cs231n에서의 RNN 내용에 추가로 얻는것이 있기를 기대합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170417150539]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[772]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[딥러닝의 기초 부분은 볼 때마다 헷갈리는 부분이 조금씩은 있는것 같다. 잊을 법하면 이렇게 복습을 할 수 있다는 것은 아주 좋은 기회이다. RNN의 편미분식이 Backprogation through time을 통해 얻은 것이 소개되었다. 예전부터 이 수식이 어떻게 유도되는지 궁금하였고, 본인 스스로 시도해본 적이 있으나 제대로 해내지 못했었다. 그래서 이 편미분식의 도출과정이 소개 되었으면 참 속 시원한 발표였을 거라는 개인적인 아쉬움이 남는다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170417172920]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[773]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[Backpropagation은 다 안다고 생각했는데도 다시 보면 조금씩 헷갈리는 부분이 있는 것 같습니다. Backpropagation 외에도 초기 weight 설정, Optimization tricks,  Prevent Overfitting에 관한 설명도 매우 좋았습니다. 교수님이 마지막에 Non-linear Optimization에 관한 강의는 꼭 수강하라는 말씀을 하셨는데 쟈코비언이나 모멘텀 등 확실히 이쪽 용어를 제대로 알고있으면 이해하기가 훨씬 더 쉬웠을 것 같습니다. 시간을 내서 조금씩 틈틈히 공부할 계획입니다. 단순히 딥러닝 기법만을 공부하기보다는 이 기법에 적용되는 많은 개념들이 수학이나 최적화 등 기타 여러분야의 지식들이 결합된 것이라는걸 다시한번 꺠달을 수 있는 좋은 시간이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170417220640]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[775]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[Chain rule, Non-linear Function, Weight Initialization 등 딥러닝의 기초 부분에 대해 전반적으로 복습할 수 있는 좋은 기회 였습니다. 많은 애플리케이션과 관련 논문들이 쏟아져 나오고 있는 데, 연구하는 입장에서 그 기저에 있는 이론을 이해하고 결과를 더 개선시킬 수 있는 포인트를 찾는 안목을 어떻게 기를 수 있을까라는 생각을 많이 하게되는 세미나 였습니다. LSTM에 대해 설명하는 부분에서 하이퍼볼릭 탄젠트를 왜 쓰는가에 대한 의문이 있었는 데, 다양한 의견을 들어보면서 도움이 많이 되었던 것 같습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170417224422]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[776]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[CS231n에서 했던 이야기지만, 역시 사람은 망각의 동물인 것 같습니다. Initialization, Backprop, SGD trick 등 이미 한번 공부했던 내용이지만 새로 듣는 것처럼 들었습니다. 세미나 시간 도중 이야기했던 RNN계열의 ReLU 이야기는 참 어려운 것 같습니다. 이 뿐만 아니라, 신경망 자체의 내부 기작을 파악하는 것이 워낙 어려운 일이다 보니 어떤 옵션이 더 좋은 선택이냐를 분석하기 어렵다고 생각합니다. 
사실 논문에서 주장하는 것도 "이런 상황이 있길래 이렇게 바꿨더니 성능이 더 좋더라"의 형태로 분석하는 경우가 많다보니, 그럴듯하고 성능이 좋아보일지언정 이를 명확하게 결론을 내리는 일은 참 힘들지 않나라는 생가을 했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170419114317]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[777]]></uid>
		<content_uid><![CDATA[319]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[수학에 깊은 조예가 있어야 딥러닝을 제대로 이해할 수 있겠다는 생각이 듭니다. Gradient 관리나 파라메터 초기값을 어떻게 주어야 하는지 등 학습 스킬 문제가 실은 수학의 제문제와 큰 연관이 있다는 생각이 들어서입니다. 아직까지는 딥러닝에 대해 초보적 이해 수준에 머물고 있는데 여러 방면의 공부를 통해 그 수준을 높여 가겠습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170419165805]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[778]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[RNN이 생기게된 배경을 자코비안과 부등식을 이용한 upperbound를 통해 확인하고, 그 구조를 복습하는 시간이 되었습니다. 이것만으로는 exploding은 사실 완벽히 이해되지 않았는데, 조금 고민해보려 합니다. 이번 세미나 발표에서 궁금증이 들었던 것은 ___(밑줄)을 채우는 문제에 대해서 사람들은 쉽게 맥락 정보를 통해 채울 수 있다는 점인데, 그게 사람은 왜 가능한지 생각해보면 여러가지 작용의 결과로 나온 것이겠지만, 그저 history라는 이름으로 포장할 것이 아니라, 추가로 NER 정보까지 input으로 준다면 더욱 generate하는데 용이할 것이라고 생각했습니다. 물론, 이것도 아이디어를 그냥 던지는 것입니다...ㅎㅎ 생각해볼 문제가 아닐까 세미나를 들으며 떠올려봤습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170513221104]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[779]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[개인적으로 관심이 있었던 bi-directional RNN에 관한 설명을 들을 수 있었서 좋았습니다. 학습과정에 다뤄지지않아 조금 아쉬웠으며, 개인적으로 흥미로웠던 아이디어는 hidden state에서 hidden state로 넘길 때, weight initialization을 identity matrix로 한다는 점이었는데 이는 내 직전 sequence에서 정보를 그대로 전달하겠다라는 의미에서 굉장히 직관적으로 좋은 initialization 방법 같습니다. 비록 수학적으로 증명이 되어있지는 않은 사안이지만 위와 같이 model에 대한 이해를 바탕으로한 직관을 적용해보는 것도 좋은 연구의 방향이 아닐까 생각됩니다. 새로운 개념을 창안하고 수식으로 증명하여 실험해보는 것이 제일 좋은 방향이겠지만, 향후 연구를 함에있어서 직관이 떠오르면 바로 try and error를 거치는 과정을 가지는 자세를 지니도록 해야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170514145015]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[780]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[RNN을 개념적인 측면에서 뿐만 아니라 수식을 이용해서 자세히 설명해준 점이 복습을 하는데 큰 도움이 되었습니다. 흥미롭게 봤던 부분은 [2]번 정리에 해당하는 내용입니다. Gradient를 Cliping이 제가 예전에 발표하였던 Wasserstein GAN과 유사한 점이 있습니다. 이때도 Cliping을 하되, 얼마만큼을 해야할 것인지 하는 문제가 있었습니다. 소개해준 논문에서도 Hueristic 솔루션을 제안하였는데, 이런 상황에서 일반적으로 Cliping할 범위를 찾을 수 있는 프레임워크도 재미있는 연구 주제가 아닐까 하는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170514192746]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[781]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[이번시간에는 Language model 관점에서의 RNN을 학습하는 방법에 대해서 배웠습니다. Backpropagation 과정이 수식으로 어떻게 전달되는지 알아보고 거리가 길 경우 Gradient Vanishing/explosion이 생기는 문제에 대해서 상기해볼 수 있었습니다. Bi-directional RNN은 이번에 처음 보게 되었는데 자세한 설명이 있었으면 좋았을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515090921]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[782]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[기계 번역 과정에서 기존 방법의 문제점과 RNN을 이용하여 번역에 적용한 사례에 대한 발표를 들었습니다. Gated Recurrent Unit에 대해서 처음으로 세미나 시간에 다루었는데 Backpropagation 과정에 대한 자세한 절차를 설명해주셔서 내용을 이해하는데 도움이 많이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515110525]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[783]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[GRU의 내부 기작과 기계 번역의 Scheme을 개략적으로 알 수 있었습니다. 다만, 세미나 시간에도 이야기했다시피 어떤 형태의 input을 받아 어떤 output을 내고, 이를 이용하여 어떻게 Loss를 산출하는 지 이해하기 어려웠습니다. 처음에는 Embedding 뒤 이를 input과 output으로 사용하지 않을까 싶었지만, 일반적으로 사용하는 방법은 따로 있지 않을까 하는 생각이 들었습니다.
GRU의 Forward 및 backward를 준비하는 데에 수고 많이하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515113625]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[784]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[RNN의 개괄에 대해서 알 수 있었습니다. Vanishing gradient 중, 기초적인 RNN은 vanishing이 발생하지만 LSTM이나 GRU는 이를 해소했다고들 이야기합니다. 이 또한 준비해준다면 더 좋은 발표가 되지 않을까 싶습니다. 다음 시간의 Bidirectional RNN도 기대하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515113832]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[785]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[GRU의 LSTM의 차이에 대해서 알 수 있는 세미나였습니다. GRU를 LSTM simpler라고 보는 관점에 대해 동의하며, 개인적으로 기계번역분야에서 과연 정말로 seq2seq loss를 word 단위로 계산하는 지에 대한 의문이 들었습니다. 예를 들어 한국어를 영어로 번역할 경우 seq2seq loss를 계산하기위핸 word의 one hot encoding의 차원수가 10만차원 이상이 될 것이라고 생각했기때문입니다. 물론 실제 기계번역을 학습하는 데에 있어서 연구실의 환경과 같이 데스크탑 한 대에서 학습하진 않을 것이기 때문에, 실제로 위의 seq2seq loss를 그대로 활용할 것 같긴하지만 개인적으로 실제 구현상에서 궁금한 부분이있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515123422]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[786]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[먼저, 세미나 준비를 공들여 했음이 느껴지는 발표였습니다. 특히 뒷파트에서 GRU의 구조 및 흐름을 파악하는 부분 뿐만 아니라, BPTT를 설명하는 부분이 정말 좋았습니다. seq.로 들어가는 데이터로부터 여러 loss가 연속적으로 나오는데, 그것을 어떻게 병합해야하는지, 그리고 그 수식은 왜 *이 아니라 +으로 되어 있는지에 대한 얘기를 나눴습니다. 앞으로 공부하는데 많은 도움이 될거 같았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515141940]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[787]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[해동이의 GRU 가 메인으로 된 세미나 였습니다. 가장 하이라이트였던 computation graph기반의 backpropgation은 이전에 RNN에서 기창형님이 해준 방식처럼 사용자가 알아듣기에 아주 좋은 형태였다고 생각합니다.  RNN의 그림으로 사용한 그림들로 설명한 방식들은 정말 많지만 이런식으로 설명한 자료는 많이 없을거라고 생각합니다. 그리고 중간에 이야기가 나왔던 output structure에 대한 고민은 상당히 해봐야할것같습니다. 만약 output node가 10만개 이상이라면 저희가 가진 개인 PC로는 어림없는 크기가 될것 같습니다. 그리고 본인이 공부하면서 이해가 안되는 부분에 대하여 하나하나 차근차근 설명해 주어서 참 좋았던것 같습니다 :) 개인적으로 RNN에 대하여서는 적용 분야에 대하여 생각해보고 있는데 새로 시작하는 연구분야에 적용하면서 차근차근 되세겨 가면서 공부해볼 작정입니다. 고생하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515202641]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[788]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[다음 시간에 다를 bi directional RNN의 경우 개인적으로는 RNN의 경우 Memory의 cell state가 이를 다 표현해야 되기 때문에 심각하게 오버피팅된 자료가 될 상황이 생긴다고 생각합니다. 하지만 이를 보정해주고 다른 path를 열어줄것같은 느낌이 있는데 이에 대하여 다음 시간에 기대하도록 하겠습니다. 그리고 clipping에 대하여서는 위에 해동이가 언급한 Wasserstein에서도 사용되며, 실제적으로 Gradient의 explording을 잡아주는역할도 하지만 얼마전에 삼성팀에서 스터디한 batch renormalization에서도 업데이트할 파라미터 자체를 clipping을 하는것을 볼 수 있습니다. 이는 초반 iterlation에서 bias가 심하게 되어 다시 본 모습으로 돌아오기까지 시간을 생각해서 미리 방지하고 잘못된 방향에서는 크게 잘된 방향으로 옮겨지는 것을 패널티로 삼고 한것이라고 생각합니다. 저희가 계속적으로 하고 있는 text분야에서 RNN을 사용할때의 저희 연구실에서 할 수 있는 분야로써 기회가 생기고 다같이 공유하면서 발전해 나갔으면 좋겠다고 생각하였습니다. 그리고 개인발표와 같이 준비하면서 고생 많았을것 같은데 고생하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170515203117]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[789]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[이번 시간에는 RNN을 학습하는 방법에 대해서 세미나를 진행했습니다. 무엇보다 vanishing gradient/ explosion에 대해 수식적으로 자세하게 설명한 점이 좋았습니다.
또한 말로만 들었던 bi directional RNN에 대해 알 수 있었습니다. 다음 시간에 bi directional RNN의 학습 과정과 backpropagation이 어떻게 진행되는지에 대한 발표가 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170516080901]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[790]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[기계 번역에 있어서 기존의 statictical 기반의 방법과 이들의 문제점부터 현재 사용되는 방법론과 GRU에 대한 자세한 작동 방법까지 알 수 있었던 세미나였습니다.
무엇보다 GRU의 forward와 back propagation에 대해 이전 기창 형님이 RNN과 LSTM을 설명했던 것과 같이 graph로 그려서 설명해 주셔서 보다 쉽게 이해할 수 있었습니다.
또한 질문이 나왔던 바와 같이 실제 현업에서 machin translation에 RNN, GRU 등이 사용될 때 input과 output이 어떻게 정의되는지도 궁금해 졌습니다. 이론적으로는 가능하지만 실제로 수만 수십만개의 단어를 one-hot encoding으로 받게 된다면 메모리가 부족할 것 같은데 실제로 어떤 방법으로 구현이 되는지 한번 알아봐야겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170516081431]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[791]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[RNN에 대한 세미나를 진행하였습니다. 전반적인 RNN에 대하여 알 수 있는 시간이었으며, RNN에서 vanishing gradient가 왜 발생하는지 수식으로 설명한 부분이 있어 좋았습니다. 흔히 sequence modeling을 위하여 RNN을 사용하지만 facebook에서 발표한 Convolutional Sequence to Sequence Learning 논문을 보면 CNN sequence model또한 좋은 성능을 발휘함을 알 수 있습니다.  bidirectional RNN의 학습이 어떻게 이루어지는지에 대한 다음 발표도 기다려집니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170516135717]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[792]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[이해가 용이한 세미나였습니다. cs231n 세미나를 진행할 때도 역전파에 대하여 좋은 세미나를 진행하였던 것으로 기억하는데 이번에도 RNN의 성능 향상을 위한 unit인 GRU의 forward 방식과 backward방식을 직접 만든, 이해하기 쉬운 장표로 발표하여 좋았습니다. 또한 이해가 가지 않는 부분에 대해서 대략적인 설명만 하고 넘어가는 것이 아니라 본인이 이해할 때까지 공부하고, 그러한 흐름을 그대로 발표해주어 듣는 입장에서 받아들이기 더 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170516141512]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[793]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[이번 세미나부터 뭔가 핵심적인 방법론을 배워가는 것 같아 더욱 집중했던 세미나였습니다. RNN은 기존에 있는 ANN 신경망 구조에서 이전 레이어의 히든 스테이트와 현재 레이어의 스테이트를 공유하는 학습기법을 공부해 보왔습니다. Bi-directional RNN같은 경우는 이미지나 다이나니 메모리 네트워크를 공부할 때 접할 수 있었는데 현재 이러한 방식이 좋은 성능을 보이니깐 이러한 방식을 쓰는구나하고 느꼈습니다. 구체적인 수식이나 증명인 부분들을 따로 강의를 다시하면 보면서 논문을 읽어가면서 이해해 보도록 노력하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170517164346]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[794]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[GRU의 backpropagation의 흐름을 자세히 볼 수 있었던 세미나였습니다. 중간에 bidirection과 rnn의 성능을 높이기 위한 토론의 시간도 매우 유익했습니다. 실제로 우리 연구실 대부분이 알고리즘의 기초 개념들은 어느정도 숙지하고 있다고 생각합니다. 동작원리가 어떻게 되는지 수리적인 부분이나 원리에 대해서 하나하나 짚어가는 것이 앞으로 세미나가 진행되는 방향이었으면 좋겠습니다. 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170518225540]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[795]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[예전에 연구실에서 잠깐 논의가 됬었던 부분에 CNN은 activiation 함수로 relu의 성능이 좋지만 RNN은 sigmoid나 tanh가 더 좋다는 것이 있었습니다. 실제로 gradient vanishing이나 exploding이 해결되서 수렴할 수 있어야 알고리즘으로써 역할을 할 수 있는 만큼 이 논의는 중요하다고 생각합니다. 본 알고리즘의 동작 원리와 개선방향, 그리고 알고리즘이 해석해내는 데이터의 특징등에 대한 더 깊은 탐색이 필요함을 느낀 세미나였습니다. 발표자분도 고생하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170518230027]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[796]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[다소 표현력은 부족하였지만 RNN에 대한 core를 잘 다루었던 세미나라고 생각됩니다!
LSTM에 앞서서 gradient vanishing을 억제하기앞서서 RNN에 regularization을 적용한 사례가 있었을까 궁금해서 찾아봤더니 있더라구요. "RECURRENT NEURAL NETWORK REGULARIZATION"에서는 Dropout을 RNN에 apply한 사례로 LSTM이전의 gredient vanishing을 어떻게 해결하였는지도 reference를 통해 확인 할수 있습니다. 물론 최신의 방법론과 트릭들이 좋은 성능을 보여주지만 그것이 등장한 이유나 기원을 알면 공부해가는데 좋은 방법이 되지 않을까 생각됩니다. 고생하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170518230139]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[797]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[LSTM이나 GRU에 대해서 설명이 잘 되어 있다는 해외 블로그에서도 볼 수 없었던 자세한 Back-propagation 강의 였습니다. 많은 책들이 게이트가 어떻게 생겼는지에 대해서만 설명하고 코드 예제로 바로 지나가기 때문에 내부적으로 어떻게 동작하는 지 잘 알 수 없었는 데 많은 도움이 되었습니다. 수리적으로 따라가기 어려운 저도 이해할 수 있는 좋은 발표 였습니다.  발표 준비 하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170518232905]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[798]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[RNN 기초에 대해서 잘 설명했던 세미나 였던 것 같습니다. RNN에 항상 궁금했던 점이 왜 Relu를 쓰지 않고 tanh나 sigmoid를 사용할까라는 점이었습니다. 이번주에 준비하시는 Bi-directional LSTM이 감성분석에서도 우수한 성능을 내고 있고, 앞으로 연구하는 주제에도 적용이 필요하여 많은 기대가 됩니다. 이번 세미나 발표 준비하시느라 고생하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170518233641]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[799]]></uid>
		<content_uid><![CDATA[321]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[RNN의 기본 구조에 대해 학습하는 시간이었습니다. CS231n 강좌 때 제가 vanilla RNN의 역전파에 대해서도 계산그래프로 설명한 적이 있는데요. 그 때 조금 미진했던 수식 부분을 잘 설명해주신 것 같습니다. 발표할 때 지나치게 긴장만 하지 않으면 완벽한 발표가 될 거 같습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170519115012]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[800]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[GRU 셀에 대해 차근차근 설명해주신 점이 정말 좋았습니다. 개인적으로도 해야지 해야지 하면서 살펴보지 못했던 부분인데요, 덕분에 큰 도움이 되었습니다. 수식적으로도, 직관적으로 쉽게 이해할 수 있었던 발표였습니다. 발표를 들을 때마다 발표자의 전달력에 감탄하게 됩니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170519115137]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[801]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[기창형님의 텍스트 사랑이 느껴지는 재미있는 세미나 발표였습니다.
먼저, 제가 이해가 안되었던 부분은 바로 beam search부분 입니다. 그부분이 batch단위에서 flexible하게 변화하는것인지 아니면 전체 데이터를 보고 (training data) fix해서 하는 것인지 모르겠습니다.
그리고 rescursive에서 틀을 맞춰놓고 하는부분이 일반적인 bi directional RNN에서 잡아주지 않을까 생각합니다. 그리고 이 2가지에 대하여 성능에 대한것에 대한 부분이 있으면 더 재미있을것 같습니다. 개인적으로 POS tagger에 대한 raw단에서 연구하시는 분들을 존경합니다. 왜냐하면 data부터 전부 재 생산하여서 실험해야 하기 때문입니다. 오늘 기창 형님께서 여러가지 예시를 들어가면서 들으면서도 개인적으로 생각이 계속 되었던 부분은 저 많은 경우의 수를 다시 단어단위(12만개)에서 더 쪼갠다음 경우의 수를 생각하며 그 경우의 수가 주어가 생략되는 등의 예외상황까지 고려한 tagger가 나오려면 힘들지 않을까 라고 생각하였습니다. 이부분에 대하여 연구를 하여 개발에 성공한다면 한국어 텍스트마이닝에서 feature set으로 보는것이상 즉 담화분석 이상의 것이 나오지 않을까 조심스럽게 예상해 봅니다. 발표 재미있게 들었습니다.!]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170519171256]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[802]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[발표 준비에 많은 노력과 시간을 투자했다는게 보인 발표였습니다. 특히 GRU를 foward propagation과 backward propagation할 때 어떤 과정으로 이루어지는지 그래프로 설명하는 부분이 굉장히 좋았습니다. 그래프 노드 하나하나 이해하기 쉽게 정성들인 덕분에 손으로 따라가면서 쉽게 이해할 수 있었습니다. 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170519171408]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[803]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[가장 기초적인 Recursive Neural Network에 관련된 강의였습니다. 단어가 아닌 구 단위로 임베딩을 한다는 아이디어가 굉장히 참신하다고 느꼈습니다. 역시나 저번 세미나 발표때처럼 손으로 한땀한땀 직접 만든 그래프가 학습 과정을 이해하는데 많은 도움이 되었습니다. 
발표 마지막에 한국어 포스 태깅 부분도 유익하게 들었습니다.  특히 선문말어미를 파악하기 굉장히 어려운데 초성 중성 종성으로 나눠서 접근한다 해도 데이터 구축이 매우 어려울 것으로 보입니다. 항상 영어를 예시로 공부하다가 한국어를 예시로 생각해볼 수 있는 좋은 시간이었습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170519172429]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[804]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[Recursive neural network는 이번 시간에 처음 접했습니다. 세미나 시간에 언급했다시피 RNN은 Recursive neural net의 특수한 케이스입니다. RNN은 시간 순서대로 그대로 계산하면 되지만,  Recursive neural net은 데이터에 따라서 뒤에있는 데이터가 먼저 계산되야할 수도 있습니다. 하지만 그렇다고 모든 경우를 다 고려한다면 시퀀스 길이에 대하여 팩토리얼 단위의 경우를 계산해야할 것입니다. 이를 위하여 Beam search를 사용한다는 것으로 이해했는데, beam search가 어떻게 작용하는지 알 수 없어서 Recursive neural net를 이해하는 데에 어려웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170519204700]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[805]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[국문 베이스의 장점을 잘 살린 발표였다고 생각합니다. 마지막 파트에서 한글 parsing에 대한 설명을 예시를 들며 진행한 부분이 앞으로 도움이 될것이라 생각합니다. cs224d 강의에서는 tree를 구축하는 부분이 적당한 loss를 정의하고 진행한다는 등의 얘기가 적당량 나오기는 하지만, 다음시간에 창엽이형이 다루는듯 하여 따로 질문은 하지 않았습니다. 그부분이 명확하지 않아 이해가 어려웠던점이 아쉬웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170520120351]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[806]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[원 강의에서 Recursive network의 목적식을 설명하는 부분이 어려워서 이번 발표에서 도움을 얻고자 하였는데, 그 내용이 빠져 있었던 점이 아쉬웠습니다. 특별 세션처럼 마련된 국문학 발표 내용이 매우 흥미로웠습니다. 일상에서 아주 쉽게 사용하고 있는 국문을 분석하기 위해서 참 어려운 지식이 필요하다는게 아이러닉했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170521220408]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[807]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[중견연구에서 휴대폰 리뷰의 감성 점수를 찾고자 할 때 각 단어들의 감성 점수만 있으면 찾을 수 있겠다고 생각했는 데, Parse Tree가 있어도 쉽지 않다는 것을 알 수 있었습니다. 한국어 문법은 학교 다니면서도 들어본 적이 없었는 데 많은 도움이 되었습니다. 기창 씨가 국문학 베이스에 개발까지 해서 Stanford Parser와 같은 DSBA Parser를 만들어주면 잘 가져다 쓰겠습니다. 이번주 세미나 준비를 하고 있는 데 위에서 말씀 하신 것 처럼 Tree를 어떻게 학습하는 지에 대한 내용은 아쉽게도 없습니다. 제 파트에서는 논문 3편을 소개 하고 있는 데 관련해서 꼭 필요한 내용이 있다면 찾아서 준비해 보겠습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170522201251]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[808]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[5]]></user_uid>
		<user_display><![CDATA[김 보섭]]></user_display>
		<content><![CDATA[Recursive neural network를 활용할 수 있는 Tree 구조의 parser에 대한 설명이 Recursive neural network를 개념적으로 나마 이해하는 데 큰 도움이 되었습니다. Tree 구조의 한글 pos-tagger를 실제로 Recursive neural network로 학습시키기위해서는 그러한 Tree 구조 (본 세미나에서 설명했던 개념을 토대로한 pos-tagging)를 담아내어 생성된 학습 데이터가 필요해 보이는 데, 그 부분만 해결할 수 있다면 pos-tagger를 만들어낼 수 있을 것 같은 생각이 듭니다. 이 세미나를 들으면서 느낀점은 과거 hmm으로 pos-tagger를 만들 때, 실제로 가정이 pos-tagger는 latent variable인 상태에서 그 latent variable에서 나온 token들이 문장을 이루고 있다는 가정인 데(일종의 generative model), 실제로 recursive neural network로 학습을 만약에 하게된다면 그 반대의 개념이 되는 것인지 약간 생각해보아야할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170522220332]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[809]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[이번 시간에 Recursive Neural Network에 대해서 처음 배우게 되었습니다. 순차적으로 진행되는 Recurrent Neural Network에서 확장된 개념으로 계층적으로 데이터들이 결합된다는 점이 흥미로웠습니다. 지난번 RNN, LSTM 세미나 시간과 마찬가지로 backpropagation 과정에 대해서 상세히 설명해 주셔서 구현하는데 많이 도움이 될 것 같습니다.  국어국문학 전공자의 관점에서 한국어 파싱 결과에 대해서 해석해주시는 부분이 매우 흥미로웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170526104425]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[810]]></uid>
		<content_uid><![CDATA[322]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[GRU 셀을 하나하나 그리면서 설명하여 쉽게 이해할 수 있었습니다. 또한 수식적인 부분에서도 정리가 잘되어 이해하기 쉬웠습니다. 표지 앞장에 한용운 시인의 시가 있어서 GRU를 활용한 application을 살짝 기대했었는데 그부분이 없어 아쉬웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170526124208]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[811]]></uid>
		<content_uid><![CDATA[324]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[RNN에 비해 약간 생소한 recursive neural network에 대해 배우는 시간이었습니다. recursive network를 들으며 느낀점은 다른 딥러닝 방식 또한 computation complexity가 엄청나지만 recursive network는 다른 방법에 비해 더 많은 비용이 소요될 것이라는 것이었습니다. 이때까지의 강의에서는 주로 영어를 활용한 application에 대해 이야기 했는데 이번시간에는  한국어를 대상으로 얘기한 점도 참신했습니다. 매번 느끼지만 한국어는 parsing을 포함한 여러 응용 분야에 적용하기에 복잡하고 어려운듯 합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170526124653]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[812]]></uid>
		<content_uid><![CDATA[327]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[강의에서 다룬 알고리즘의 종류가 많아서 준비하는데 공이 많이 들었을거라 생각됩니다. 각 알고리즘의 목적과 작동원리를 명확하게 설명해준 발표였다고 생각합니다. Recursive neural networks를 저번 시간부터 공부하면서 느낀점은, 문법 정보를 통해 아주 많은 작업을 할 수 있다는 것입니다. 풍성한 정보를 담고있는 데이터가 기술적인 확장에 아주 중요하다고 생각이 들었습니다. 제가 참여하고 있는 중견연구과제에서 이런 양질의 데이터를 만드는 작업을 하고 있기 때문에 더욱 흥미가 갔습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170527135304]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[813]]></uid>
		<content_uid><![CDATA[327]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[데이터는 머신러닝의 기본 베이스이지만, 최근에서야 이 중요성을 체감하고 있습니다. 데이터만 있으면 시도할 수 있을 것들이 데이터의 부재로 인해 할 수 없습니다. 구글, 페이스북, 네이버 등이 뛰어난 성취를 보이는 것의 대부분은 충분한 데이터와 이를 다룰 수 있는 컴퓨팅 자원 덕분이라는 것을 알아가고 있습니다. 
지난 시간에 이어서 공부한 Recursive NN 또한 그렇습니다. 아이디어는 심플하더라도, 이를 실제로 사용하기 어려운 이유는 데이터의 구조가 복잡하고, 복잡한 만큼 실제로 구축하기 어려운 데이터이기 때문입니다.
이번 시간에는 Recursive NN을 이용하는 방법들에 대해서 다양한 어플리케이션을 공부하였습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170527143845]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[814]]></uid>
		<content_uid><![CDATA[327]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[기본 Recursive NN의 단점들을 커버하기 위해 여러 방면으로 발전된 다양한 Network들을 살펴볼 수 있는 좋은 시간이었습니다. 매번 Recursive NN 구조를 공부할 때마다 느끼는 점이지만 문장 파싱이 굉장히 중요하다는 것을 알 수 있었습니다. Recursive Neural Tensor Network처럼 기본 네트워크보다 발전된 구조를 가지고 있더라도 양질의 데이터가 아주 많이 중요합니다. 특히나 한국어 파싱 부분에서 많은 고민을 해볼 수 있는 좋은 기회였습니다. 발표 정말 잘 들었습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170528232153]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[815]]></uid>
		<content_uid><![CDATA[327]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[세미나 시간을 통해 Recursive NN의 활용에 대해서 배우는 시간을 가졌습니다. 강좌를 진행하는 강사님이 제안한 방법론이 3개나 되는데, 연구실에서 가진 데이터의 질의 중요성을 느꼈습니다. 지난번에 박은정 박사님께서 와서 세미나 진행해주신것과, 여러 기업의 설명회등을 통해서 들은 내용에서도 그랬는데, 이번에도 또 한번 느꼈습니다. 특히 한글 corpus는 이정도의 퀄리티를 가지려면 아직 멀었다는 점에서 조금 아쉽기는 하지만, 미리 공부해두고 활용할 그날을 대비하는 것이 저희의 자세가 아닌가하는 생각이 들었습니다. Recursive NN의 경우에는 사실 관심 밖이었는데, 이 모형은 그 특징이 매우 유연하다는 점인데, 활용도가 높을 것이라는 생각 또한 가져봤습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170529155602]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[816]]></uid>
		<content_uid><![CDATA[327]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[다양한 Recursive Neural Networks에 대해 살펴본 시간이었습니다. 발표자 덕분에 이해가 깊어졌습니다. 감사합니다. 다만 적용할 수 있는 아키텍처가 많은 반면 한국어의 경우 파싱된 말뭉치를 구하기가 어려워서, 향후 데이터 확보가 중요할 것 같다는 생각이 듭니다. 데이터가 확보되는대로 Recursive NN을 적극 활용해보고 싶습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170602173017]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[817]]></uid>
		<content_uid><![CDATA[327]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Recursive NN에 대하여 알아 보았습니다. 말씀하신것처럼 데이터셋이 상당히 중요 한 문제가 아닌가 생각해 보았습니다. 그리고 실제 Beam search를 어떻게 해야 하는지 다음 세미나가 궁금하며 이해하면서 다시 생각해 보면 재미있겠다는 생각을 하였습니다. 실제로 다른 방법론와 얼마나 차이가 나는지도 궁금합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170602194614]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[818]]></uid>
		<content_uid><![CDATA[328]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[이번시간에는 Beam search부분과 CNN text classifier에 대하여 알아 보았습니다.  embedding vector까지 gradient 전파를 하는부분을 얼마전의 세미나에서 부터 알게 되어서 쉽게 이해할 수 있었습니다. 그리고 Iterlation 마다 network구조가 바뀌게 되면 batch단위로 올라갈때 모두 바뀌었다는 의미안에서 network parameter를 어떻게 loss를 전파하는지가 이해가 잘 되지 않았습니다. 전체 적인 부분에 대하여 다시 한번 찬찬히 이해하여야 되겠습니다. 마지막으로 설명해 주신 명사 온톨로지 부분은 2007년도에 생성되었고 몇번 클릭이 안된 상황이 재미있었습니다. 알려주시지 않았다면 아마 장기간동안 몰랐을것이라고 생각됩니다. 공유해 주셔서 감사하고 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170602195232]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[819]]></uid>
		<content_uid><![CDATA[328]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[이전에 언급되었던 Beam Search Algorithm을 다시 찾아보고 상세히 알려주는 세심함에 큰 감명을 받았습니다. 딥러닝 방법론은 구조도 중요하지만 그 뒷단에서 작동하는 알고리즘도 참 중요하다는 생각을 요즘 자주합니다.  CNN을 RNN에 적용한 방법은 콜럼버스의 달걀 같았습니다. 참 알고나면 쉽고 왜 저런걸 생각 못 했지 싶지만, 그것을 착안해내기 까지 연구자가 참 많은 노력을 들였을거라 생각합니다. 마지막에 소개해주신 한국어 명사 온톨로지는 앞으로 두고두고 어디다 써먹을 수 있을지 고민해봐야할 거리입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170623170758]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[820]]></uid>
		<content_uid><![CDATA[328]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[지난 시간에 질문이 많았던 beam search에 대하여 다시금 설명을 하셨습니다. beam search algorithm을 통하여 RNN에서 트리구조를 찾는데에 computational efficiency를 높일 수 있었습니다. 이 외에도 CNN을 통한 텍스트 분류(Yoon Kim, 2014)도 높은 performance를 가짐을 다시금 확인하였으며 페이스북에서 오픈한 CSSL(Convolutional Sequence to Sequence Learning) 또한 CNN이 RNN보다 빠른 속도를 가지고 있음을 보여주고 있습니다. 이러한 경향을들 볼 때, 기존의 것들을(beam search, CNN) 잘 이용하는 것이 중요하다는 생각을 해본 세미나였습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170623190105]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[822]]></uid>
		<content_uid><![CDATA[328]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[전반적인 강화학습에 대한 이론을 알수 있었습니다. 환경, 상태, 액션을 통해 강화학습이 이루어 진다는 것을 확인할수 있었습니다. 중간에 사람이 개입하여 모델의 트레이닝 에러가 악순환적으로 증폭되는것을 방지하는 방법론을 확인할수 있었고, 자율주행차에 있었어서 이미지분석과 강화학습 두가지 분야가 접목된다는 것도 알수있는계기가 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170702190628]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[823]]></uid>
		<content_uid><![CDATA[334]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[AlexNet에 대해서 복습할 수 있는 좋은 기회였습니다. 큰 구조만 머리에 남아있고 구현에 쓰였던 다양한 기법들은 까먹었었는데, 이번 발표를 통해 다시 상기시킬 수 있었습니다. 발표자료를 보면 논문의 원본을 캡쳐하고 중요한 부분을 하이라이트한 형식이 많았습니다. 중요 부분만 강조했음에도, 발표자료 치고는 글이 긴 것 아닌가 하는 생각이 들었습니다. 패러프레이즈를 해서 좀 더 간결하게 자료에 담는건 어떨까요.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170705111206]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[825]]></uid>
		<content_uid><![CDATA[334]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[이미지 인식 분야에 큰 붐을 일으킨 AlexNet을 다시 한 번 살펴볼 수 있는 좋은 기회였습니다. 워낙
 예전 기법인 만큼 자세하게 파고들지 않아  세미나에서 보충할 수 있었습니다. 특히 RGB를 PCA로 transform했다는 부분은 처음 공부할 때는 분명 알고있었지만 잊고 있었는데 다시 한 번 복습할 수 있었습니다. 하지만 발표 자료에 논문 글이 통째로 들어가있어서 가독성이 떨어지는건 아쉬웠습니다. 글자를 최대한 더 줄이면 좋은 발표자료가 될 수 있지 않을까 합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170705130326]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[826]]></uid>
		<content_uid><![CDATA[334]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[무엇보다도 첫 세미나 발표라 많이 떨렸을텐데 자연스러운 발표를 하는 모습이 인상깊었습니다. AlexNet은 CNN측에서 잘 알려진 구조인지라 대략적인 정보는 알고있었어서 복습하기에 좋았다고 생각합니다. R-CNN을 지난 cs231n에서도 다뤘지만 제대로 이해하지 못하고 넘어갔었는데, 이번에 최성준씨 패캠자료로 진행하는 스터디와 세미나를 통해 디테일까지도 공부가 되었습니다. 물론 직접 논문을 읽어보는 것이 가장 좋겠지만, 추후 읽기 전에 개념을 잡는 용으로 너무 좋은 시간이었다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170706002407]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[827]]></uid>
		<content_uid><![CDATA[335]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[첫 발표임에도 떨지 않고 자신이 전달하고자 하는 바를 전달하고자 노력하는 모습이 보기 좋았습니다. 자료 준비 뿐만아니라, 발표에서도 이번 세미나를 위해 논문을 디테일하게 읽었고, 이해하기위해 노력한 흔적이 보였다고 생각합니다. cs231n에서 공부했던(재선이가 발표했던것 같은...) ZF-net의 경우 visualization에 un-pooling과 Deconv를 사용하는데 그 부분을 깊게 공부해두지 않아 개인적으로 이해가 잘 안되는 부분이 있었습니다. 논문을 읽어보고 모르는 부분이 있으면 물어보도록 하겠습니다. 뒷부분에 남은 발표도 기대가 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170706002918]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[828]]></uid>
		<content_uid><![CDATA[335]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[Google Lenet은 이미 익숙한 네트워크고 중요한 개념이 쓰인 유명한 구조지만 다시 한 번 복습할 수 있는 좋은 기회였습니다. ZF Net은 Deconvolution과 Unpooling 개념과 관련해서 모호했던 부분들을 정리할 수 있었습니다. 발표자가 세미나 자료 준비를 굉장히 열심히 한 것으로 보였습니다. 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170706211106]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[829]]></uid>
		<content_uid><![CDATA[336]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[Imitation learning에 대하여 배웠습니다. 처음 보는 개념이였는데 상당히 자세하게 설명해 주셔서 왜 필요한지 왜 연구가 되어야 하는지 어떠한 환경에서 사람이 개입하여야 되는지 알게 되어서 재미있었습니다. 참 생각해 보면 이같은 상황은 environment의 외생을 파악을 못하기 때문에 발생한다고 생각합니다. 나중에는 외생에 따른 environment를 다시 actuator를 통하여 학습하는 방식으로 발전하기 않을까? 라는 생각을 하였습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170707163102]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[830]]></uid>
		<content_uid><![CDATA[334]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[AlexNet은 CNN에서 가장 중요한 break point중 하나라고 생각합니다. 
performance도 그때 당시 SOTA였지만, 이 논문을 위해서 실험을 GPU 2개에서 어떻게든 tensorflow같은 것도 없는 상황에서 한땀한땀 진행한 엔지니어적인 부분을 정말정말 개인적으로 높게 평가 하고 있습니다. 현재는 많이 사용하지 않는 normalization 같은 방법론 같은것들도 자세하게 한다면 왜 이렇게 변화되어 왔는지를 알 수 있지 않을까 생각합니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170707163301]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[831]]></uid>
		<content_uid><![CDATA[335]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Deconvolution을 다시 보니 생소하네요. 덕분에 다시 복습할 수 있는 좋은 기회였습니다. 첫 발표임에도 상세히 잘 준비했다고 생각합니다. ZF-Net과 Google LeNet 둘 모두 연구자들이 오랜기간에 고민한 흔적이 느껴져서 참 감명깊은 논문입니다. 문제점에 봉착했을때 그것을 해결한 시도들이 참 아름답습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170709203113]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[832]]></uid>
		<content_uid><![CDATA[336]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[전반적인 강화학습에 대한 이론을 알수 있었습니다. 환경, 상태, 액션을 통해 강화학습이 이루어 진다는 것을 확인할수 있었습니다. 중간에 사람이 개입하여 모델의 트레이닝 에러가 악순환적으로 증폭되는것을 방지하는 방법론을 확인할수 있었고, 자율주행차에 있었어서 이미지분석과 강화학습 두가지 분야가 접목된다는 것도 알수있는계기가 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170709204309]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[833]]></uid>
		<content_uid><![CDATA[334]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[전반적으로 알렉스넷에 대해서 알수 있었고, 알렉스넷은 VGG 넷과 유사한 네트워크 구조를 가지고 있으며 더 간단한 버전이라구 할 수 있습니다. 기본적으로 이러한 방법론이 나온 기본 버전이 아닐까 싶습니다. 컨볼류션레이어 -&gt; 풀링레이어 -&gt; 풀리레 이어 로 많은 연구분야에 대해서 사용된것같습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170709204535]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[834]]></uid>
		<content_uid><![CDATA[336]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[강화학습의 스터디의 첫 시작이었습니다. 비록 강화학습 세미나가 중간에 번복되고 다른 주제로 바뀌었지만 잠시나만 예전에 공부했던 내용을 되짚어 볼 수 있는 좋은 시간이었습니다. 이전의 Silver의 강의는 Dynamic programming 부터 차근차근 밟고 올라가는 구조였는데, 이번 강의는 Real pratices에서 쓰이는 방법론 부터 설명하는 것이 흥미로웠습니다. Imitation learning의 경우 사람이 남긴 데이터를 모방한다는 아주 단순해 보이는 방식을 택했지만, 강의에서 제시한 실험 결과를 보면 우수한 성능을 보여주었습니다. 수리적 기반을 다지면서 가는 방식도 좋지만, 단순하면서도 현실 문제를 해결하는데 적용 가능한 방법론을 먼저 학습하면서 공부하는 방식도 장점이 있다고 생각했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170709205016]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[835]]></uid>
		<content_uid><![CDATA[336]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[현실에서의 강화학습이 어떤식으로 진행되는지에 대하여 알 수 있었던 세미나였습니다. 어떠한 action에 대하여 reward를 주어야 하는지가 모호하여 imitation learning이라는 개념이 나오게 되었고, 사람을 단순히 따라한다는 것 또한 순간순간의 오차들에 의해서 결과가 좋지 않음을 알 수 있었습니다. 이를 해결하기 위하여 학습 중간에 사람이 개입하여 오차를 줄여주는 방식을 취하였습니다. 강화학습을 공부하면서 느낀점은 아기를 학습시키는 것과 유사하다는 점입니다. 아기들처럼 사람의 모습을 지속적으로 모사학습한다면, 다방면에서 사람과 같은 performance를 지니는 기계가 머지않아 나올 것이라 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170709205022]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[836]]></uid>
		<content_uid><![CDATA[335]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[구글넷이 다른 구조들 보다 복잡한 구조를 가진것 같은데 각 특징은 하나에 필터에 담겨져 있고, 이를 1x1 컨브로 다시 파라미터를 줄이는 방법의 설명이 쉽게 이해 되어 많은 도움이 되었습니다. ZF넷의 행렬연산에 대해서는 다시한번 생각해보고 정확히 이해할 것같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170709205047]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[837]]></uid>
		<content_uid><![CDATA[336]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[Intro로서 강화학습의 전반적인 내용에 대해 알아보는 시간이었습니다. 기본적인 내용은 예전에 했던 강화학습과 같지만 이번 강의는 들으면서 정리도 잘 안되고 굉장히 깊다는 생각을 했는데 발표자가 잘 정리하여 임팩트있게 발표한것 같습니다. 인트로인데 수식이 이렇게 많고 어려우면 앞으로 어쩌나 걱정입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170710094111]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[838]]></uid>
		<content_uid><![CDATA[335]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[CS231n에 나왔던 convolution들을 복숩할 수 있었던 좋은 기회였습니다. 분명 그때 잘 이해했다고 생각했는데 다시보니 생소하여 반복학습의 중요성을 깨달았습니다. 특히 ZF net의 deconv부분을 다시 한번 봐야겠다는 생각을 했습니다. 첫 발표임에도 발표자가 내용을 잘 전달해주어 내용을 잘 받아들일 수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170710094439]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[839]]></uid>
		<content_uid><![CDATA[334]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[2012년에 CNN을 통해서 computer vision에서 크게 성능 향상을 이룬 AlexNet에 대한 세미나 발표를 들었습니다. AlexNet은 GPU 2개를 사용하여서 많은 연산량들을 계산하는데 사용하였는데, 딥러닝 연구자들이 GPU 사용에 관심을 가지게 된 연구라고 할 수 있습니다. 향후의 CNN기반 딥러닝 구조들도 AlexNet연구를 기반으로 하기 때문에 AlexNet에 대한 설명이 많은 도움이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170710100204]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[840]]></uid>
		<content_uid><![CDATA[335]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[ZFNet과 GoogLeNet에 대한 발표를 들었습니다. 첫발표임에도 열심히 준비해온것을 느낄 수 있었고, ZFNet에 대해서는 간략하게만 알고 있었는데  Deconvolution 단에서의 수식적으로 전개하는 방법에 대해서 배울수 있어서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170710100524]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[841]]></uid>
		<content_uid><![CDATA[336]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[강화학습의 기초적인 부분을 다시한번 remind 해볼 수 있는 시간이었습니다. 다른 세미나 주제로 변경되기는 했지만 관심있는 분야이기 때문에 혼자서라도 공부해서 연구논문을 작성하는데 적용해보고 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170710100727]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[842]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[실제로 사용하다 보면 VGG Net이 성능이 나쁘지 않습니다. ImageNet에 대하여 실험을 한 결과로는 ResNet이 좋지만 실제적으로 단시간에 사용한다는 가정이면 개인적으로 VGG도 나쁘지 않다고 생각합니다. 개인적ㅇ로 CNN구조는 거의 끝까지 오지 않았나 생각합니다. ResNet이후로는 기존의 나온것의 Variant들이 많이 나오는 추세같아 보입니다. 많이 연구되고 있는 Multi-modal learning을 위해서 network의 크기를 줄이는 연구로 이와 같이 큰 네트워크도 언젠가는 작아지지 않을까 생각합니다.]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170714205139]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[843]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[각자 대비되는 특색이 있는 두 연구가 한 세미나에서 같이 다루어져 더욱 재미있는 발표 였다고 생각합니다. VGGNet은 기존의 연구결과를 자세히 분석하여 문제점과 개선방안을 찾아낸 방식의 연구인 반면, ResNet은 새로운 구조를 제안하는 방식의 연구라고 생각합니다. ResNet 구조의 혁신성은 이미 잘 알려져 있습니다. 그것보다 저는 VGGNet의 연구가 참 인상 깊었습니다. 기존의 연구 결과에 대한 분석만으로도 우수한 연구 성과를 달성했다는 점이 많은 것을 시사합니다. 논리적 사고와 비판적 시각에 대해 한 번 더 고찰해볼 수 있는 시간이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170715191846]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[844]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[쉽게 생각할수 있는 구조에서 변화를 주며 구조적인 이득을 보고자 하는 연구들이 LeNet, ResNet으로 사람들에게 많이 알려지기 시작했다고 생각합니다. 단방향으로 쭉 보내는 구조에서 진해화서, skip-connection 같이 남들이 먼저 생각하지 못한 구조를 생각해 내는 것이 재미있는 연구주제가 되지 않을까 생각했습니다. 그리고 ResNet 연구 이후에 이 구조가 왜 좋은 성능을 보이는지, 앙상블 관점에서 연구를 한 사례도 재미있게 들었습니다. 얼핏 보면 구조의 특징으로 인해 학습이 잘된 것 처럼 보이지만 사실은 앙상블이었다 라는 주장이었는데, 하나의 구조에 대하여 여러 관점으로 보고 개선시킬수 있는 여지가 있겠구나라는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170716162124]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[845]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[VGG는 CNN에서 vanilla 모델로 여겨질 정도로 단순한 모델에 경지에 왔습니다. 이후의 ResNet은 정말 혁신이였을 것이라고 생각합니다. 이 두가지 모델을 공부하게 되어 재미있는 발표였습니다.
한편, Residual module이 나오게 된 배경과 그 의의에 대한 설명이 아쉬웠습니다. 논문 내에서 설명하는 이유는 극단적으로 깊어질 때에 모델의 성능이 낮아지는 이유가 일반적인 convolution이 identity mapping을 잘 학습하지 못하기 때문이라고 합니다. 이 말에 따르면 김해동 형이 얘기한 것과도 연관이 되는데, Residual network 또한 새로운 구조를 제안한 것도 맞지만, 이 배경에는 기존의 vanilla convolution의 연구 결과를 굉장하게 분석한 것이라고 생각할 수 있습니다.
이와 더불어, residual module을 이론적으로도 탄탄히 보강하여 1000 layer level에서도 잘 작동하는 연구에 대한 논문 "Identity Mappings in Deep Residual Networks"도 참고하면 좋을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170716162338]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[846]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[개인적으로 VGG Net의 가장 흥미로운 점은 2014 이미지넷 대회에서 분류 문제에 2등을 했음에도 불구하고 해당년도 1등인 GoogLeNet못지않게 널리 쓰인다는 점입니다. 그만큼 간단하고 직관적이며 성능도 훌륭한 네트워크라 생각합니다. ResNet의 경우에는 레이어를 정말 깊게 쌓으면 결과가 그만큼 좋지 않을까 라는 단순한 아이디어에서 시작하여, 그렇지 않은 실험 결과를 보고 이유를 파악하고 그 이유를 해결해나갑니다. F(x)의 간단한 변형 만으로도 문제를 해결할 수 있음을 보고 문제를 해결함에 있어 항상 복잡할 필요는 없다는 교수님의 말씀을 떠올렸습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170716164930]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[847]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[실제 toy data로 실험해본 결과 resnet보다 alexnet이 훨씬 좋은 성능을 보이는 경우도 있음을 확인했습니다. imagenet의 데이터는 그 구조가 복잡하고 데이터 사이즈가 작지 않아 resnet이 더 나은 성능을 보인 것으로 생각됩니다. resnet은 망이 깊어지면 성능이 저하되는 문제를 degradation문제라 정의했고 그것을 해결하는 방법으로 skip connection을 제안했지만 과연 그것이 내가 다루고 있는 dataset에서도 적용되는 개념인지는 적용해 보아야 알 수 있는 문제라고 생각됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170716210939]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[848]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[VGGNet과 ResNet에 대해 배우는 세미나였습니다. VGGNet도 유익했지만 개인적으로는 ResNet을 앙상블로 보는 관점이 인상적이었습니다. 그런데 width를 늘리게 되면 channel을 추가하면서 parameter search에 많은 시간이 소모될 것이라 하였는데 time complexity 관점에서 비교하는 내용이 있었다면 더 좋았을 듯 합니다. 하지만 첫 발표임에도 불구하고 깔끔하고 군더더기 없이 전달력이 좋은 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170717221109]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[849]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[과제에서 VGGNet을 사용하고 있는데 좀 더 상세한 내용에 대해 알 수 있어서 좋았습니다. Residual Network도 구현해보면서 해당 내용에 대해서 제대로 이해 할 수 있도록 공부할 계획입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170718091030]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[850]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[기계학습의 논리 흐름이 잘 정리돼있어서 혹시나 누군가를 알려줄 일이 생긴다면 참고해서 만들기 좋을 것 같습니다. 이번 강좌도 NLP관련된 강좌다 보니 워드 임베딩부터 시작을 했습니다. 아직까지는 다들 아는 내용이겠지만 앞으로 QA, STT , TTS등의 여러 application 들을 다룰것이 기대가 됩니다. 다른 사람들을 위해 한글로 자료를 만들어야겠다는 생각을 했다는 점에서 감동받았습니다.ㅎ]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170720130738]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[851]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[자연어처리 기법들에 관해 본격적으로 강의를 시작하기 전 흐름을 살펴볼 수 있는 강의였습니다. 교수님의 비정형데이터분석 수업 시간에도 다루었던 내용들이 기본 개념들로 나왔습니다. 수업시간에 들었던 내용들을 한번 더 훑어보면서 기억을 떠올릴 수 있는 좋은 발표였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170720232427]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[852]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[자연어 처리를 이해하는데 가장 기본이되는 기초적인 내용을 친근한 소재를 통해 알기 쉽게 다룬점이 놀라웠습니다. 앞으로 Language Modeling, QA 등 심도있는 내용을 다루기 전에 다시 한번 기본을 확인해 볼 수 있었던 좋은 시간이었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170721113243]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[853]]></uid>
		<content_uid><![CDATA[338]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[VGG 넷은 깊은 레이어로 뉴럴 네트워크 구조의 형태를 만들어 학습하게 되었고 이는 기존의 알렉스 넷보다 좋은 성능을 가졌다. 하지만 레이어의 층이 깊어질수록 Resnet 넷이라는 뉴럴 구조를 만들어내 잔차를 설정하여 보다 더 vanishing gredient의 문제를 극복하지 않았나 싶다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170721135708]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[854]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[외국 대학교의 수업들을 들으면, 수업 전체의 구조가 정말 쉽게 시작해서 깊게 끝나는것 같습니다. 이번 강의도 강좌를 시작하는 강의로 다음주부터 시작되는 RNN부터 text to speech, QnA와 같이 깊은 내용까지 배우게 될 것 같습니다. 다른 사람에게 지식을 전달할 때 어떤식으로 접근하는 것이 좋은지 생각하게 해주는 강의였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170721135823]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[855]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[자연어의 기본적이  학습을 살펴보았고, 통계학을 잘모르는 사람도 알수 있는 강의 였던것 같습니다.  한편으로는 우리가 다 알고 있다고 생각하고 놓지고 있는 부분은 없지 않을까 생각도 됩니다. 알고 있다고 해도 그 방법론에 대해서 논문으로 써의 아이디어를 만들기 위해서는 파고 또 파야 된다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170721135915]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[856]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[cs231n, cs224d를 할 때도 느꼈지만 초반 강좌는 쉬운 내용을 다루고 있다고 생각합니다. 다시 한 번 리마인드 하고 앞으로 강좌를 준비해야 할 것 같습니다. PT는 발표자가 발표하고 이해한 부분을 전달하기에 편한 방식으로 작성하는 것이 좋다고 생각하지만, 처음 보는 저의 입장에서는 한글로 된 자료가 다소 더 편하게 접근할 수 있는 것 같아서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170721143253]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[857]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA["Somethings are our friends"를 보고 "이렇게까지 해야하나?"라는 생각을 했습니다. 
하지만 곧 "이 레벨의 사람들은 이렇게까지 하는구나"라는 생각이 들었습니다.
쉽게 설명할 수 있어야 진짜로 이해한 것이라고 할 수 있다고들 합니다. 이를 적극적으로 실천하는 모습이며, 본받아야겠다는 생각을 했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170722151013]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[858]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[이번 NLP강좌가 저번부터 새로운 주제들을 다루는것으로 알고 있는데 다들 열심히 토론하고 습득하여 실제 task에서 유연하게 쓸수 있었으면 좋겠습니다. 그리고 쉽게쉽게 설명할 수 있는 것은 그 문제를 완벽하게 이해하고 발표할때의 청중의 상황까지 고려하는 레벨이 되야 된다고 생각합니다. 이런점에서 재미있게 들었습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170723231918]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[859]]></uid>
		<content_uid><![CDATA[342]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[저번 시간에 워드 벡터를 배웠고 이번에는 Language model을 학습하는 단계로 넘어왔습니다. 비정형데이터분석 시간에 배웠던 내용들이 대부분 이었지만, 복습 차원에서 아주 유용한 세미나 시간이었습니다. 발표에서 MLE의 단점을 언급했는데요,  Sample space를 충분히 알지 못할때 MLE는 과적합 되는 우려가 있다는 단점이 있습니다. 이것을 해결할 수 있는 방안으로 Bayesian을 사용하면 Prior 있기 때문에 몇 개의 관측치에 모델 전체가 흔들리는 상황을 어느정도 해결할 수 있다고 합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170725175609]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[860]]></uid>
		<content_uid><![CDATA[343]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[아주 열심히 준비를한 흔적이 곳곳에서 보입니다. 소개해준 논문의 저자가 원하고자 하는 목적을 이루기 위해 기계 학습의 여러 분야에서 방법론을 차용해 와서 사용한 것이 인상 깊었습니다. R-CNN과 RNN, 그리고 Graphical Modeling까지 사용을 했네요.  제가 captinong 연구를 할 일은 아마 없을 확률이 높겠지만, 연구 목적을 달성해 나가는 모습이 아주 인상깊었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170725181214]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[861]]></uid>
		<content_uid><![CDATA[343]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[명준이의 첫 research presentation이었습니다.
여러부분에서 공부를 한 흔적이 많이 보입니다.마지막으로 proposal한 research topic은 상당히 흥미로웠습니다. 개인적으로 결과가 궁금합니다. 만약 Non linear mapping이 성공한다면 번역의 의미를 넘어서 언어학의 새로운 관점이 될 수도 있다고 생각합니다.
왜냐하면 Neural net기반의 번역은 되는 메커니즘 자체가 논리적으로 이해가 쉽습니다. 하지만 제안하는 방법은 Embedding을 하는것 자체가 번역을 기반으로 하는것이 아니기 때문에 parsing rule, embedding parameter에 제약사항이 굉장히 많다고 생각합니다. 그런데도 불구하고 소수의 라벨로 Mapping rule이 만들어 진다면, 이는 사람이 사고하고 적고 앞뒤의 구문자체가 언어에서의 독립성이 배재된다고 생각하기 때문에 궁금합니다. 응원합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170728000951]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[862]]></uid>
		<content_uid><![CDATA[342]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[요즘 RNN에 대하여 학습해 보고 공부하면서 승완이가 발표하는 부분에 대하여서도 흥미롭게 잘 들었습니다. 개인적으로 RNN은 데이터가 들어갔을때 처음본 space에서( 새로운 데이터 공간 )에서 cell state가 흔들리는 문제가 있다고 생각합니다. 그리고 many to many model에서는 첫 state에서는 multi layer neuralnet과 같다고 생각해서 보통 뒤로 빼서 loss를 구하는데 이번에 배우는 세미나 주제에서는 이를 어디까지 다룰지 궁금합니다.
그리고 이를 보완하기 위해서 우리가 많이 다루었던 bi-directional 방법이 있고 찾아보니 skip-connection을 RNN에서 쓰기도 하는데 효과는 아직 data에 많이 dependent하다고 합니다.
개인적으로 실험해본 결과는 Multi-layer RNN을 한다고 했을때 input,output dropout을 하는것이 이런 N-gram의 RNN 의 straightforward한 부분을 많이 보완해 줄 수 있다고 생각합니다. 실제로 regression task에서 training을 해보면 이게 많이 느끼는 부분이 있었습니다. Keystroke도 어떻게 보면 signal에서의 N-gram과 많이 흡사합니다.처음부터 Input Output을 결국 bigram di-graph를 넣은것과 마찬가지인데, dropout을 했을때가 성능이 상당히 괜찮게 나옵니다. 이는 결국, 우리가 Multi layer에서의 neuralnet에서 regularization, 역할을 한다고도 볼수 있지만 RNN에서는 가보지 못한 길을 오히려 여러가지 길로 가서 학습량을 증대하는 역할을 한다고도 볼수 있다고 생각합니다. RNN을 조금씩 더 공부하고 있는데 앞으로도 기대됩니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170728001732]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[863]]></uid>
		<content_uid><![CDATA[342]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[여러가지 Language model 학습 방법을 복습할 수 있는 좋은 시간이었습니다. 이론으로는 잘 알고있지만 Recurrent Neural Network의 경우 아예 구현을 해본 적이 없어서 간단한 구조도 차례로 해볼 예정입니다. 간단한 이론이라도 밑바닥부터 차근차근 구현할 줄 알아야 진정한 자기 자산이 되는게 아닐까 하는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170728144328]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[864]]></uid>
		<content_uid><![CDATA[343]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[수식 하나하나 자세히 설명한 발표 잘 들었습니다. 준비하는데 많은 노력을 쏟은 것이  보였습니다. 마지막에 개인 연구 주제로 발표한 부분도 매우 흥미로웠습니다. 저번 스터디에서 Neural Style 부분에서 아이디어를 얻은 것 같은데,  스타일 이미지 피쳐 맵의 그램 매트릭스와 언어의 semantic을 담고있는 그램 매트릭스를 잘 매칭시키는게 굉장히 중요할 것 같습니다. 제대로 모델 튜닝만 잘 시킨다면 희소 언어의 style에 다른 희소언어의 content를 접목시켜 재미있는 결과물이 나올 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170728145003]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[865]]></uid>
		<content_uid><![CDATA[342]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[count based language model부터 recurrent neural network를 사용한 language model까지 다시한번 복습할 수 있었던 좋은 시간이었습니다. 이번 세미나를 통해 RNN에 대해서 부족했던 부분을 다시한번
 체크하고 공부하는 시간을 가질 수 있었습니다. 다음시간 제 부분도 열심히 준비하도록 하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170728163048]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[866]]></uid>
		<content_uid><![CDATA[339]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[자연어 처리를 딥러닝을 이용하여 문제를 해결하는 내용들에 대한 기본적인 내용들을 복습할 수 있었습니다. Lecture 1b의 'Deep Neural Networks Are Our Friends' 강의에서는 기계학습에 대해서 잘 모르는 사람들도 쉽게 이해 할 수 있도록 상세히 설명을 해 놓았는데 발표자료를 준비하는데 필요한 점에 대해서 생각해 보는 계기가 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170729171408]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[867]]></uid>
		<content_uid><![CDATA[342]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[Language model이 발전한 과정을 짚어본 시간이었습니다. 처음에는 통계기반의 확률적 모형으로 접근을 했고 점차 Network 구조를 이용했는데, 최근에는 Language model에서 대부분이 Network 구조를 사용하는 것으로 나타나고 있습니다. 이미 배운 내용이 많았지만, 반복적으로 듣는 중에 새로운 관점으로 해당 내용이 채워지는 경험을 하고있는 터라 재미있게 듣고있습니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170802002246]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[870]]></uid>
		<content_uid><![CDATA[342]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[language 모델의 다양한 학습 방법에 대해서 배울 수 있었습니다. 기존의 방법들의 단점을 보완하는 방법으로 새로운 학습방법들이 나오고 있는데 RNN 방법의 단점을 보완할 연구들이 기대가 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170804101644]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[871]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[LSTM, GRU등이 나오게 된 배경과 Deep RNN이 나온 흐름, 그리고 loss 부분에서 softmax를 근사해서 접근해보고자 하는 내용을 다루었는데, 영상에서 디테일하게 다루지 않았던 부분을 다뤄줘서 너무 유익한 시간이었습니다. 영상에서는 하나하나를 간단히 이름만 짚고 넘어가는 수준이었는데, 명준이가 유투브에 녹음본을 올려주면 그걸 듣고 수식을 하나하나 따라가면서 공부를 해봐야겠습니다. 발표 준비 퀄리티에 놀랐습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170807110836]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[869]]></uid>
		<content_uid><![CDATA[343]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[논문 연구에 대한 발표를 꼼꼼히 준비한 느낌을 많이 받았습니다. 본 논문의 내용과 전후 관계까지 파악하여 어떤 부족한 부분 때문에 이런 아이디어를 냈다라는 흐름에서 연구 대상을 어떻게 선정하는지에 대한 느낌도 받을 수 있는 부분이 있었다 생각합니다. 명준이의 개인연구 주제 발표와 본 논문 발표를 들으며 image와 text는 알고리즘의 구조적 논리만 충분히 만족시킨다면 서로 바꿔서 적용해도 되는 부분이 여럿 있구나 라는 생각을 다시한번 갖게 되었습니다. 여러 아이디어가 있지만 공간이 부족해 생략하고, 오프라인으로 찾아오시면 논의해 보겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170802003820]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[872]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[1.과 2. 문항은 이전 세미나에서도 여러번 다루었던 주제이지만, 나머지 문항들은 이전에 자세히 다루어지지 않았거나 처음 보는 내용들이 다수 있어 유익한 정보를 많이 얻었습니다. 특히 재미있게 봤던 것이 3. 문항입니다. Continuous Bag of Words에서 쓰인 negative sampling과 hierarchial softmax 이외에도 큰 단어 집합을 다루는 여러가지 방법을 배울 수 있어서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170807110945]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[873]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[지난 CS224D에서도 RNN을 다루었지만, 이번 강의에서는 LSTM의 탄생 배경을 순차적으로 설명하여 이해를 도왔다고 생각합니다.
또한 Loss에 대하여 심도있는 고민을 할 기회는 없었던 것 같습니다. 이에 대하여 조금이나마 더 나아갔고, 강의에 포함되지 않은 자료들도 준비하여 좋았습니다.
전체 Case를 모두 다룰 수 없을 때, Class를 Sampling 하여 Loss를 계산하는 방법을 Candidate sampling이라고 하는데, 이에 대하여 텐서플로에서 정리한 https://www.tensorflow.org/extras/candidate_sampling.pdf 를 참고하면 많은 공부를 할 수 있을것이라 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170807132406]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[874]]></uid>
		<content_uid><![CDATA[343]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[이미지 캡셔닝에 대하여 자세하게 설명을 들은 것은 처음이였습니다. 특히나 해당 발표에서 이야기한 연구는 Target이 명확하게 존재하지 않음에도 caption을 적어준 다는 것이 놀라웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170807132711]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[875]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[RNN 관련 다양한 수식들과 기법을 설명해주는 시간이었습니다. RNN의 드롭아웃이라든지, 소프트맥스 확률을 구할 때 계산량을 줄이는 방식이라든지 하는 실전 팁들이 앞으로 크게 도움이 될 것 같습니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170807155251]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[876]]></uid>
		<content_uid><![CDATA[343]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[17]]></user_uid>
		<user_display><![CDATA[이기창]]></user_display>
		<content><![CDATA[발표 준비를 꼼꼼하게 했다는 인상을 받았습니다. 마지막 논문 아이디어 발표도 인상 깊었습니다. 다만 제안한 기법과 관련 목적(함수)를 정의할 때 공간과 공간을 매핑하는 데 중점을 둘 것인지, 공간 내 인스턴스와 인스턴스를 매핑하는 데 중점을 둘 것인지 두 가지 선택지가 있을 것으로 보이는데 후자 쪽에 강조를 하면 더 좋은 연구결과가 있을 것이라 생각합니다. 다시 말해 기계 번역의 경우 소스랭귀지 문장 혹은 구 단위를 입력으로 해서 타겟랭귀지의 문장 혹은 구가 출력으로 나오게 되는데 문장 혹은 구 단위 정확도가 기계번역의 핵심이 될 것이기 때문입니다. 앞으로 대단한 발전이 예상됩니다! 기대가 큽니다. 발표 준비하느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170807155438]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[877]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[명준이가 RNN에 대한 발표를 하였습니다.
[1] 먼저 해당 과정을 열심히 정리를 잘 하였습니다. 듣는사람입장에서 편하게 들을 수 있었던거 같습니다. 먼저 항상 이야기하는 Vanilra에서의 문제점으로 Vanishing/Exploding gradient에 대하여 이야기 하였고 왜 LSTM이 이를 방지하여주는지에 대하여 차근차근 정리하였습니다.
[2] 다음으로는 Regularization method에 대하여 발표하였고 Dropout에 대하여 발표 하였습니다. RNN에서의 Dropout는 상당히 큰 의미가 있다고 개인적으로 생각합니다. 먼저, Input output에 대하여 Dropout을 하는데 이는 데이터에 대한 regularization뿐만아니라, time에 따른 noise도 제거해 준다고 생각합니다. 그리고 sequence에 따른 variable을 새로 생성해 주는, RNN에서의 Unseen data distribution을 보완해 준다는 점에서 상당히 중요하다고 생각합니다.
[3] NCE (Noise Contrastive Estimation)에 대하여 다루었는데 이는 상당히 중요한 loss이며 어쩔때는 (삼성 과제)에서는 NCE loss가 더 좋지않을수도 있습니다. (Small vocab)
수고하셨습니다. RNN 관련해서는 아직까지 GPU Training에서 Cell state가 길어지면 training time이 극악으로 올라갑니다. 이에 대하여 앞으로 다루면 참 좋을것 같은데 앞으로가 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170808015555]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[878]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[강의에서 대충 언급하고 넘어간 내용들을 자세히 다뤄줘서 아주 좋았습니다.  LSTM 관련해서는 구현해본 적이 전혀 없고 공부도 깊게 하지 못했는데 강의 자료를 보면서 아주 자세히 공부할 수 있을 것 같습니다. 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170809145420]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[879]]></uid>
		<content_uid><![CDATA[345]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[STN의 아이디어 자체는 복잡하지 않습니다. 하지만 이해하는 데에 쉽지 않고, 저자가 상세히 설명하지 않아 굉장히 복잡했던 것으로 기억합니다. 그럼에도 다양한 자료를 찾아보며 이에 대한 설명을 잘 해주었습니다.
덧붙이자면, STN을 직접 학습시켜보았는데, 논문에서 서술하지 않은 핵심적인 트릭들이 있습니다. 논문상에는 비선형 함수로 ReLU와 Softmax만을 사용하였다고 서술하지만, 제 경험상으로는 안정성을 위하여 Localize Network에 tanh와 같이 상대적으로 진동이 적은 함수를 써야했습니다. 또한, 마찬가지로 안정적인 학습을 위하여 initializing은 identity transform을 하도록 weight는 0, bias를 identity로 주는 것이 이상적인 것으로 보입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170810215332]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[880]]></uid>
		<content_uid><![CDATA[345]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[먼저, 개인적으로 재윤이에게 감사합니다. 이번에 이 Network를 쓸일이 생길것 같은데 (지금은 학습할 데이터를 엄청난게 만들고 있지만) 하나의 이정표를 만들어줘서 고맙습니다. 개인적으로 더 관심이 있는 부분은 Width Height 비율 변환, Rotate와 이탤릭체같이 변화 하는거 이외에 3D를 Tranformation하는 부분이었습니다. 이를 확장시키면 이전에 말했던것 처럼 콜라병처럼 변환이 가능한 모델이 가능할것 같은데 이번에 제대로 공부해 보려고 합니다.
그리고 계속 생각해보고 있는 부분은 이모델의 가정은 대부분이 제대로 보고 있다는 가정이라고 생각합니다. 대부분이 삐딱(?)하게 보고 있다면 이 모델은 당연히 삐닥하게 보는것이 최적이라고 생각할것이라고 생각합니다. 그 부분에서도 숫자의 6이 삐딱하게 적은부분에서 전부 서 있는것과 비슷하게 끝부분이 휘어 있다면 이것도 문제가 되는 부분이라고 생각이 드는데 이 부분은 직접 꾸부려서 input으로 넣은 output을 보면서 해석해 보려고 합니다. 고마워요! :)]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170810220924]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[881]]></uid>
		<content_uid><![CDATA[345]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[최신 논문의 정보를 상세하게 알려준 발표자와, 댓글을 통해 본인의 노하우를 알려준 연구원들 모두에게 감사드립니다. 기계 학습 분야의 논문이 하루가 다르게 쏟아져 나오고 있어서 최신 경향을 따라잡기가 쉽지 않은데, 이렇게 세미나를 통해서 놓쳤던 연구들을 알 수 있어서 무척 유익합니다. 이제 발전의 끝에 다했다는 CNN으로도 아직 이렇게 많은 일들을 할 수 있는 것을 보아 호사가들의 걱정은 기우가 아닐까 합니다. 예전 세미나에서 공부한 자료에서 Hinton교수가 invariants in image data를 아주 중요한 문제라고 언급하였는데, 이 난제들이 해결되어 가는 모습을 보니 아주 큰 자극이 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170810222110]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[882]]></uid>
		<content_uid><![CDATA[345]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[첫번째 주제로 spatial transformer network 라는 주제에 대한 발표를 들었습니다. 기존 CNN 구조에서 공간적인 변형이 있을경우 파악하지 못하는 점을 해결하려는 연구 목표가 좋았습니다. interpolation 방법에 대해서 잘 몰랐는데 이번 발표를 통해 설명을 들을 수 있어서 좋았습니다. Deformable Convolutional Network는 필터 사이즈가 고정되어 있지 않고 좌표가 이동하는 점이 흥미로웠습니다. 학습 구조가 아직 이해가 안되서 논문을 읽어볼 생각입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811111421]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[883]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[RNN의 Vanishing/ Exploding gradients 문제, LSTM/RNN  학습과정을 상세히 수식적으로 설명하여 도움이 많이 되었습니다. 관련 내용을 구현하는데 도움이 많이 될 것 같습니다 . 많이 준비한 흔적이 느껴지는 발표여서 좋았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811114235]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[884]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[RNN, LSTM, GRU, vanishing gradient 등 Cs224에서 다뤘던 내용이어서 다시 한번 복습하고 넘어가기 좋은 세미나였습니다. 세미나 내용 중 새로웠던 점은 dropout에 대한 부분인데 마지막에 기존의 dropout에 더해 connection을 share하여 모든 부분에서 dropout을 적용한다는 점이 새로웠습니다. RNN과 관련된 dropout은 아직 생소하여 더 공부해봐야 할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811123233]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[885]]></uid>
		<content_uid><![CDATA[345]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[두 논문 모두 기존 CNN의 한계를 깨고자 노력한 논문들인데요, 첫번째는 input 자체를 좀 더 task에 맞는 형태로 변형시키는 것이고, 두번째는 filter의 크기가 고정되어있다 라는 CNN의 가정 자체를 풀었다고 이해했습니다. 특히 첫번째의 STN의 경우에는 subnetwork 구조를 통해 theta를 학습하는데, 처음에는 이해가 되지 않다가 마지막에가서 이해가 되서, 앞의 설명중에 놓쳤던 부분을 이해하기위해 다시 읽어봐야겠다는 생각을 했습니다. 다만 아쉬운 것은 STN의 경우에는 label이 있어야하고, 정의된 task를 잘 풀수 있는 방향으로 input을 수정해나간다는 것인데, 이부분이 label 없이 가능하다면 정말 좋겠다라는 생각을 했습니다. 요새 GAN공부를 하고 있는데, 이를 풀기위해 잘 적용해보면, STN의 subnetwork부분을 GAN으로 접근해볼수 있지 않을까 하는 생각을 해봤습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811151237]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[886]]></uid>
		<content_uid><![CDATA[343]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[준비하는데 많은 노력을 쏟은 것이 보였습니다. 논문의 전반적인 느낌을 잘 설명해주었고 카파시의 설명을 그대로 받아들이지 않고 잘못된 점을 지적하는 부분이 좋았습니다. 또한 개인연구 아이디어인 매핑 방식은 아주 흥미로웠습니다. 앞으로의 발표들도 기대하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811154719]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[887]]></uid>
		<content_uid><![CDATA[344]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[LSTM이 나온 배경에 대하여 잘 설명해 주었습니다. 전개가 논리적으로 아주 합당하였고 loss에 대하여 설명하는 부분이 매우 좋았습니다. 본 강의에서는 제대로 짚어주지 않았던 부분들을 직접 논문을 찾아 이해하고 설명하여 전체 수식을 이해하지는 못하였으나, 듣는 입장에서도 흐름을 따라가기 편했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811155019]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[888]]></uid>
		<content_uid><![CDATA[345]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[theta의 학습을 통해 본 이미지를 transform 시킨다는 논문에 대하여서 접근 방법은 매우 흥미로우나 mnist 데이터와 같이 단일 대상이 아닌 실제의 이미지와 같이 object가 많은 경우에는 적용하기 힘들어 보였습니다. 이를 보완하는 방식을 생각해봄도 좋은 일일 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811155810]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[889]]></uid>
		<content_uid><![CDATA[345]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[Spatial Transform Network는 개인적으로 공부하면서 상당히 어려움을 느꼈던 논문인데 이해하기 쉽게 정리하여 깔끔히 설명해 주셔서 대단히 감사합니다.  두번째로 발표한 Deformable CNN 같은 경우 기존의 image의 모든 부분을 살펴보았던 CNN과는 다르게  중요한 부분을 선택하여 feature를 만든다는 점이 굉장이 신기했습니다. 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170811231611]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[890]]></uid>
		<content_uid><![CDATA[346]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[대부분의 내용을 강의나 이전 세미나에서 배운 것이었지만, 복습의 차원에서 재미있게 들은 발표였습니다. 이 강의에서 흥미롭다고 생각했던 것 중 하나는 문서 불류기의 종류를 소개할때 딥러닝 기반의 방법과 통계적 방법을 서로 다른 종류의 것으로 언급했다는 점입니다. 물론 분서 분류와 직접적으로 연관된 중요한 내용은 아니지만 생각해볼 것이 많은 주제라고 생각합니다. 예전에 Quora에 누군가 통계 학습 방법이 딥러닝을 언젠가는 이길 날이 올것이냐고 질문을 올렸는데, 거기다 Bengio교수가 딥러닝도 통계적 방법이라는 답변을 단 것이 기억에 나네요. 하지만 많은 사람들이 딥러닝과 통계 기반 학습을 구분해서 받아들이고 있는 것 같습니다. 제 추측에는 딥러닝을 설명할 수 있는 체계적인 통계적 이론이 아직 없기 때문이 아닌가 생각합니다. 최근 이 분야의 연구들이 미국을 중심으로 조금씩 불붙고 있는것 같습니다 (2017 ICML Best paper 참고). 저는 개인적으로 관심이 많은 주제라 눈여겨 보고있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170812182151]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[891]]></uid>
		<content_uid><![CDATA[347]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[GAN의 original paper에서는 GAN이 JSD를 사용하기 때문에 다른 generative models보다 더 선명한 이미를 생성한다고 추측하고 있습니다. 하지만 이 추측은 다른 여러 논문들에서 반박되었습니다. 따라서 mode collapse도 JSD 때문이라고 말하기 어렵다고 저는 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170812183317]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[892]]></uid>
		<content_uid><![CDATA[346]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[저도 김해동 연구원과 마찬가지로 해당 사항이 흥미로웠습니다. 통계 vs 기계학습, 혹은 통계 vs 신경망의 구도와 경계는 어떻게 결정지어지는지 아직까지 잘 모르겠습니다. 
자세하게 생각해본적은 없지만, 느낌상으로는 모델을 가정할 때 연역쪽에 치우치냐, 귀납쪽에 치우치냐에 따라 통계와 기계학습이 나뉜다는 생각도 하지만, 이 또한 경계선이 명확하지 않다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170814170521]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[893]]></uid>
		<content_uid><![CDATA[347]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[대 GAN시대에 GAN을 자세히 공부해 본 적 없지만, 큰 얼개를 잡을 수 있는 세미나였습니다. 수식을 하나하나 차근차근 유도한 것이 좋았습니다. GAN은 수리적인 정리나 notation이 많은 만큼, 스스로 깊게 생각할 필요가 있는 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170814170645]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[894]]></uid>
		<content_uid><![CDATA[347]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[대 GAN시대에 GAN을 어디다가 써야할지 아이디어가 떠오르지 않아서 GAN을 자세히 공부해 본 적 없지만(2) 수식을 알기쉽게 풀어서 설명한 점이 좋았습니다. 개인적으로 사람 사진에 대하여 MSE를 counterpart로 둔것은 반칙이 아닐까 생각합니다. 보통 Image에서 Huber혹은 L1 loss를 무조건 사용하기 때문입니다. 그리고 현재는 GAN자체의 첫번째로 행하는 RANDOM한 방식 자체를 넘어서 학습하는 논문들도 나오고 있는데 기회가 된다면 차근차근 공부해야 되겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170817163440]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[895]]></uid>
		<content_uid><![CDATA[346]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA["Bengio교수가 딥러닝도 통계적 방법이라는 답변을 단 것이 기억에 나네요."
제가 알기로 Bengio교수님쪽에서 Deep learning에 대한 모델에 관한 수학적 근거를 하나하나 밝히는 연구를 하고 있다고 컴퓨터 공학과의 박사과정 지인에게 들은적이 있습니다. 개인적으로 대화를 하였을때도 해당 연구가 꼭 필요하고 개인적으로 모델에 대한 수학적 배경은 상당히 중요하다고 생각합니다. 수학자들이 쎄게 말하는것처럼  "그냥 데이터 가져와서 모델에다가 '때.려.박.아.서' 나오는 결과를 믿는게 과연 과학이냐고 말할것에 반박으로 꼭 필요한 통계적 검증이라고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170817163818]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[896]]></uid>
		<content_uid><![CDATA[347]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[GAN에 대해서 응용사례만 확인해 봤지 이렇게 수식하나하나 확인할 수 있는 좋은 기회였습니다. 수식 증명을 다른 사람에게 이해시키기는 어려운 일인데 세미나 목적에 맞게 모두가 이번에 겐을 제대로 이해하게되는 계기였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170818094857]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[897]]></uid>
		<content_uid><![CDATA[346]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[로지스틱을 중점으로 분류를 문제를 다뤘던거 같은데요, 클로즈 폼은 아니지만 컨벡스하다라는 내용을 다시 리 마인드 하게 되는 계기가 되었고 그 후에 뉴튼 메서드와 그래디언트의 비교를 확인을 할 수있는 좋은 세미나 였습니다:)]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170818095226]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[898]]></uid>
		<content_uid><![CDATA[348]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[딥러닝을 긍정적으로 바라보는 관점과 부정적으로 바라보는 관점의 차이를 알 수 있는 계기가 되었습니다. 성능이 좋은 알고리즘도 좋지만 먼저 왜 그것이 좋은지 생각해 보는 좋은 계기가 되었습니다. 일반화 성능은 트레이닝 에러의 일정 시점 이후에 반비례한다는 데이터 분석의 스테레오 테입을 깨는 논문 설명이었습니다. 자비스 짱]]></content>
		<like><![CDATA[1]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[1]]></vote>
		<created><![CDATA[20170823201316]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[899]]></uid>
		<content_uid><![CDATA[348]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[4번 논문의 경우 ResNet을 공부하면서 가장 많이 느꼈던 부분이었습니다. 152층이 이미지넷 데이터 학습에서 제일 좋은 성능을 보여주었으며 1000층까지 쌓아보았더니 오히려 정확도가 떨어진다는 논문 내용이 있었는데 처음 공부할 때 이게 데이터가 부족하다보니 이런 현상이 발생하는게 아닌가 하는 생각을 했었습니다. 
1번 논문의 실험 결과들을 보며 저자가 연구에 엄청 많은 노력을 쏟았다는 것을 알 수 있었습니다. 수많은 실험을 통해서 우리가 딥러닝에 대해 한번 더 생각해 보게 해주는 좋은 내용이었습니다. 좋은 발표 감사합니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170823204859]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[900]]></uid>
		<content_uid><![CDATA[346]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[준홍이형 댓글의 마지막줄이 와닿는데요, 최근 딥러닝이 유행하며 매직박스처럼 어떤 함수 f에 input을 넣으면 원하는 형태가 딱 나오길 바라는 사람들이 많은 것 같습니다. 물론, 잘 모르시기에, 그리고 거품이 잔뜩 껴있기에 그럴 것이라고 생각되는데요, 과학적 방법론을 통해 어떤 현상을 설명할 때 보통 통계적으로 검정을 한 뒤 통계적으로 확인한 결과 유의하며, 어떠한 의미를 갖는다 라고 얘기를 합니다. 그런데 이게 진짜 과학적인 것인지, 혹은 내 주장을 다른사람들에게 설득하기 위해 대조군에 대한 실험을 빈약하게 한 것은 아닌지 분명히 짚고 넘어갈 부분일 듯 합니다. 본 세미나에서는 텍스트 분류기에 대한 이야기를 했는데, 사람이 하던 시절에 비해 현재 기술이 많이 발전함을 느꼈고, 그 속에서 좀 더 좋은 모형을, 방법론을 개발하기 위해 힘써야겠다는 생각을 했습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170824095057]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[901]]></uid>
		<content_uid><![CDATA[347]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[GAN을 최근에 스터디를 하며 여러 버젼에 대해 알아가고 있는데, 사실 어디다가 사용해야될지 모르기에 응용하기가 쉽지 않은 것이 사실인 것 같습니다. 조금이라도 더 선명하고, 남들이 못했던(예술 작품 생성) 것을 하려고 하는데, 결국에는 논문에서 우리 사진이 더 선명하다라고 하는 뿐이었습니다. unsupervised learning을 이용해 sample을 generate하고, 이를 통해 network의 feature를 좀 더 잘 학습시킬수 있는 방향으로든, deep clustering이든 좋은 결과물이 나오길 기대해봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170824095643]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[902]]></uid>
		<content_uid><![CDATA[348]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[우리 연구실에서도 딥러닝을 많은 사람들이 관심있어 하는데, 본 세미나는 정말 많은 것을 배울 수 있는 시간이었다고 생각합니다. 딥러닝 특성상 overfitting이 일어날 수 밖에 없고, 이를 잡고 일반화 성능을 끌어 올리기 위해 많은 기법들(BN, dropout, norm, early stopping, ...)을 이용하는 것으로 알고있습니다. 본 세미나 강의를 통해 어느정도 그럴것이라 생각은 했지만, 충격적이었던 것은 구글이 내부적으로 가진 데이터와 컴퓨팅 파워가 진짜... 어마어마한 수준을 넘어섰다는 것입니다. 그러기에 그렇게 좋은 성능을 낼 수 있었던게 아닐까 생각했습니다. 그런데도 최근에 중국이 상위권을 휩쓰는 것을 보며 중국의 힘을 느낄 수 있는 시간이었습니다. 아무튼 본 세미나를 통해 deep learning이 데이터를 마냥 외우는 것이 아니고, task에 맞는 feature를 적절한 구조를 통해 random label이어도 잘 학습할 수 있으며, 많은 데이터가 있으면 최고다 라는 정도로 간추려 이해했습니다. 준비하느라 정말 고생 많이 했을 듯 합니다. 수고수고]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170824101325]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[903]]></uid>
		<content_uid><![CDATA[352]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[기계번역 분야에서 사용될 방법론에 대한 설명을 들을 수 있는 세미나였습니다. beam search는 맨 아래 줄을 보니 동화형 발표한 것이 맞는거군요 기존에 잘못 알고있었던 듯 합니다. 요새 Deep NLP가 계속 겹치는 내용만 다뤄서 복습 차원에서 공부하고 있었는데, 이제 슬슬 응용사례들이 나오는 듯 해서 긴장이 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170824103317]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[904]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[발표한 내용을 아주 자세하게 정리해서 올려주셨네요. 이 후기 글을 읽으면서 배웠던 내용을 복습할 수 있어 아주 유익합니다. 블랙박스 모델로 불리우는 인공신경망 기반의 모델의 작동 과정에서 개선점을 찾으려는 최근 시도들이 아주 재미있습니다. Attention을 RNN 연구에서 Gate 이후로 가장 주목해야할 방법이라고 소개한 글을 본 적이 있습니다. 그 때는 와닫지 않았지만 Attention을 자세히 살펴보니 앞으로 가능성이 매우 큰 방법이라는 생각이 듭니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170828161259]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[905]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[연구실에서 Deep learning을 데이터에 적용하면서 자주 등장하는 것 중 하나가 Attention이라고 생각하는데요, 이번 기회를 통해 이것이 어떤 맥락에서 출발했고, 어떤 이점이 있는지 까지 파악할 수 있는 시간이었습니다. 전에 준홍이형에게 전해듣기로는 attention이라는 개념을 만든 사람이 학부생이라는 얘기를 들었습니다. 배운 만큼 보일텐데, 저 학부생은 얼마나 많은 것을 이미 알고 있을지 가늠조차 안됩니다. 지난번에 스터디에서는 image captioning 관련해서 image의 feature map을 attention하는 것을 들었었는데, 기본적인 attention에서 많은 부분이 응용되고, 파생되어 변화되었다고 생각됩니다. 얼른 괜찮은 형태를 고민해서 적용해볼 날이 오길 기대해봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170829211657]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[906]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[RNN기반을 학습하다보면 아직까지는 속도가 극악이라는 것을 알게 됩니다. 실제로 번역 같은 경우 sequence가 길어지면 PC를 사용하는 입장에서는 웃픈 상황이 발생합니다. 이 상황에서 좀더 향상된 속도를 바랄 수 있는 방법으로 개인적으로 청취하였었습니다.  attention논문은 제가 알기로 15~16년도에 정말 많은 논문이 나온것으로 알고 있습니다. 그리고 Text input의 Image attention의 가장큰 문제점에 관하여 예시를 들어주었습니다. 실제로 메커니즘 자체는 문맥에서 나오는 최소 kernel사이즈 크기의 가장 distinct한 면적을 찾는 문제로 바뀌게 됩니다. 최적화 입장에서는 비슷한 kernel에 할당할 수 밖에 없는 문제가 되어 버립니다. 이 같은 경우 메일 아날로그 시계만 시계로 보여주다가 전자 시계를 넣으면 전혀 다른것으로 예측하게 됩니다. (대부분이 아날로그 시계라면) 이런점이 현재 라벨의 실제 prior에 맞는 것을 학습한다고 하더라도 특성 사람이 원하는 output을 뽑는 문제를 어떻게 해결해 나갈지 앞으로의 연구들이 궁금합니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170830213957]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[907]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[NLP 분야에서 가장 잘 모르고 있던 attention부분이라 열심히 들었지만 완벽하게 이해는 다 하지 못했습니다. 아예 어떤 방식으로 작동하는지도 모르고 있었는데, linear, non-linear 등 여러 방식의 attention 방법들을 들으면서 공부할 수 있는 좋은 기회였습니다. 특히 지난 학기 비정형 데이터 분석 수업시간에 진행했던 프로젝트랑 비교해보니 차이점이 상당히 재미있게 느껴졌습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170907210329]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[908]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[DenseNet을 이름만 들어보고 어떤 구조인지 모르고 있었는데 정말 쉽고 재미있게 잘 설명해주셔서 이해하는데 큰 도움이 되었습니다. ResNet의 skip-connection을 블럭 전체로 확장시킨다는 아이디어가 어찌보면 간단하면서도 이렇게 큰 성능향상을 불러올 수 있다는게 놀라웠습니다. 요즈음 네트워크   구조를 공부하면서 느끼는 점이지만 "획기적인" 아이디어가 나오지 않고 있습니다. 구조 내에서 조금씩 기술적인 변화를 주면서 에러를 낮춰가다가 DenseNet이 구조 전체에 나름 큰 변화를 준 듯 합니다. 앞으로 비전 분야에서 더 획기적인 아이디어가 나오기를 기대해봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170908133503]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[909]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[컨볼루션의 최신 기법을 발표해주셔서 감사합니다. 발표중에 말한대로 그래디언트를 직접적으로 전달하는데 좋은 구조였다는 생각이 들었습니다. 트렌드를 볼 때, 처음에는 복잡한 방법으로 성능을 크게 올리고 이후 쉬우면서 더 좋은 성능을 내는 구조들이 나오는 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170908150943]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[910]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[이미지는 처음에 Raw input이 matrix로 이루어져 있는 것, 이것에 대한 Feature를 생성하는 법을 이해하는 것이 쉽지 않았으며, Text에 있어서는 One-hot encoding과 이후 임베딩하는 것이 낯설었습니다.
음성 신호에 대한 것도 마찬가지인 것 같습니다. Raw input도 있고, 이로부터 feature를 추출하는 과정이 있습니다.
이것은 발표자가 언급한대로 굉장히 낯설었습니다. 음성신호가 어떻게 이루어져있는지, 이를 어떻게 변환하는지는 전혀 아는바가 없습니다. 그래서 더 어렵다고 느꼈습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170908151305]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[911]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[어텐션은 굉장히 흥미로운 발상인 것 같습니다. 딥러닝의 단점 중 하나인 해석이 불가능하다는 것을 약하게나마 해결합니다. RNN에만 사용되는 것은 아니지만, 어텐션을 이용하면 RNN으로 생성되는 feature를 aggregation하여 fixed size input을 갖는 모델의 input을 만들 수 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170908151621]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[912]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[음성은 아직 해본적이 없어서 듣는것에 대하여 전부 이해하기는 쉽지 않았습니다. 하지만 raw data transfrom이후에서는 다른 분야와 크게 다르지 않다고 생각합니다.  요즘은 분야의 경계가 개인적으로는 거의 의미가 점점 없어지는거 같습니다. 발표하였던 atrous convolution이 원래는 오디오에서 쓰이던 것을 이미지로 가지고 와서 더 좋은 성능이 나온것처럼 여러가지 데이터를 다뤄보면서 언젠가는 음성을 다뤄볼 기회가 있으면 다양한 관점으로 볼 수 있지 않을까 생각합니다. HMM의 경우 요즘 sota로 쓰이는 경우가 있는지도 궁금합니다. 그리고 제일 궁금한 처음에 LPC변환을 하지 않는것과 하는것이 별 차이가 없다고 언급하였는데 그렇게 되면 왜 변환을 하는것인지도 나중에 궁금하여 한번 찾아 보려고 합니다. 발표하느라 수고 하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170908205201]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[913]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[최근 음성비서 상품을 AI에 관심을 둔 회사라면 다들 만들어보려 하는 것 같습니다. 아마존 에코에서 시작하여, 국내에서는 SKT의 누구, KT의 기가지니에 이어 오늘은 카카오미니를 출시한다고 기사에서 확인했습니다. 그 기반이 되는 것중 하나가 음성에 대한 처리, 자연어에 대한처리, 그리고 task에 대한 modeling이 될 것 같습니다. 그런 관점에서 음성 처리가 앞으로는 필요한 부분이 되리라 생각이 듭니다. STT와 TTS 모두 중요한데, 그 기반을 잡는 용어 정리로서의 세미나로는 충분했다 생각이 됩니다. 발표자료가 올드하여 충격적인 부분이 있었지만, 일반적인 젊은이의 느낌을 벗어난 약간의 충격으로 재미있는 발표이기도 했습니다. 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170911200936]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[914]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[ResNet까지 구조의 발전을 살펴보는 세미나, 자료, 발표는 여러 차례 연구실 내부, 외부에서 확인 가능했습니다. 이후 나온 DenseNet을 이름만 듣고 기억하고 있었는데, 역시 준홍이형 답게 꼼꼼하게 리뷰해주어서 이해가 쏙쏙 되었습니다. 물론 따로 읽어보긴 해야겠지만, 개인적으로 공부할 때 큰 도움이 될것 같습니다. 뒤의 반절은 발표를 하지 못했는데, 나중에 기회가 된다면 꼭 듣고싶습니다. 개인적으로, DenseNet의 경우 하나의 크기에 대해서만 쭉 concat 하는 방식이었는데, 뭔가 여기서 변화를 줄 수 있지 않을까 하는 생각을 세미나를 들으며 했습니다. 발표 준비 수고하셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170911201457]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[915]]></uid>
		<content_uid><![CDATA[348]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[지금까지의 deep learning 이슈들에 대해서 실험적으로 답을 보여준 논문들이라 생각됩니다. 어떠한 현상을 설명하기 위해서 실험 설계를 어떤식으로 해야하는지를 생각해 볼 수 있었던 시간이었습니다. 컴퓨팅 파워가 엄청나겠지만 그럼에도 이러한 실험들을 했다는 것에 대해서 대단하다 생각하였고 이러한 흐름을 가지고 여러 논문을 엮어 좋은은 발표를 해 준 재선이에게 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170919150435]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[916]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[논문 발표 세미나의 목적에 아주 부합하는 발표였습니다. 기계 학습 분야에 논문이 쏟아져 나오고 있는 상황에서 모든 주목 받는 최신 논문을 다 읽어 보는 것은 현실적으로 어려움이 있습니다. 이럴때 논문 발표 세미나를 통해 다른 연구원들이 흥미로운 논문을 요약해서 알려주는 것이 최신 연구 경향을 따라가는데 아주 큰 도움이 되고 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170920113509]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[917]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[음성 관련 연구는 확장 가능성이 아주 크다고 생각합니다. 활용 방안에서 다양한 곳에 쓰일 수 있는 것만이 아니라, 이론적으로 여러 분야에 파급될 수 있습니다. 우리가 음성을 파장으로 처리하기 때문에, 음성 분야의 연구가 다른 파동을 기본 단위로 다루는 모든 신호 처리 분야에 영향을 줄 수 있을 것이라 생각합니다. 앞으로의 연구 동향이 기대됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170920113811]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[918]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[딥마인드 연구진들이 end-to-end방식으로 문제를 정의하기 위해 노력하는 모습이 흥미롭습니다. 예전부터 들었던 생각인데 왜 end-to-end방식이어야 하는걸까요? 기계 학습 함수를 해석할 수 있는 수리적 도구가 있으면 조금 더 정교하게 분석이 가능할 것 같습니다. 지금은 end-to-end가 당연히
 추구되어야 할 덕목으로 여겨지고 있는 것 같습니다.  하지만 저는 여기에 대한 뒷받침이 부족하다고 생각되어 아쉬움이 남습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170920120448]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[919]]></uid>
		<content_uid><![CDATA[358]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Wasserstein GAN은 저도 예전에 흥미를 가지고 봤던 논문이라 이번 발표가 참 재미있었습니다. 볼 때마다 느끼는 거지만 참 어려운 논문입니다. 기계학습을 이론적으로 분석하기 위한 시도에 제가 관심이 많은데요, 관련 내용을 이해하기 위해선 측도이론이 필수적이더군요. 다음 학기 부터는 실해석학을 열심히 공부해볼 계획입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170920152750]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[920]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[많은 분야가 그렇듯이 TTS 분야도 그 흐름이 rule base -&gt; prob base -&gt; model base로 발전하는 것을 소개를 해주었고, 오랜만에 PGM도 접하는 세미나가 되었습니다. TTS는 언뜻 보기엔 그냥 정의된 내용대로 소리를 만들면 되지 않나 싶지만, 구분, 연음처리, 감성 까지 많은 부분이 미해결된 분야입니다. 최근 음성비서가 나오면서 STT, TTS가 필요한 기술로 각광받고 있는데, 기계적인 비서가 아닌, 진짜 사람과 대화하듯 말할 수 있는 기술을 위해서는 STT, TTS가 중요하겠습니다. 얼핏 듣기로는 텍스트에 있는 이모티콘 등을 이용해서 음성에 감성을 덧입힌다고 했던 것 같은데, 여러 응용 기술들이 있을 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921131132]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[921]]></uid>
		<content_uid><![CDATA[352]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[우선 conditional의 정의에 대하여 새로이 받아들일 수 있는 시간이었습니다. 또한, encoder와 decoder의 구조를 어떻게 설정하냐에따라 달라지는 점들을 확인 할 수 있었습니다. 명준이도 개인연구로 진행 중이지만, 기계번역 분야는 여전히 연구와 발전 가능성이 많은 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921131208]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[922]]></uid>
		<content_uid><![CDATA[358]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[기존의 vanilla GAN에서 사용된 KL, JS divergence 방법들이 목적을 달성하는데 어려움이 있어 새로운 metric을 제시하는 부분이 가장 인상깊었습니다. 무엇보다 해당 개념이 70년대에 이미 나온 논문에서 차용했다는 것을 보고 역시 옛것을 무시하면 안될 것 같다는 생각이 들었습니다. GAN은 아직 공부해본적이 없어 이해하는데 어려움이 있었기에 처음부터 차근차근 공부해봐야 할 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921132109]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[923]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[attention이라는 방법론을 통하여 DNN이 가지는 한계점을 어느정도 해소 할 수 있습니다. CNN을 넘어 RNN에까지 attention이 전파되는 속도는 정말 놀랍습니다. 또한 이런한 방법을 통하여 VQA, 챗봇, 또는 image captioning과 같은 분야의 performance를 높일 수 있으니 앞으로도 꾸준히 연구가 진행 될 것이라 생각됩니다. 마지막 주제인 attention의 종류는 아직 온전히 이해하지 못하여 따로 공부를 해볼 생각입니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921132354]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[924]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[TTS 분야의 변화 흐름을 처음부터 자세히 설명해주셨습니다. 대세를 따라 end-to-end 방법으로 문제를 해결하기 위해 많은 노력을 했는데 해동형님과 마찬가지로 굳이 end-to-end로 모델을 학습해야 하는 것에 대해는 약간 의문점이 있습니다. 저의 개인 연구의 목표중에 하나도 end-to-end가 무조건 좋은 것이 아니라 목적에 따라  phase로 나누어서 모델을 구축하는 것이 더 효과가 좋다는 것을 보이는 것이기 때문입니다. End-to-end가 항상 좋은것이 아니다 라는 것을 보이는 것도 재미있는 연구 주제가 될 수 있을 것이라 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921132617]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[925]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[DenseNet은 ResNet의 구조 이후에 오랜만에 보는 앙상블 구조가 아닌 새로운 구조의 네트워크 입니다. ResNet의 발전된 형태를 가지고 있는데, 언제나처럼 참 내놓고 보면 간단한 아이디어이지만 그 간단한 생각을 떠올리기 위해서는 엄청난 내공을 가지고 있어야 하는 것 같다는 생각을 하였습니다. 더 이상의 구조 변화는 힘들 것 같다는 생각을 하고 있었는데,  본 논문을 읽고 나서는 또 다른 네트워크도 나올 수 있겠구나 라고 생각이 변하였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921133042]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[926]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[QA 분야도 정말 재미있어 보이지만, 세미나 시간에 얘기했듯이 한국어 QA 데이터셋이 없는 상황입니다. 외국의 경우에는 이렇게 많으 데이터 셋을 이용해서 여러 분석을 할 수 있다는 것이 마냥 부럽기만 합니다. 만약 번역기에 돌려서 학습한다 해도 번역기에 dependent 하겠지요. Answer sentence selection에서 binary 문제로 바꾸는 발상은 충분히 멋있었다 생각합니다. 어려운 문제를 쉽게 바꿔서 풀 수 있다면 더할나위 없을 듯 합니다. 마지막으로 발표자가 VQA에서 사진이 없다면? 이라는 가정을 하고 드랍쉽이 뭘 내렸을까요? 라는 질문을 했는데,  너무 재미있었습니다. 앞으로도 파이팅하길 바랍니다. 수고 많았습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921133343]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[927]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[Convolution network의 가장 최신 기법에 대해 설명해 주셨습니다. 텐서플로우로 예를 들어 설명해 주셔서 이해하는데 상당히 많은 도움이 되었습니다. 논문 전체를 꼼꼼히 리뷰해주셔서 정말 감사합니다. Concatenation이라는 상당히 간단한 방법으로도 높은 성능의 향상을 보이는 것이 놀라웠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921133525]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[928]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[매우 새로웠지만 그만큼 어려운 세미나였습니다. 아직까지 연구실에서 음성 데이터를 다루어 본적이 없는 것으로 알고 있습니다. 재선이가 언급한 것처럼 음성 raw data에서 feature를 추출하는 방법부터 이해를 해야할 것 같습니다. 발표자료는 조금 더 fancy해진다면 더할나위 없을 것 같습니다다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921134340]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[929]]></uid>
		<content_uid><![CDATA[358]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[GAN의 문제점을 해결하고자 wasserstein을 사용하였습니다. WGAN 논문이 30쪽이 넘고 수식도 엄청 많이 나오는 것으로 알고있는데 발표자가 잘 정리를 해줘서 논문을 읽는 것 보다 훨씬 많은 것을 이해할 수 있었습니다. 논문은 너무 어렵습니다.........]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921163057]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[930]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[RCNN계열의 논문들을 공부할 때 본 것은 end-to-end방식이 네트워크 전체를 하나의 loss function으로 학습 할 수 있기 때문에 좋다는 것이었습니다. 어떤 하나의 목적을 가진 상황이라면 구조를 여러가지로 나누는 것 보다 하나로 통합하는 것이 더 좋지 않을까라는 개인적인 생각입니다. 또한 발표를 들으면서 본인이 공부했던 PGM지식으로 강의의 notation을 해석하는 것과 다른 점을 해석하려는 모습이 좋았습니다. 공부가 부족한 분야라 많은 것을 이해하지는 못하였지만 상당히 흥미로운 세미나였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921163843]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[931]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[우선 발표자의 질문에 답을 했는데 틀려서 몹시 마음이 아픕니다.... 왜 리버가 내렸을까요... 본 연구에서도 데이터 셋을 만들었던 것 처럼, 한국어 데이터 셋을 어떤식으로 구성할지 고민해보는게 연구의 시작점이 될 수 있지 않을까 생각했습니다. 또한 질문에 대한 대답으로 80%가 넘게 한 단어만 사용한다는 것을 보고 질문을 답에 맞춰서 하는게 아닌가 라는 생각을 해보았습니다. 자연스러운 질문에 자연스러운 대답이 나오려면 확실히 text generation과 연계가 되어야겠다고 생각하였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921164246]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[932]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[먼저 발표 자료 재미있었습니다. 또한 하나하나 논문을 읽어보고 critic한 부분에 대하여서 감사드립니다. 해당 critic이 모여서 개인적으로는 지식이 증가한다고 생각합니다. NLP분야는 정말 쉽지 않은 분야라고 생각합니다. 
[1] 텍스트로써 텍스트를 말하는 환경을 100% 맞추는것은 사람과 학습 방식이 다르다고 생각하기 때문입니다. 사람은 오감을 학습하고 눈으로 본 텍스트를 혹은 말하는 텍스트를 글귀를 보고 생각합니다. 현재 Text를 보고 Text를 맞추는것은 오감중에 몇가지를 지우고 빠르게 backpropagation으로 계산식을 만드는과정이라고 생각합니다.
[2] 이러한 환경하에서 QnA가 나왔으며 이러한 데이터들이 모여서 점점 좋아질거라고 생각합니다. 발표자가 정말 좋은 예시를 들어주었는데, 탬플러 드라군 이 아닌 리버라는 답변은 text문맥이외에 다른 오감을 다시 텍스트로 녹이는 과정에서 생기는 답안이라고 생각합니다. 앞으로 이러한 문제에서 데이터가 쌓이고 보는 관점이 달라지면 다양한 input에 있는 network에서 하나의 답으로 가는 universal classifier가 언젠가는 만들어질 거라고 들으면서 생각했습니다.
발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921213225]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[933]]></uid>
		<content_uid><![CDATA[358]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[요즘 GAN GAN하면서도 쓰일 분야를 찾지 못하여서 공부를 미루고 있었는데 간단하게 자세히 정리해주어서 고마웠습니다. 개인적으로 질문드렸던 것처럼 Weight를 Clipping하는것이 상당한 효과를 경험을 하였고 많은 논문에서 제시합니다. 따라서 Distance를 기존것을 쓰고 clipping을 하였을때도 효과가 올라갈텐데 이 부분에서 어느정도의 성능이 나오는지 찾아보아야 되겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170921213443]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[935]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[재미있는 발표였습니다. NLP 분야에서 가장 어려운 분야 중 하낙 QnA가 아닌가 합니다. 주어진 텍스트를 이해하고 거기에 알맞게 답을해야 하는데, 컴퓨터가 과연 이러한 문제를 제대로 해낼 수 있을까 하는 근본적인 회의감도 느껴집니다. 현재의 텍스트 분석의 핵심적인 역할을 하는 빈도수 만으로 언어가 가지고 있는 역할을 제대로 포착할 수 있을지 궁금합니다. 이런 어려운 문제에 어떻게든 과학적인 방식으로 문제를 해결해 보고자 시도하는 연구자들에게 박수를 보냅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170926154818]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[936]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[918]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[이유는 여러가지가 있을텐데요. 첫번째로는 프로세스의 단순화일 것 같습니다. 딥러닝이 어렵다고 느끼는 사람들도 있겠지만, 기존의 연구들에 비하면 훨씬 쉬우면서도 좋은 성능을 내고 있다는 생각을 합니다. 예를들면 딥러닝 이전에는 이미지를 분석하기 위해 CRF 등의 PGM이론이나 HoG와 Optical Flow와 같은 각종 처리 기법을 모두 알아야 했었죠. 하지만 지금은 CNN으로 대부분 해결이 됩니다. 마찬가지로 TTS도 기존에는 GMM과 HMM을 조합하여 사용했다고 합니다. 하지만 딥러닝 하나로 대체하면서 굉장히 태스크를 진행하는 데에 있어서 단순화되었고, 쉬워졌죠.
다음으로는 실제로 더 나은 솔루션을 제공할 것이라고 생각합니다. Step by step으로 최적화를 이루게 되면, 최종적인 목표(분류, 회귀, 생성 등)에 대하여 가장 좋은 솔루션을 제공하지 않을 것으로 생각이 됩니다. 각각의 스텝에서의 최적이 최종 솔루션에 대한 최적을 보장하지 않을 것이기 때문이죠. 프로세스의 단순화보다는 성능 향상 측면에서 더 큰 이유가 있다고 생각합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170927013431]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[937]]></uid>
		<content_uid><![CDATA[358]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[GAN을 여러번 듣게 되지만 직접 자세히 들여보지 않아 여전히 어려운 것 같습니다. 이런 논문들을 보면 신기한 점이 여럿 있습니다. 먼저 기존의 틀을 깬다는 것입니다. 딥러닝에서 Loss는 크로스 엔트로피, KL Divergence와 같은 것은 너무나 당연했습니다. W-GAN은 이를 깼고, 새로운 loss를 이용하여 학습하였고, 성공적임을 입증하였습니다. 두번째로는 이를 보여주는 예시를 잘 만드는 것 같습니다. 굉장히 복잡하고 어려운 개념이 들어가지만, 이 개념이 어째서 좋은지 (어떻게 보면 지나치게) 단순한 예를 잘 보여주는 것 같습니다. 이 논문에서는 theta를 이용한 예시가 참 인상깊었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170927133857]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[938]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[쉽지 않지만 흥미로운 주제를 이용하여 재미있는 예시를 들어준 발표였습니다. 딥러닝 계열의 연구 중 "이게 될까?"라는 생각이 드는 연구들이 여럿 있는 것 같습니다. 지난 번에 창엽이형이 발표한 memory attention이 그랬고, QnA도 그렇습니다. 한편으로는 된다는게 놀랍고, 이를 성공적으로 끌어내기 위해서는 어느정도의 데이터를 어느정도의 하드웨어에 입력해야하는지에 대한 궁금증도 있습니다.
그렇게 찾아본 결과 VQA 데이터는 공개되어있다고 합니다. 자세히는 보지 않았지만 학습 데이터를 기준으로 400만개 이상의 데이터를 사용하는 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20170927134732]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[939]]></uid>
		<content_uid><![CDATA[352]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[기존의 Unconditional 한 방법들과는 다르게 조건부 문맥 X(문장, Label, 이미지, 문서, 음성, 질문 등)이 주어지면 그것을 바탕으로 번역, 요약, 자막 생성, 음성 인식등에 사용된다는 접근 방식이 매우 흥미로웠습니다. 번역과정에서 필요한 Encoder-Decoder 모델, beam search 방법들에 대해서도 새로 알게 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171007204836]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[940]]></uid>
		<content_uid><![CDATA[347]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[저역시 상기 작성자와 같은 의견입니다.
GAN에 대해서 다양한 측면에서 이를 개선하려고 하는 노력이 개선되고 있습니다.
실제로 성공적인 응용사례가 있다면 그 domain에 적합한 개선 방안도 더욱 구체적이지 않을까 생각됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008022946]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[941]]></uid>
		<content_uid><![CDATA[346]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[현재 저희의 주된 관심사가 Deep learning 이기때문에, 주된 토픽인 딥러닝을 맹신하거나 과신하는 경향이 있습니다. 하지만 이번 세미나에서 소개한 기본적인 machine learning 방법론들 역시 때로는 더 합리적이고 적합한 경우도, 있다고 생각합니다. 기존 기계학습에 대한 학습도 놓쳐서는 안될 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008023641]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[942]]></uid>
		<content_uid><![CDATA[348]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[딥러닝이 성공한 이유  중 하나는 방대한양의 데이터와 이를 처리가능한 기술이라 생각됩니다.  그런 의미에서 3번째 Deep Learning is Robust to Massive Label Noise 논문은 재미있는 논문이라 저도 기억이 납니다.  딥러닝의 네트워크 구조에 대해서 다양한 연구가 진행되는 반면에, 이를 학습하는 전체적인 실험설계에 대한 다양한 접근도 흥미로운 연구주제가 될것 같습니다. 또한 이를 통해 효율적인 딥러닝 학습 프레임도 구성하여 시간대비 모델 학습 수렴에 대한 연구분야도 재미있는 연구주제가 될 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008024049]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[943]]></uid>
		<content_uid><![CDATA[352]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[Conditional Language Models에 대해서 처음 접하는 분야라 흥미로운 lecture였습니다.
Beam search의 경우, 다양한 NLP task(POS tagging, parser, translation 등)에서 활용되지만, 개인적으로 이부분에 대한 정확한 이해가 부족하다고 생각죕니다. 개인적으로 학습을 통해 이에대해서 정리하고 시간이 허락한다면 연구실 구성원들과 이를 공유하고 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008024353]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[944]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[attention에 대해서 다양한 해석이 가능할 것 같습니다. 일반적으로 attention이란 학습시 좀더 주의를 깊게 학습에 반영한다는 개념에서 나와 다양한 방식으로 이를 반영하고 있습니다. 하지만 앞서 김준홍 학생이 언급한만큼 atention논문은 15~16년도에 정말 많은 시도가 있었습니다. 이러한 다양한 attention mechanism을 동일한 조건에서 비교하여 각각의 장단점을 비교해보는 것도 좋은 연구주제가 될수 있지 않을까 생각됩니다. 감사합니다 ^^]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008024711]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[945]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[Dense Network, 최근에 읽은 Deep Networks with Stochastic Depth 논문의 연장선상에 있는 연구라 생각됩니다. CNN 구조에 대한 생각의 변화를 통해서 이를 연구의 성과물로 도출하기까지 직관적으로 확 와닿는 연구라 생각됩니다. 개인적으로 때때로 배우는데 급급해서 이에 대한 다양하고깊이있게 생각하는 시간을 무심코 지나가버리기 일수입니다. 앞으로는 논문을 읽어 나갈때 시간에
 쫒기지 않고, 머리속으로 다양한 상상을 해가며 논문을 읽고자 노력하겠습니다.
감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008025316]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[946]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[음성학에 대해서는 문외한이라 매우 신선하게 들은 세미나시간이었습니다.
음성관련해서는 다양한 tutorial들이 부족하고 음성 관련 데이터 처리 지식도 부족하여 이를 실제로 체험해보지 못한것이 현실입니다. 실제로 부딪혀서 알아가는 것만큼 좋은 경험도 없을것이라 생각됩니다. 시간이 허락하고 기회가 된다면 실제로 음성에서 spectrum 추출까지의 과정을 통해 rawdata 추출부터 네트워크롤 통한 학습도 해보고 싶은 분야입니다.
감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008025621]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[947]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[최근 구글 및 아마존, 네이버, 카카오등  다양한 음성인식 스피커의 경쟁구도가 진행되면서,  이부분에 대한 관심도 커져가는 상황이라 생각됩니다. 이미지에서의 CNN을 텍스트에 활용하면서 다양한 연구가 진행된 것과 같이 TTS 분야에서도 다른 분야의 다양한 approach의 장점을 계승하여 접목시키면 좋은 성과가 나올것이라 생각됩니다.
좋은 발표 감사드립니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008025938]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[948]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[Q&amp;A 는 크게 selection과 generation으로 나누어 생각해볼수 있을 것 같습니다. 전자의 경우 주어진 문장과 Source Text로부터 적합한 query를 통해 answer keyword를 선택하거나 주어진 답안중 가장 similarity가 높은 answer sentence를 선택하는 것과, 후자의 경우 Seq-to-Seq를 통해 적합한 답안 문장을 작성하는 것이 될것이라 생각됩니다. 후자의 경우 sentence comprehension과 연결되기도 합니다. IBM의 경우 watson이후로 이분야에 대해서는 많은 연구를 진행했고 실제로 선봉장이라 생각됩니다. 하지만 한국어의 경우, Q&amp;A 에 대한 학습데이터가 부족하여 실제로 적용해보고싶지만 현실적인 문제에 부딧히기도 합니다. 한번쯤 다양한 시도를 통해서 꼭 연구해보고 싶은 분야입니다. 
좋은 발표 감사드립니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008030643]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[949]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[NLP 관점에서 정해진 몇 개 정답 중 적절한 것을 고르는 classification 문제는 text를 생성해서 답을 주는 문제보다는 상대적으로 쉬울 것이라고 생각합니다. RNN이 과연 문맥적 의미를 이해하는 알고리즘인가에 대해서는 아직도 의문점을 가지고 있지만 실제 많은 연구 결과물들이 가능함을 보여주고 또 더 많은 연구 방향이 있음을 말해주기에 꾸준히 공부할 필요를 느낍니다. 다만 논문을 볼 때마다 아쉬운 점은 한국어 데이터셋이 부족한 점입니다. 해외는 데이터셋을 구성하는 것이 하나의 논문이 될 수 있고 또 그렇게 공유된 데이터셋이 많기에 더욱 연구가 박차를 가할 수 있다고 봅니다.
발표자 분께서는 발표 준비하느라 고생하셨고 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008140733]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[950]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[발표자료는 새로운 양식으로 수정하고 보충설명이 필요한 내용 추가해서 업로드 할 계획입니다.
좋은 후기 남겨주셔서 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008145116]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[951]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Attention model 에 대해서 명칭만 들어보고 잘 알지 못하는 상태였는데, 발표를 통해서 기계 번역과 이미지 캡션 문제에 대해서 attention model이 어떻게 적용되는지 이해 할 수 있었습니다. 다른 연구 주제에서도 유용하게 쓰일 수 있는 model이라는 생각이 들었습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008145524]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[952]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[NLP에서의 QA 문제를 조금씩 공부하면서 점점 너무 어렵게 느껴집니다.. Memory Network에서도 해당 문장과 질문과의 유사성을 찾고자 벡터 자신,내적, concat으로 유사도, 그리고 정답도 같이 학습합니다. 전에 봤던 논문에서도 결국 NLP의 감성 분류 문제, Aspect Term Extraction 문제도 결국 QA 형태로 바꿀 수 있는 데, 혼자서 테스트를 하면 할 수록 잘 안된다(?)라는 느낌을 많이 받고 있습니다. Supervised 방식으로 학습하려고 해도 Label이 있는 도메인이 너무 한정적이다 보니 다른 도메인으로 확장하려면  수많은 연구자들의 노력이 필요할 것 같습니다. 발표자가 준비를 많이 했다는 느낌을 많이 받았습니다. 발표 준비하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008191639]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[953]]></uid>
		<content_uid><![CDATA[353]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[딥러닝 NLP답게 CNN, RNN, 병합방식을 추구하는 접근법으로 문제를 해결하는 전형적인 틀을 가지고 있었습니다. 뿐만 아니라 요새 인기가 있는  attention을 적용하여 문제를 해결하는 점이 앞으로 이것을 공부해야 할 것 같습니다. 학습에 주요한 부분이 어딘지 보는 것은 정말 흥미로운 연구분야인것 같습니다. soft attention은 명확한 설루션이 존재하여 SGD로 학습하기 용이하지만,  hard attention은 샘플링기반으로 분포를 추정하는것 같아 앞으로 통계학지식을 배양해야 할것 같습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008234754]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[954]]></uid>
		<content_uid><![CDATA[354]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[개인적으로 이미지 분류는 발전할께 거의 없는것 같습니다. 이 덴스 컨볼루션도 병합 학습 구조를 바꾼 개념은 효율적인 학습과정을 선보였지만 성능에 있어서 Resnet이후 그리 큰 변화는 없는 것 같습니다. 어떻게 보면 같은 맥락의 학습방법인것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008235032]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[955]]></uid>
		<content_uid><![CDATA[356]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[음성학 관련해서 공부할 기회가 없는데 자세히 알 수 있는 좋은 기회가 된 것 같습니다. 개인적으로 관심이 있는 학문문야 입니다. 누가 이미 잘 개발한 알고리즘을 갔다 쓰는게 아니라, 아직까지 음성인지에 있어서 한계점을 가지고 많은 연구들을 하고 있고, 이는 연구하는 사람입장에서 동기를 자극 하는 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008235547]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[956]]></uid>
		<content_uid><![CDATA[358]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[GAN은 저희 연구분야에서 다루기 힘든 분야인것 분명한 사실인것 같습니다. 명확한 성능지표가 나타나지 않고 학습방법도 굉장히 수리적인 것 같고요. 하지만 무언가 얽매힌 틀에서 벗어나 새로운 시도를 해보는 것은 기존의 것을 새롭게 개발할 수 있는 시발점이 않을까 생각해봅니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171008235812]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[957]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[발표자가 세미나를 준비하는 과정을 보았는데 이분야는 굉장히 힘들고 어려운 분야인것 같습니다. 발전과정을 한번 되새겨 본다면 기존에는 히든마코프모델을 활용해서 음성인식했다고 하면은 현재는 딥러닝을 기반으로 하는 음성인식으로 넘어가는 것같습니다. 현재 zero-shot learning 또한 한번도 학습되지않는 쿼리 언어또한 학습하기 위해서 음성인식에 사용되는 것으로 알고 있는데 이런 분야도 공부해보는 게 현재 목표로 두고 있습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171009000642]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[958]]></uid>
		<content_uid><![CDATA[359]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Text to speech 의 전통적인 방법부터 model을 활용한 방법들에 대해서 배울 수 있었습니다. 문맥에 따라서 같은 text이더라도 다른 prosody(운율) 차이가 생길 수 있는데, 이러한 점을 어떻게 기존 연구들에서는 학습하였는지 궁금하여 참고문헌이나 관련논문들을 찾아서 공부해볼 계획입니다. 또한 TTS의 성능평가는 subjective하기 때문에 이를 측정할 수 있는 객관적인 지표가 중요하다고 생각됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171009000708]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[959]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[QA는 일반적은 많은 데이터를 요구하는 것 같습니다. 그래서 기업단위가 아닌 학교단위에서 연구하기 힘들텐데 발표자는 현재 이런 분야를 연구하고 있다는 점이 높이 살만할것 같습니다.  translation또한  여러 머신러닝 알고리즘이 많이 적용하는 것 처럼 다양한 선형연구들이 존재하는 것같고 많은 reivew 또한 필수적인 것 같습니다. 최근에 zero shot learning을 기계번역에 적용한 사례가 구글 리서치에 있는것 같은데 참조하시면 좋은 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171009001233]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[960]]></uid>
		<content_uid><![CDATA[360]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[Apple 사의 siri 서비스가 사용자의 질문에 대답을 하는 형태인 Question Answering 문제라고 볼 수 있는데, 사용자가 원하지 않는 대답이 나올 수 도 있고, 복잡한 질문에는 시스템이 이해하지 못하는 경우가 있어서 연구가 필요한 분야라고 생각이 됩니다. 발표전에는 관심이 없던 분야여서 모르고 있었는데, Reading comprehension, Answer sentence selection, Visual Question Answering 등으로 QA를 바라볼 수 있고, 발표를 통해서 딥러닝 구조를 통해서 QA문제를 해결하는 방법에 대해서 배울 수 있었습니다. 발표자의 많은 노력을 느낄 수 있었던 발표였습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171009002239]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[961]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[발표 재미있었습니다. 결국 최종 목적은 같은데 문제가 되는 지점인 Decoder의 Initial state에 대한 접근 방법 이었습니다. memory에 대한 접근법은 상당히 타당하다고 생각합니다. 여러가지 모델을 보면서 뒷쪽에 힘을 실을것인가 웨이트를 늘리더라도 전체를 가지고 가는게 좋은것인가 등의 고민이 있는 연구였습니다. 메모리 부분을 단순 stacking을 하는것이 아니라, weight를 학습하겠다는것도 인상 깊었습니다. 하지만, memory자체를 non-linear mapping function으로 쓰면 더 좋지 않을까라는 생각을 하였습니다. 이는 코딩해보면서 한번 시도해 봐야 되겠습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171011134635]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[962]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[개인적으로는 ROM에 대해서만 알고 있었는데 RAM 등 다양한 Attention model에 대해 설명해주셔서 감사합니다. 무엇보다 1학년때 배웠던 stack, queue, deque의 아이디어에서 착안하여 memory structure을 구성한 부분이 가장 흥미로웠습니다. 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171011174730]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[963]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[12]]></user_uid>
		<user_display><![CDATA[김형석]]></user_display>
		<content><![CDATA[Memory 개념을 도입해서 attention 매커니즘을 수행하는 방식에 따라 매우 다양한 연구가 진행되어 오고 있는것 같습니다. ROM 에서는  일반적으로 좋은 성능을 보여준다느 bi-directional LSTM 이 아닌 one way로 LSTM을 적용한 word-by-word 모형이었다는 점도 흥미로웠습니다. 추석 연휴기간 중에도 발표 준비하느라 수고하셨습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171013232846]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[964]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[Detail한 세부내용이 한번에 이해되기에 어려운 강좌였습니다. 논문을 보며 수식을 하나하나 따라가야 구체적으로 이해할 수 있을 것 같았습니다. 다만 전체적인 관점에서 Encoder-Decoder관계에서, Encoder가 생성해내는 고정된 길이의 벡터가 가질 수 있는 한계점을 느끼고 그를 극복하기 위한 방법으로써 memory를 사용한다는 점은 충분히 이해가 되었습니다. memory의 사용 방식이 점차 발전해가는 과정을 이번 세미나가 설명해 준 듯합니다. 관련된 연구를 진행할 때, 이 자료가 좋은 참고자료가 될 듯 합니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171015011336]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[965]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[태스크 자체부터 이해하기 어려웠습니다. 필요시 많은 공부가 필요할 것이라고 생각합니다. 한편으로는 Stack, Queue 등의 자료구조 형태에 영감을 받아서 모델을 설계하는 것이 인상적이였습니다. 그리고 이 역시 마찬가지로 "이런 모델이 학습이 잘 될까?"라는 생각이 들었습니다. 앞으로 또 어떤 모델들이 나올지 기대가 됩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171015233436]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[966]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[20]]></user_uid>
		<user_display><![CDATA[김 창엽]]></user_display>
		<content><![CDATA[이전에 발표했던 DMN에 대해서만 알고 있어서, 실제 메모리 처럼 ROM, RAM 으로 구성할 수 있다는 것을 처음 배웠습니다. 그 외 자료 구조에 대해서도 알기 쉽게 설명한 점이 인상깊었습니다. 결국 네트워크를 학습하면서 필요한 정보를 프로그램을 실행할 때 메모리 상의 스택에 끊임없이 쓰는 것처럼 필요한 정보를 기록한다면 훨씬 더 어려운 문제도 풀 수 있을 것 같습니다. 발표 준비 하시느라 고생 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171016000601]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[967]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[정리를 잘 해놓은 댓글인것 같습니다. 텍스트 분석을 일반적으로 자소단위로 볼것이냐 단어단위로 볼꺼라는 관점에서 시작할 수 있을 것 같은데 이 경우도 그렇고 일반적으로 단어 기반의 분석이 더 성능이 좋은 것 같습니다. 디코더 인코더를 가지는 굉장한 복잡한 구조인것 인것같은데 이 구조는 자료구조를 활용한 구조라는 점에서 다른 분야도 새롭게 시도해보는 점을 고려해봐야 겠습니다]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171016102138]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[968]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[Attention mechanisim을 컴퓨터의 메모리의 관점에서 생각한 것이 흥미로웠습니다. 이런 관점에서 생각해보니 attention이 중요한 부분에 weight를 준다는 것을 넘어 아주 다양하게 해석될 수 있는 것 같습니다. 이런 해석에서 영감을 받아 자료구조의 내용을 접목시켰고, 실제로도 성능향상을 이루었다고 하니 아주 대단하다는 생각이 듭니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171017131012]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[969]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[RNN의 한계점을 극복하기 위해서 Memory라는 개념을 도입한 내용과 자료구조 개념을 도입한 내용의 발표를 들었습니다. 수식이 복잡하여 바로 이해하기에는 어려웠지만 새로운 관점의 연구방법을 배울 수 있었고, 해당 강의 내용을 상세히 설명한 발표자료가 좋았습니다. 자연어 처리 이외의 분야에서 RNN을 사용할때도 고려해볼 만한 주제라 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171017132647]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[970]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[9]]></user_uid>
		<user_display><![CDATA[조 수현]]></user_display>
		<content><![CDATA[RAM과 attention을 함께 사용한다는 개념이 잘 와닿지 않았습니다. 이해하기 위해서는 더 공부해봐야 할 것 같습니다. memory를 이해를 위해 stack, queue 등의 개념들도 다시한번 복습할 필요가 있을 듯 합니다. 하지만 분명히 memory까지 고려하여 attention model을 구축한다는 점에서 다른 모델에 비해 장점은 있는 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171017135324]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[971]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[얼마전 페이스북 페이지를 통해 relational reasoning 논문이 한창 대단하다는 얘기를 들었습니다. 그래서 한번 시간내서 읽어야겠다 생각했지만 역시나 읽지 못했구요. 그런데 이번 기회를 통해 마치 읽은 것 같이 자세히 설명을 들었습니다. 아래에 깔려있는 feature extraction 부분은 CNN과 RNN cell이고, 윗단에서 어떻게 각 객체간의 relation을 task에 맞게 추출하느냐의 문제인데, pair-wise하게 vector를 concat하고, 최종 label을 이용해 산출하는 아이디어에서 정말 감탄했습니다. 최근 각 network의 구조를 조합하고, 새로이 만들고, unsupervised learning을 하는 등 여러 변형을 만드는데, 이 또한 새로운 장르의 가능성을 제시하지 않았나 생각이 듭니다. 아까 준홍이형이 얘기했던, pair-wise한 것을 거리정보까지 고려하자 라는 부분에서 저는 detection network를 추가해주면 객체 추출을 통해 어떤 pixel을 봐야할지 알 수 있고, 좀 더 효율적인 접근이 되지 않을까 생각했는데, 뒤에서 network parameter를 보니 그리 deep한 구조가 아니어서 괜찮겠다 싶었습니다. 연구실원의 요구에 따라 논문 발표를 준비해주셔서 더 뜻깊지 않았나 싶습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171017170402]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[972]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[일단 개인적으로 요청한 논문을 해줘서 너무 감사합니다. 그리고 역시나 재미있었습니다. 요즘 어떻게 섞느냐 섞는 재료를 앞에서 당겨 오느냐가 거의 대부분의 성능 논문입니다.  여기서 shuffle부분이 재미있는 부분인거 같습니다.  spatial 정보를 잃어도 알아서 ANN이 잡아주는역할을 합니다. 그리고 elementwise-sum을 하는데 실제 많이 트레이닝 해보면 이 부분이 약할거 같지만 그렇지가 않습니다. 생각보다 neural net이 학습할 수 있는양이 많다는것도 요즘 많이 깨닫고 있는 부분이어서 아주 재미있게 봤습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171017170926]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[973]]></uid>
		<content_uid><![CDATA[363]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[일단 이런 논문들에서 항상 느끼는거는 데이터 싸움이다로 끝이 나는거 같습니다. 해당 방식들은 데이터가 적을경우 쉽지 않은 방법론이고, variant들이 가정자체가 많이 데이터가 있다는 가정입니다. summarization에 대한 variant도 다양한 각도로 본다면 나중에는 정말 재미있는 결과가 나오지 않을까 생각해 보았습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171017171147]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[974]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[25]]></user_uid>
		<user_display><![CDATA[정 재윤]]></user_display>
		<content><![CDATA[예전에 논문이 바로 나왔을 때 한 번 읽고 제대로 이해하지 못했는데 오늘 세미나 발표를 들으면서 훨씬 깊게 이해할 수 있었습니다. 관계추론이 필요없는ㄴ 태스트에서는 이미 딥러닝이 매우 좋은 성과를 내고 있는 상황입니다. 거기에 더해 사람이 압도적으로 잘 한다고 생각하고 있었던 관계추론 부분까지 딥러닝이 쫓아왔다는게 대단히 고무적이라는 생각이 들었습니다. Image classification처럼 최근 많이 연구된 분야가 아니라서 발전 가능성이 매우 높고 재미있는 분야인 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171017225739]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[975]]></uid>
		<content_uid><![CDATA[363]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[Encoder와 Decoder가 함께 있는 RNN 기반의 모델들 역시 굉장히 다양한 영역에서 그에 맞는 다양한 구조로 나타나고 있습니다. 저는 주로 CNN 위주의 모델을 다루다보니 이에 익숙하지 않고, 논문 설명들에 대하여 와닿지 않는 것 같습니다. 여유 있을 때에 기본적인 모델이라도 구현하고 익숙해져야하겠다는 생각이 들었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171018180810]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[976]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[19]]></user_uid>
		<user_display><![CDATA[박재선]]></user_display>
		<content><![CDATA[직접 읽어보지 않아 자세하게는 모르겠지만, 모든 조합을 고려한다는 것이 재미있었습니다. 한편으로는 모든 조합을 고려하기에 inference time이 어느정도 걸릴까라는 생각도 들었습니다. 물론 Faster R-CNN과 같은 Object detection도 굉장히 많은 region proposal에 대하여 연산함에도 1초가 되지 않는 시간에 연산이 끝나지만, 조합을 다룬다는 것은 그보다 훨씬 더 많은 경우의 수를 따지지 않나 싶었습니다. 굉장히 재미있는 발표였습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171018181047]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[977]]></uid>
		<content_uid><![CDATA[363]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[본 논문에서 개인적으로 가장 흥미로웠던 부분은 생성확률 이었습니다.  GAN등으로 없던 것을 생성해 낼 때, 이미지의 경우 continuous하므로 적당한 숫자만 있어도 사람이 보기에 오호... 그래도 그림같긴 하네? 라는 생각을 할 수 있는데 반해, 텍스트의 경우 discrete한 성질이 있어 필요한 글자가 오지 않으면 이질감을 쉽게 느끼는 문제가 있습니다. 그런 맥락에서 봤을 때 생성확률이라는 장치를 걸어둔 것이 충분히 의미있는 접근법이지 않았나 생각합니다. 그리고 마지막에 한계점에 달아두셨듯 가용한 데이터가 한글로는 없다는 한계를 언급하셨는데, 대선때 4차 산업혁명 얘기를 가지고 많은 토론을 나누셨듯, 정부지원으로 어느정도 해소되면 좋지 않을까 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171018221814]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[978]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[26]]></user_uid>
		<user_display><![CDATA[서하 송]]></user_display>
		<content><![CDATA[역량이 부족해 발표 내용을 온전히 이해하지 못했지만 매우 흥미로웠습니다. 기존의 분석에서 더 나아가 좀더 사람의 사고방식을 모사하는 알고리즘이라고 생각합니다. 처음 이 논문이 나왔을 때 주변에서 대단하다고 많이들 말씀하셨는데 이번 기회에 세미나로 접하게 되어 기쁩니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171019153505]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[979]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[Relational Reasoning의 핵심은 모든 feature들의 관계를 특정 조건과 함께 고려하여 관계성이 가장 높은 feature를 찾아내는 것이라고 이해했습니다. 다른 표현으로 보자면 object간의 관계를 추론해내는 것으로도 이해가 되었습니다. 논문 제목이 내용을 매우 적절히 표현하고 있는 것 같습니다. 아직 element wise sum과 그 이후의 network가 어떤 의미를 지니는 것인지에 대해서는 의문점이 남지만 앞단의 cnn feature와 lstm feature의 조합이 주는 의미에서 새로운 것을 배웠습니다. 좋은 논문 리뷰 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171020172823]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[980]]></uid>
		<content_uid><![CDATA[361]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[우선 ROM방식에서 가장 좋은 성능을 발휘하는 모델이 bi-directional이 아니라 one-way이라는 점이 흥미로웠습니다. 어느 한 방법론이 모든 실험에서 가장 좋은 성능을 나타냄이 아님을 다시 확인 할 수 있었고, 이에대한 해석은 모델에 대한 충분한 이해를 한 후 다시 생각해 볼법한 것 같습니다. 또한 stack개념을 보면서 역시 머신러닝과 그에 관련한 필요한 수학들만 공부하기보다는 자료구조나 보안 OS와같은 학문들도 공부해야하지않을까 생각하였습니다. 미국에서 돌아오자마자 발표하느라 수고 많으셨습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171021102722]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[981]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[23]]></user_uid>
		<user_display><![CDATA[서 승완]]></user_display>
		<content><![CDATA[논문을 읽을때마다 느끼는 점이지만 아이디어 자체는 심플한 것 같습니다. 하지만 한번 더 생각해보면 역시 천재적이구나 싶습니다. 우선 combination을 통하여 object들 간의 정보를 모두 담을 수 있다는 발상이 신선하였습니다. 어떻게 생각하면 준홍이형의 말씀처럼 spatial정보를 애써 풀어해친다고 생각하고 시도조차 하지 않을 수 도 있지만 막상 살펴보면 되려 task에 필요한 모든 정보를 취함을 알 수 있습니다. 또한 재선이의 말처럼 시간이 오래 걸릴수도 있겠지만 결국 n개의 objects에서 2개의 combination을 찾는 nC2가 되기때문에 생각보다는 빠르지 않을까 싶습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171021103818]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[984]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[13]]></user_uid>
		<user_display><![CDATA[박민식]]></user_display>
		<content><![CDATA[CNN을 이용하여 특징들을 추출하고, RNN단에서 질문들에 대해 학습하여 상황에 맞는 정답을 찾아내는 Relation reasoning 구조라고 들었습니다. Relation Network 학습부분을 100% 로 이해한 정도가 아니어서 다시 논문을 읽어보면서 복습해볼 계획입니다. 좋은 발표 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171024114748]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[985]]></uid>
		<content_uid><![CDATA[362]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[object set의 simple function(MLPs)과 element-wise sum의 결합으로 CNN, LSTM의 구조를 구현했다는 점이 인상깊습니다. 최근 관계형 추론 관련해서 실험결과를 살펴보면 그 결과의 성능이 매우 높지 못할지라도 그 관계를 추론할 수 있다는 점에서 의미가 깊은 방법론 인것 같으며, 관계형으로 학습된 네트워크를 다시한번 fine-tunning을 통한 best모델을 구현 한 논문도 보았던 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171116201358]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[986]]></uid>
		<content_uid><![CDATA[363]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[이번 세미나에 학회참석으로 인해 참석하지 못했지만, 이 방법론의 구조를 생각해 보면 기존모델의 출력값 어휘분포와  원문 문서로부터의 attention 분포사이에서 학습된 생성 확률을 반영하여 새로운 단어를 생성하여 문서를 요약하거나 기존 원문 text를 재현하여 요약할지를 학습하여 결정하게 되는 것이 인상 깊었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171116201650]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[987]]></uid>
		<content_uid><![CDATA[379]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[안녕하세요, 김동화입니다. 이번 세미나에서는 텍스트방법의 다양한 이슈들을 다루고 있었고, 기존의 방법론들을 탈피해서 좋은 모델들을 시도 했다는 점이 굉장히 인상깊습니다. 수일치부터 시작해서 문법파싱까지 다양한 task를 목적에서 파생된 모델링을 언급했다는점이 기존 세미나보다 새로움을 느낄수 있었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171116202135]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[988]]></uid>
		<content_uid><![CDATA[381]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[6]]></user_uid>
		<user_display><![CDATA[Donghwa KIM]]></user_display>
		<content><![CDATA[글자크기가 작은 점이 아쉬운 세미나였습니다. 하지만 전반적인 RL를 다뤘다는점이 발표자의 노력을 느낄수 있었으며 교수님의 피드백에 따라 점점 개선되는 모습을 느낄 수 있었습니다. 솔직히 RL을 세미나 했을때 해외에서 영어공부를 하고 있어서 제대로 이해하는 부분이 없어, 즉 사전지식이 부족해 세미나를 이해는데 어려움이 있었습니다. 앞으로 이런 부족한 점을 보완하도록 하겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171116202440]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[989]]></uid>
		<content_uid><![CDATA[379]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[언어학적 지식을 텍스트 분석용 기계 학습 모델에 적용하는 작업은 아주 흥미로운 작업처럼 보인다. 분석의 성능 향상에 크게 기여해줄 수 있을 것 처럼 보이지만, 강의를 듣고 느낀점은 언어학적 지식을 계산 가능한 형태로 표현하고, 알고리즘의 구동 과정에 녹여내는 일이 만만하지 않다는 것이다. 파싱 트리 데이터를 구하기 어렵다는 근원적인 문제점을 차치하고라도, 문법 정보를 활용하는 것이 쉬워 보이지만 어려운 작업이라고 느꼈습니다. 앞으로 흥미로운 연구가 많이 있을 것이라 예상되는 분야입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171118164458]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[990]]></uid>
		<content_uid><![CDATA[381]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[8]]></user_uid>
		<user_display><![CDATA[김 해동]]></user_display>
		<content><![CDATA[예전에 강화학습 세미나를 하던때가 생각나네요. 그때도 생각했던 것이지만 강화학습은 참 어려운 것 같습니다. 진입장벽이 높아서 저는 손을 놓고 있었는데, 승완이를 포함해서 강화학습 공부를 꾸준히 진행하고 있는 연구실 인원들이 있어서 관련 내용을 세미나를 통해서 환기할 수 있어서 아주 감사하게 생각하고 있습니다. 저도 스타크래프 예시를 들으면서 multi-agent 환경에서 각 행위자의 보상함수를 어떻게 정의할지 고민하고 있었는데, 때마침 명준이가 질문을 해주었고 발표자가 게시글에 참고자료를 올려주었네요. 공부를 하는데 큰 도움이 되었습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171118165713]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[991]]></uid>
		<content_uid><![CDATA[379]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[텍스트 분석의 다양한 주제들을 다루어 주셔서 감사합니다. 가장 흥미로웠던 점은 accuracy 같은 정량적 지표들의 적합성에 대한 주제였습니다. Naive Bayes model 과 비교한 예씨를 통해 보다 확실하게 느낄 수 있었습니다. Model 의 성능을 입증하기 위해서는 단순히 정량적 지표 뿐만 아니라 더욱 자세한 분석이 필요하다고 느꼈습니다.
Word를 3가지 mode의 결합으로 표현한 부분도 흥미로웠습니다. Word2vec 이 갖는 단점을 상대적으로 간단한(?) 아이디어로 해결한점이 인상깊었습니다. Word vector에 대한 연구도 상당히 흥미로워 보이며, 이미지와는 다르게 텍스트의 vector 값이 절대적인 값이 아닌 고질적인 문제도 해결했으면 하는 바램입니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171119145233]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[992]]></uid>
		<content_uid><![CDATA[381]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[24]]></user_uid>
		<user_display><![CDATA[장 명준]]></user_display>
		<content><![CDATA[1학기 초에 큰맘먹고 강화학습을 공부했다가 크나큰 좌절을 맛보고 손도 안대고 있었는데, 발표자께서 기본 개념부터 정리해주셔서 큰 도움이 되었습니다. 매우 복잡한 Multi-agent 환경에서 행위자들의 reward function을 어떻게 정의했을까 궁금했었는데 이에대해 참고자료를 공유해주셔서 감사합니다. 한번 열심히 공부해보겠습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171119145825]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[995]]></uid>
		<content_uid><![CDATA[381]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[강화학습 이름만 들어도 두려움이 느껴지는 주제인데요, 최근 재미있는 연구 성과가 많이 나고 있으며, DNN에 합쳐서 사용하기도 하는 추세가 된 것 같아 이젠 물러서지 않고 진짜 공부를 해야되는 때가 왔나 싶은 생각이 듭니다. 교수님께서 말씀해 주신 것처럼 앞 장표에서는 강화학습에 대한 내용을 전반적으로 훑어줘서 너무 좋았습니다. 다만, 용어가 어떨땐 영어, 어떨땐 번역된 단어로 등장해서 이게 맞나? 하는 찰나에 내용이 지나가는 부분이 가끔 있어서 개인적으로 이해하기가 쉽지는 않았습니다. 이후에는 최근 RL에서 진행되고 있는 연구, 그리고 가장 핫하게 사용되는 방법론에 대해 짚어주었는데, 이런 흐름을 알고 추후에 공부한다면 분명 큰 도움이 되리라 생각합니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171119232205]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[996]]></uid>
		<content_uid><![CDATA[381]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[강화학습에 대해 전반적으로 되짚어 볼 수 있는 시간이었습니다. 다소 오래전에 공부했던 개념이라 기억이 나지 않는 부분도 있었고, 이해를 잘못했던 부분들도 확인하였습니다. 강화학습도 머신러닝의 큰 한 축을 이루고 있기 때문에 그 내용을 어느정도 이해하고 있어야 한다고 생각합니다. 또한 수식적인 접근이 많은 분야이기에 공부를 하려면 기초적인 개념을 확실히 정립해가면서 해야한다고 생각합니다. 최근의 트랜드까지 다뤄준 좋은 세미나였습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171120191407]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[994]]></uid>
		<content_uid><![CDATA[379]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[7]]></user_uid>
		<user_display><![CDATA[서 덕성]]></user_display>
		<content><![CDATA[DeepNLP 마지막 강의로, 연구실 내부 세미나에서 텍스트 관련 다루었던 내용을 총망라한 마지막 정리 강의가 아니었나 싶습니다. 재미있었던 것은, 단어를 벡터로 표현할 때 어떻게 하면 더 정보를 많이 가질 수 있는 것이냐는 주제였습니다. 위 댓글에서 명준이가 언급한 것처럼, 단어의 vector는 절대적인 값이라고 생각하지 않습니다. task에 무관하게 단어가 잘 표현된 vector와 주어진 task를 위해 최적화된 vector가 있다면, 아직까지는 후자가 연구자들에게 받아들여지는 상황이기에 task를 학습하는 동안 단어의 embedding vector까지도 학습시키고자 한다고 생각합니다. 또한 단어를 표현하기 위해 강의에서 언급한 방식은 (Word, Morphological, Character)를 모두 엮어내는 방식인데, 단어에 대한 정보를 많이 담기 위한 노력은 알겠는데, 무언가 잘못된 방향일수도 있을것 같다는 생각을 했습니다. DeepNLP를 마치며 정말 공부할 것이 많구나 라는 생각이 듭니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171119231802]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[997]]></uid>
		<content_uid><![CDATA[379]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[18]]></user_uid>
		<user_display><![CDATA[모경현]]></user_display>
		<content><![CDATA[DeepNLP 유종의 미를 거둬주셨습니다. 여러가지 주제들을 말씀해 주셨지만 가장 많은 생각을 하게 했던 주제는 word 표현에 관한 부분이었습니다. 단어에 대한 많은 정보를 담아내는 방식으로 벡터를 생성하려는 시도는 이해했으나 과연 부분부분 나누어 단순히 정보를 concat하는 것이 올바른 접근 방식인가에 대해서는 의문이 남습니다. 많은 NLP문제들을 풀기 위해서는 자연어로 이루어진 단어나 문장들이 컴퓨터가 이해할 수 있는 방식으로, 그리고 그 방식이 효율적이어야 한다고 생각합니다. 앞으로도 연구가 많이 되어야 할 분야인 것 같습니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171120192000]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[998]]></uid>
		<content_uid><![CDATA[379]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[대망의 마지막 강의였습니다. 제가 중간에 질문드린것처럼 Input단을 여러가지 feature로 쓰는것은 당연히 text기반의 task에서 좋거나 비슷한 정도의 결과가 나올것을 기대할 수 있을것 같습니다. 제가 생각할때의 text기반의 연구 방향은 점점 unsupervised로 가고 있다고 생각하는데 이 부분에서 큰  break through가 나오는 연구가 있으면 정말 재미있겠다고 생각하였습니다. label이 많은 사람이 승자인 구조에서 벗어나는 연구가 나온다면 상당히 흥미로울것 같습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171121123502]]></created>
		<password><![CDATA[]]></password>
	</data>
	<data>
		<uid><![CDATA[999]]></uid>
		<content_uid><![CDATA[381]]></content_uid>
		<parent_uid><![CDATA[0]]></parent_uid>
		<user_uid><![CDATA[10]]></user_uid>
		<user_display><![CDATA[김 준홍]]></user_display>
		<content><![CDATA[강화학습에 대한 전반적인 기초 지식부터 새로 상기시켜주었고 A3C라는 새로운 정의를 말해주었습니다. 솔직히 이야기하여 강화학습부분은 손을 많이 놓고 있는 상황이라 전부 흡수 하기에는  아직 역부족입니다. 하지만 지금 하고 있는 연구들이 어느정도 마무리되면 현재 강화학습으로 풀 수 있는 문제에 대한 아이디어가 있어서 해당 부분에 대하여 연구해볼 계획입니다. 강화학습이 현재,  environment 구조에서 무한대로 뽑아낼 수 있는 구조가 아닌 상황에서 어떻게 처리해야 할까 항상 고민중입니다. 현실상황에서 single agent라는것과 몇가지 action으로 행해지는 경우는 많이 없다고 생각합니다. 실제 로봇에서 actuation도 action은 무한대로 관절부분에서 생기며 이를 강화학습을할때의 environment를 구하기 위한, 생각지 못한 외생을 위한 imitation learning같은 것들을 좀더 면밀히 살펴보고 지금 해보고자 하는 연구에서 적용해볼 생각입니다. 새로운 강화학습 개념을 준비하느라 고생 많았습니다. 감사합니다.]]></content>
		<like><![CDATA[0]]></like>
		<unlike><![CDATA[0]]></unlike>
		<vote><![CDATA[0]]></vote>
		<created><![CDATA[20171121123845]]></created>
		<password><![CDATA[]]></password>
	</data>
</kboard_comments>
<kboard_meida>
</kboard_meida>
<kboard_meida_relationships>
</kboard_meida_relationships>
</kboard>